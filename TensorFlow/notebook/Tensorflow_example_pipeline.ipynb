{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### component that obtains data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data_op():\n",
    "    return dsl.ContainerOp(\n",
    "        name = 'Obtain Data',\n",
    "        image = 'mojolaoluwa/get-data-component:v.0.1',\n",
    "        arguments = [],\n",
    "        file_outputs={\n",
    "            'data': '/obtain_data/data'\n",
    "        }      \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### component that does preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_op(data):\n",
    "    return dsl.ContainerOp(\n",
    "        name = 'Preprocess Data',\n",
    "        image = 'mojolaoluwa/preprocess-component:v.0.1',\n",
    "        arguments = [\n",
    "            '--data', data\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'X_train':'/preprocess_data/X_train.npy',\n",
    "            'X_test':'/preprocess_data/X_test.npy',\n",
    "            'y_train':'/preprocess_data/y_train.npy',\n",
    "            'y_test':'/preprocess_data/y_test.npy'     \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### component for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op(X_train, y_train):\n",
    "    return dsl.ContainerOp(\n",
    "        name = 'Train Model',\n",
    "        image = 'mojolaoluwa/train-tensorflow:v.0.1' ,\n",
    "        arguments = [\n",
    "            '--X_train', X_train,\n",
    "            '--y_train', y_train   \n",
    "        ],\n",
    "        file_outputs={\n",
    "            'model':'/train_data/classifier.h5'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### components for predicting on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_op(X_test, y_test, model):\n",
    "    return dsl.ContainerOp(\n",
    "        name='Predict Model',\n",
    "        image=\n",
    "        arguments = [\n",
    "            '--X_test', X_test,\n",
    "            '--y_test', y_test,\n",
    "            '--model', model\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'results':'/predict_data/results'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining pipeline and including its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Churn modelling pipeline',\n",
    "   description='An ML reusable pipeline that performs customer segmentation to determine customers with high risk of leaving a bank .'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def churn_reuseable_tensorflow_pipeline():\n",
    "    _obtain_data_op = obtain_data_op()\n",
    "    \n",
    "    _preprocess_op = preprocess_op(\n",
    "        dsl.InputArgumentPath(_obtain_data_op.outputs['data'])).after(_obtain_data_op)\n",
    "    \n",
    "    _train_op = train_op(\n",
    "        dsl.InputArgumentPath(_preprocess_op.outputs['X_train']),\n",
    "        dsl.InputArgumentPath(_preprocess_op.outputs['y_train'])).after(_preprocess_op)\n",
    "\n",
    "    _predict_op = predict_op(\n",
    "        dsl.InputArgumentPath(_preprocess_op.outputs['X_test']),\n",
    "        dsl.InputArgumentPath(_preprocess_op.outputs['y_test']),\n",
    "        dsl.InputArgumentPath(_train_op.outputs['model'])).after(_train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "experiment_name = 'churn_analysis_tensorflow_pipeline'\n",
    "kfp.compiler.Compiler().compile(churn_reuseable_tensorflow_pipeline,  \n",
    "  '{}.zip'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(churn_reuseable_tensorflow_pipeline, arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "output_dir = \"/home/jovyan/trial/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessing fucntion\n",
    "\n",
    "def get_data(data_path):\n",
    "    #importing libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    import pandas as pd\n",
    "    #importing the data\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/AdeloreSimiloluwa/Artificial-Neural-Network/master/data/Churn_Modelling.csv\")\n",
    "\n",
    "    ## serialize clean data to output directory\n",
    "    with open(f'{data_path}/clean_data','wb') as f:\n",
    "        pickle.dump((data),f)\n",
    "    \n",
    "    return (print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and prediction function\n",
    "\n",
    "def preprocess(data_path):\n",
    "    import pickle\n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "    # deserialize clean data from output directory\n",
    "    with open(f'{data_path}/clean_data','rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "    # serialize clean data to output directory\n",
    "    with open(f'{data_path}/split_data','wb') as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test),f)\n",
    "   \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_path, model):\n",
    "    #importing libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "\n",
    "   # deserialize clean data from output directory\n",
    "    with open(f'{data_path}/split_data','rb') as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "    \n",
    "    #initializing the classifier model with its input, hidden and output layers\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 16, init='uniform', activation='relu', input_dim=12,))\n",
    "    classifier.add(Dense(units = 8, init='uniform', activation='relu'))\n",
    "    classifier.add(Dense(units = 1, init='uniform', activation='sigmoid'))\n",
    "    #Compiling the classifier model with Stochastic Gradient Desecnt\n",
    "    classifier.compile(optimizer = 'adam', loss='binary_crossentropy' , metrics =['accuracy'])\n",
    "    #fitting the model\n",
    "    classifier.fit(X_train, y_train, batch_size=10 , nb_epoch=150)\n",
    "    #saving the model\n",
    "    classifier.save(f'{data_path}/{model}')\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def predict(data_path,model):\n",
    "    #importing libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import load_model\n",
    "    \n",
    "    # deserialize clean data from output directory\n",
    "    with open(f'{data_path}/split_data','rb') as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "        \n",
    "    classifier = load_model(model)\n",
    "\n",
    "    #Evaluate the model and print the results\n",
    "    test_loss, test_acc = classifier.evaluate(X_test,  y_test, verbose=0)\n",
    "    print('Test accuracy:', test_acc)\n",
    "    print('Test loss:', test_loss)\n",
    "    #model's prediction on test data\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    # create a threshold for the confution matrics\n",
    "    y_pred=(y_pred>0.5)\n",
    "    \n",
    "    # confusion metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create light weight components\n",
    "get_data_op = comp.func_to_container_op(get_data)\n",
    "preprocess_op = comp.func_to_container_op(preprocess, base_image=\"python 3.7\")\n",
    "train_op = comp.func_to_container_op(train, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = comp.func_to_container_op(predict, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name='Churn modelling pipeline',\n",
    "   description='An ML reusable pipeline that performs customer segmentation to determine customers with high risk of leaving a bank .')\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def road_safety_pipeline(data_path: str ):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO) #RWO\n",
    "\n",
    "    # Create get-data components.\n",
    "    churn_get_data_container = get_data_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create preprocess component.\n",
    "    churn_preprocess_container = preprocess_op(data_path).add_pvolumes({data_path: churn_get_data_container.pvolume})\n",
    "    \n",
    "     # Create train component.\n",
    "    churn_train_container = preprocess_op(data_path,model).add_pvolumes({data_path: churn_preprocess_container.pvolume})\n",
    "    \n",
    "     # Create predict component.\n",
    "    churn_predict_container = preprocess_op(data_path,model).add_pvolumes({data_path: churn_train_container.pvolume})\n",
    "\n",
    "    # Print the result of the prediction\n",
    "    churn_result_container = dsl.ContainerOp(\n",
    "            name=\"print_prediction\",\n",
    "            image='library/bash:4.4.23', # 'gcr.io/kubeflow-images-public/tensorflow-2.1.0-notebook-gpu:1.0.0'\n",
    "            pvolumes={data_path: churn_predict_container.pvolume},\n",
    "            arguments=['cat', f'{data_path}/results.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt' #'/home/jovyan/data/clean_data'\n",
    "MODEL_PATH='churn_classifier.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = churn_pipeline\n",
    "\n",
    "experiment_name = 'churn_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "            \"model\":MODEL_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
