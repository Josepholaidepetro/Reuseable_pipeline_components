{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.local/lib/python3.6/site-packages (21.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: kfp in ./.local/lib/python3.6/site-packages (1.3.0)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in ./.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.11)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.10.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.1.4)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in ./.local/lib/python3.6/site-packages (from kfp) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.25.7)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->jsonschema>=3.0.1->kfp) (8.0.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='/home/jovyan/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/AdeloreSimiloluwa/Artificial-Neural-Network/master/data/Churn_Modelling.csv\")\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "  \n",
    "    #Save the train_data as a pickle file to be used by the train component.\n",
    "    with open(f'{data_path}/train_data', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/test_data', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'torch==1.7.1'])\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    #loading the train data\n",
    "    with open(f'{data_path}/train_data', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, y_train = train_data\n",
    "    \n",
    "    #setting model hyper-parameters\n",
    "    EPOCHS = 1\n",
    "    BATCH_SIZE =10\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "     #train data\n",
    "    class trainData(Dataset):\n",
    "        def __init__(self, X_data, y_data):\n",
    "            self.X_data = X_data\n",
    "            self.y_data = y_data\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "            return self.X_data[index], self.y_data[index]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X_data)\n",
    "        \n",
    "    train_data = trainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    #defining neural network architecture\n",
    "    class binaryClassification(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(binaryClassification, self).__init__()\n",
    "            #number of input features is 12\n",
    "            self.layer_1 = nn.Linear(12, 16)\n",
    "            self.layer_2 = nn.Linear(16, 8)\n",
    "            self.layer_out = nn.Linear(8, 1) \n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "            self.batchnorm2 = nn.BatchNorm1d(8)       \n",
    "        #feed forward network\n",
    "        def forward(self, inputs):\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.relu(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            return x\n",
    "    #initializing optimizer and loss\n",
    "    classifier = binaryClassification()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "    #function to calculate accuracy\n",
    "    def binary_acc(y_pred, y_test):\n",
    "        y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "        results_sum = (y_pred_tag == y_test).sum().float()\n",
    "        acc = results_sum/y_test.shape[0]\n",
    "        acc =torch.round(acc*100)\n",
    "        return acc\n",
    "    #training the model\n",
    "    classifier.train()\n",
    "    for e in range(1, EPOCHS+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            #setting gradient to 0 per mini-batch\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = classifier(X_batch)\n",
    "            loss =criterion(y_pred, y_batch)\n",
    "            acc = binary_acc(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            print(f'Epoch {e+0:03}: | Loss:{epoch_loss/len(train_loader):.5f} | Acc:{epoch_acc/len(train_loader):.3f}')\n",
    "    #saving model\n",
    "    torch.save(classifier.state_dict(), f'{data_path}/pyclassifier.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss:0.00090 | Acc:0.062\n",
      "Epoch 001: | Loss:0.00180 | Acc:0.125\n",
      "Epoch 001: | Loss:0.00269 | Acc:0.188\n",
      "Epoch 001: | Loss:0.00343 | Acc:0.287\n",
      "Epoch 001: | Loss:0.00426 | Acc:0.362\n",
      "Epoch 001: | Loss:0.00517 | Acc:0.412\n",
      "Epoch 001: | Loss:0.00611 | Acc:0.475\n",
      "Epoch 001: | Loss:0.00712 | Acc:0.500\n",
      "Epoch 001: | Loss:0.00815 | Acc:0.537\n",
      "Epoch 001: | Loss:0.00886 | Acc:0.613\n",
      "Epoch 001: | Loss:0.00972 | Acc:0.688\n",
      "Epoch 001: | Loss:0.01059 | Acc:0.750\n",
      "Epoch 001: | Loss:0.01167 | Acc:0.800\n",
      "Epoch 001: | Loss:0.01274 | Acc:0.825\n",
      "Epoch 001: | Loss:0.01366 | Acc:0.887\n",
      "Epoch 001: | Loss:0.01446 | Acc:0.963\n",
      "Epoch 001: | Loss:0.01536 | Acc:0.988\n",
      "Epoch 001: | Loss:0.01630 | Acc:1.038\n",
      "Epoch 001: | Loss:0.01733 | Acc:1.075\n",
      "Epoch 001: | Loss:0.01822 | Acc:1.137\n",
      "Epoch 001: | Loss:0.01912 | Acc:1.200\n",
      "Epoch 001: | Loss:0.02000 | Acc:1.275\n",
      "Epoch 001: | Loss:0.02079 | Acc:1.337\n",
      "Epoch 001: | Loss:0.02169 | Acc:1.387\n",
      "Epoch 001: | Loss:0.02267 | Acc:1.438\n",
      "Epoch 001: | Loss:0.02364 | Acc:1.488\n",
      "Epoch 001: | Loss:0.02454 | Acc:1.562\n",
      "Epoch 001: | Loss:0.02544 | Acc:1.625\n",
      "Epoch 001: | Loss:0.02632 | Acc:1.688\n",
      "Epoch 001: | Loss:0.02732 | Acc:1.725\n",
      "Epoch 001: | Loss:0.02821 | Acc:1.788\n",
      "Epoch 001: | Loss:0.02901 | Acc:1.875\n",
      "Epoch 001: | Loss:0.02992 | Acc:1.925\n",
      "Epoch 001: | Loss:0.03082 | Acc:1.975\n",
      "Epoch 001: | Loss:0.03170 | Acc:2.025\n",
      "Epoch 001: | Loss:0.03261 | Acc:2.087\n",
      "Epoch 001: | Loss:0.03336 | Acc:2.175\n",
      "Epoch 001: | Loss:0.03418 | Acc:2.250\n",
      "Epoch 001: | Loss:0.03496 | Acc:2.337\n",
      "Epoch 001: | Loss:0.03578 | Acc:2.413\n",
      "Epoch 001: | Loss:0.03672 | Acc:2.475\n",
      "Epoch 001: | Loss:0.03759 | Acc:2.538\n",
      "Epoch 001: | Loss:0.03855 | Acc:2.575\n",
      "Epoch 001: | Loss:0.03935 | Acc:2.675\n",
      "Epoch 001: | Loss:0.04010 | Acc:2.775\n",
      "Epoch 001: | Loss:0.04089 | Acc:2.862\n",
      "Epoch 001: | Loss:0.04167 | Acc:2.938\n",
      "Epoch 001: | Loss:0.04253 | Acc:3.013\n",
      "Epoch 001: | Loss:0.04341 | Acc:3.062\n",
      "Epoch 001: | Loss:0.04423 | Acc:3.150\n",
      "Epoch 001: | Loss:0.04504 | Acc:3.225\n",
      "Epoch 001: | Loss:0.04582 | Acc:3.325\n",
      "Epoch 001: | Loss:0.04668 | Acc:3.375\n",
      "Epoch 001: | Loss:0.04762 | Acc:3.425\n",
      "Epoch 001: | Loss:0.04866 | Acc:3.475\n",
      "Epoch 001: | Loss:0.04955 | Acc:3.525\n",
      "Epoch 001: | Loss:0.05059 | Acc:3.562\n",
      "Epoch 001: | Loss:0.05154 | Acc:3.612\n",
      "Epoch 001: | Loss:0.05254 | Acc:3.650\n",
      "Epoch 001: | Loss:0.05336 | Acc:3.725\n",
      "Epoch 001: | Loss:0.05435 | Acc:3.788\n",
      "Epoch 001: | Loss:0.05517 | Acc:3.862\n",
      "Epoch 001: | Loss:0.05604 | Acc:3.913\n",
      "Epoch 001: | Loss:0.05688 | Acc:4.000\n",
      "Epoch 001: | Loss:0.05777 | Acc:4.062\n",
      "Epoch 001: | Loss:0.05871 | Acc:4.112\n",
      "Epoch 001: | Loss:0.05949 | Acc:4.188\n",
      "Epoch 001: | Loss:0.06026 | Acc:4.275\n",
      "Epoch 001: | Loss:0.06103 | Acc:4.362\n",
      "Epoch 001: | Loss:0.06190 | Acc:4.450\n",
      "Epoch 001: | Loss:0.06269 | Acc:4.537\n",
      "Epoch 001: | Loss:0.06356 | Acc:4.625\n",
      "Epoch 001: | Loss:0.06441 | Acc:4.688\n",
      "Epoch 001: | Loss:0.06523 | Acc:4.775\n",
      "Epoch 001: | Loss:0.06599 | Acc:4.888\n",
      "Epoch 001: | Loss:0.06670 | Acc:4.987\n",
      "Epoch 001: | Loss:0.06749 | Acc:5.062\n",
      "Epoch 001: | Loss:0.06840 | Acc:5.162\n",
      "Epoch 001: | Loss:0.06921 | Acc:5.237\n",
      "Epoch 001: | Loss:0.06998 | Acc:5.338\n",
      "Epoch 001: | Loss:0.07086 | Acc:5.412\n",
      "Epoch 001: | Loss:0.07160 | Acc:5.513\n",
      "Epoch 001: | Loss:0.07250 | Acc:5.562\n",
      "Epoch 001: | Loss:0.07344 | Acc:5.600\n",
      "Epoch 001: | Loss:0.07430 | Acc:5.662\n",
      "Epoch 001: | Loss:0.07515 | Acc:5.750\n",
      "Epoch 001: | Loss:0.07596 | Acc:5.838\n",
      "Epoch 001: | Loss:0.07672 | Acc:5.938\n",
      "Epoch 001: | Loss:0.07753 | Acc:6.025\n",
      "Epoch 001: | Loss:0.07825 | Acc:6.125\n",
      "Epoch 001: | Loss:0.07910 | Acc:6.188\n",
      "Epoch 001: | Loss:0.07988 | Acc:6.263\n",
      "Epoch 001: | Loss:0.08070 | Acc:6.338\n",
      "Epoch 001: | Loss:0.08141 | Acc:6.412\n",
      "Epoch 001: | Loss:0.08228 | Acc:6.487\n",
      "Epoch 001: | Loss:0.08309 | Acc:6.562\n",
      "Epoch 001: | Loss:0.08414 | Acc:6.612\n",
      "Epoch 001: | Loss:0.08517 | Acc:6.662\n",
      "Epoch 001: | Loss:0.08588 | Acc:6.763\n",
      "Epoch 001: | Loss:0.08660 | Acc:6.850\n",
      "Epoch 001: | Loss:0.08737 | Acc:6.938\n",
      "Epoch 001: | Loss:0.08819 | Acc:7.000\n",
      "Epoch 001: | Loss:0.08895 | Acc:7.088\n",
      "Epoch 001: | Loss:0.08987 | Acc:7.150\n",
      "Epoch 001: | Loss:0.09073 | Acc:7.213\n",
      "Epoch 001: | Loss:0.09154 | Acc:7.312\n",
      "Epoch 001: | Loss:0.09240 | Acc:7.388\n",
      "Epoch 001: | Loss:0.09328 | Acc:7.450\n",
      "Epoch 001: | Loss:0.09398 | Acc:7.537\n",
      "Epoch 001: | Loss:0.09480 | Acc:7.612\n",
      "Epoch 001: | Loss:0.09574 | Acc:7.662\n",
      "Epoch 001: | Loss:0.09652 | Acc:7.750\n",
      "Epoch 001: | Loss:0.09727 | Acc:7.850\n",
      "Epoch 001: | Loss:0.09811 | Acc:7.925\n",
      "Epoch 001: | Loss:0.09888 | Acc:8.012\n",
      "Epoch 001: | Loss:0.09975 | Acc:8.088\n",
      "Epoch 001: | Loss:0.10059 | Acc:8.150\n",
      "Epoch 001: | Loss:0.10143 | Acc:8.225\n",
      "Epoch 001: | Loss:0.10231 | Acc:8.287\n",
      "Epoch 001: | Loss:0.10310 | Acc:8.387\n",
      "Epoch 001: | Loss:0.10376 | Acc:8.500\n",
      "Epoch 001: | Loss:0.10446 | Acc:8.613\n",
      "Epoch 001: | Loss:0.10521 | Acc:8.713\n",
      "Epoch 001: | Loss:0.10597 | Acc:8.812\n",
      "Epoch 001: | Loss:0.10683 | Acc:8.887\n",
      "Epoch 001: | Loss:0.10774 | Acc:8.950\n",
      "Epoch 001: | Loss:0.10847 | Acc:9.050\n",
      "Epoch 001: | Loss:0.10919 | Acc:9.150\n",
      "Epoch 001: | Loss:0.11006 | Acc:9.213\n",
      "Epoch 001: | Loss:0.11089 | Acc:9.300\n",
      "Epoch 001: | Loss:0.11157 | Acc:9.412\n",
      "Epoch 001: | Loss:0.11251 | Acc:9.463\n",
      "Epoch 001: | Loss:0.11347 | Acc:9.537\n",
      "Epoch 001: | Loss:0.11426 | Acc:9.600\n",
      "Epoch 001: | Loss:0.11505 | Acc:9.700\n",
      "Epoch 001: | Loss:0.11568 | Acc:9.812\n",
      "Epoch 001: | Loss:0.11648 | Acc:9.887\n",
      "Epoch 001: | Loss:0.11716 | Acc:9.988\n",
      "Epoch 001: | Loss:0.11785 | Acc:10.113\n",
      "Epoch 001: | Loss:0.11859 | Acc:10.213\n",
      "Epoch 001: | Loss:0.11962 | Acc:10.275\n",
      "Epoch 001: | Loss:0.12039 | Acc:10.363\n",
      "Epoch 001: | Loss:0.12115 | Acc:10.463\n",
      "Epoch 001: | Loss:0.12188 | Acc:10.550\n",
      "Epoch 001: | Loss:0.12257 | Acc:10.637\n",
      "Epoch 001: | Loss:0.12336 | Acc:10.738\n",
      "Epoch 001: | Loss:0.12403 | Acc:10.838\n",
      "Epoch 001: | Loss:0.12479 | Acc:10.938\n",
      "Epoch 001: | Loss:0.12553 | Acc:11.012\n",
      "Epoch 001: | Loss:0.12627 | Acc:11.100\n",
      "Epoch 001: | Loss:0.12701 | Acc:11.188\n",
      "Epoch 001: | Loss:0.12765 | Acc:11.300\n",
      "Epoch 001: | Loss:0.12837 | Acc:11.387\n",
      "Epoch 001: | Loss:0.12909 | Acc:11.488\n",
      "Epoch 001: | Loss:0.12984 | Acc:11.588\n",
      "Epoch 001: | Loss:0.13055 | Acc:11.688\n",
      "Epoch 001: | Loss:0.13139 | Acc:11.762\n",
      "Epoch 001: | Loss:0.13211 | Acc:11.850\n",
      "Epoch 001: | Loss:0.13276 | Acc:11.975\n",
      "Epoch 001: | Loss:0.13344 | Acc:12.088\n",
      "Epoch 001: | Loss:0.13417 | Acc:12.188\n",
      "Epoch 001: | Loss:0.13497 | Acc:12.250\n",
      "Epoch 001: | Loss:0.13577 | Acc:12.338\n",
      "Epoch 001: | Loss:0.13651 | Acc:12.438\n",
      "Epoch 001: | Loss:0.13724 | Acc:12.550\n",
      "Epoch 001: | Loss:0.13792 | Acc:12.650\n",
      "Epoch 001: | Loss:0.13863 | Acc:12.750\n",
      "Epoch 001: | Loss:0.13937 | Acc:12.850\n",
      "Epoch 001: | Loss:0.14012 | Acc:12.950\n",
      "Epoch 001: | Loss:0.14079 | Acc:13.062\n",
      "Epoch 001: | Loss:0.14155 | Acc:13.150\n",
      "Epoch 001: | Loss:0.14228 | Acc:13.250\n",
      "Epoch 001: | Loss:0.14303 | Acc:13.325\n",
      "Epoch 001: | Loss:0.14367 | Acc:13.438\n",
      "Epoch 001: | Loss:0.14442 | Acc:13.525\n",
      "Epoch 001: | Loss:0.14508 | Acc:13.625\n",
      "Epoch 001: | Loss:0.14592 | Acc:13.725\n",
      "Epoch 001: | Loss:0.14655 | Acc:13.838\n",
      "Epoch 001: | Loss:0.14717 | Acc:13.950\n",
      "Epoch 001: | Loss:0.14788 | Acc:14.050\n",
      "Epoch 001: | Loss:0.14861 | Acc:14.150\n",
      "Epoch 001: | Loss:0.14939 | Acc:14.225\n",
      "Epoch 001: | Loss:0.15006 | Acc:14.338\n",
      "Epoch 001: | Loss:0.15073 | Acc:14.438\n",
      "Epoch 001: | Loss:0.15151 | Acc:14.537\n",
      "Epoch 001: | Loss:0.15228 | Acc:14.637\n",
      "Epoch 001: | Loss:0.15296 | Acc:14.738\n",
      "Epoch 001: | Loss:0.15366 | Acc:14.838\n",
      "Epoch 001: | Loss:0.15428 | Acc:14.950\n",
      "Epoch 001: | Loss:0.15496 | Acc:15.050\n",
      "Epoch 001: | Loss:0.15562 | Acc:15.162\n",
      "Epoch 001: | Loss:0.15656 | Acc:15.225\n",
      "Epoch 001: | Loss:0.15726 | Acc:15.325\n",
      "Epoch 001: | Loss:0.15811 | Acc:15.400\n",
      "Epoch 001: | Loss:0.15887 | Acc:15.475\n",
      "Epoch 001: | Loss:0.15957 | Acc:15.575\n",
      "Epoch 001: | Loss:0.16030 | Acc:15.675\n",
      "Epoch 001: | Loss:0.16099 | Acc:15.775\n",
      "Epoch 001: | Loss:0.16155 | Acc:15.900\n",
      "Epoch 001: | Loss:0.16224 | Acc:15.988\n",
      "Epoch 001: | Loss:0.16308 | Acc:16.062\n",
      "Epoch 001: | Loss:0.16387 | Acc:16.125\n",
      "Epoch 001: | Loss:0.16454 | Acc:16.238\n",
      "Epoch 001: | Loss:0.16518 | Acc:16.350\n",
      "Epoch 001: | Loss:0.16580 | Acc:16.450\n",
      "Epoch 001: | Loss:0.16649 | Acc:16.538\n",
      "Epoch 001: | Loss:0.16738 | Acc:16.625\n",
      "Epoch 001: | Loss:0.16838 | Acc:16.688\n",
      "Epoch 001: | Loss:0.16925 | Acc:16.762\n",
      "Epoch 001: | Loss:0.16998 | Acc:16.863\n",
      "Epoch 001: | Loss:0.17064 | Acc:16.950\n",
      "Epoch 001: | Loss:0.17147 | Acc:17.050\n",
      "Epoch 001: | Loss:0.17236 | Acc:17.125\n",
      "Epoch 001: | Loss:0.17310 | Acc:17.212\n",
      "Epoch 001: | Loss:0.17381 | Acc:17.312\n",
      "Epoch 001: | Loss:0.17462 | Acc:17.400\n",
      "Epoch 001: | Loss:0.17525 | Acc:17.525\n",
      "Epoch 001: | Loss:0.17595 | Acc:17.625\n",
      "Epoch 001: | Loss:0.17664 | Acc:17.738\n",
      "Epoch 001: | Loss:0.17733 | Acc:17.837\n",
      "Epoch 001: | Loss:0.17793 | Acc:17.938\n",
      "Epoch 001: | Loss:0.17852 | Acc:18.050\n",
      "Epoch 001: | Loss:0.17914 | Acc:18.150\n",
      "Epoch 001: | Loss:0.17974 | Acc:18.262\n",
      "Epoch 001: | Loss:0.18045 | Acc:18.350\n",
      "Epoch 001: | Loss:0.18109 | Acc:18.462\n",
      "Epoch 001: | Loss:0.18182 | Acc:18.550\n",
      "Epoch 001: | Loss:0.18252 | Acc:18.637\n",
      "Epoch 001: | Loss:0.18327 | Acc:18.725\n",
      "Epoch 001: | Loss:0.18395 | Acc:18.837\n",
      "Epoch 001: | Loss:0.18454 | Acc:18.950\n",
      "Epoch 001: | Loss:0.18528 | Acc:19.050\n",
      "Epoch 001: | Loss:0.18595 | Acc:19.150\n",
      "Epoch 001: | Loss:0.18664 | Acc:19.250\n",
      "Epoch 001: | Loss:0.18731 | Acc:19.363\n",
      "Epoch 001: | Loss:0.18796 | Acc:19.475\n",
      "Epoch 001: | Loss:0.18892 | Acc:19.550\n",
      "Epoch 001: | Loss:0.18971 | Acc:19.650\n",
      "Epoch 001: | Loss:0.19032 | Acc:19.750\n",
      "Epoch 001: | Loss:0.19085 | Acc:19.875\n",
      "Epoch 001: | Loss:0.19163 | Acc:19.962\n",
      "Epoch 001: | Loss:0.19236 | Acc:20.050\n",
      "Epoch 001: | Loss:0.19329 | Acc:20.113\n",
      "Epoch 001: | Loss:0.19383 | Acc:20.238\n",
      "Epoch 001: | Loss:0.19450 | Acc:20.337\n",
      "Epoch 001: | Loss:0.19515 | Acc:20.438\n",
      "Epoch 001: | Loss:0.19582 | Acc:20.525\n",
      "Epoch 001: | Loss:0.19650 | Acc:20.637\n",
      "Epoch 001: | Loss:0.19726 | Acc:20.738\n",
      "Epoch 001: | Loss:0.19802 | Acc:20.812\n",
      "Epoch 001: | Loss:0.19880 | Acc:20.900\n",
      "Epoch 001: | Loss:0.19938 | Acc:21.012\n",
      "Epoch 001: | Loss:0.19993 | Acc:21.125\n",
      "Epoch 001: | Loss:0.20065 | Acc:21.212\n",
      "Epoch 001: | Loss:0.20135 | Acc:21.312\n",
      "Epoch 001: | Loss:0.20191 | Acc:21.425\n",
      "Epoch 001: | Loss:0.20265 | Acc:21.512\n",
      "Epoch 001: | Loss:0.20351 | Acc:21.587\n",
      "Epoch 001: | Loss:0.20400 | Acc:21.712\n",
      "Epoch 001: | Loss:0.20466 | Acc:21.800\n",
      "Epoch 001: | Loss:0.20531 | Acc:21.900\n",
      "Epoch 001: | Loss:0.20603 | Acc:21.988\n",
      "Epoch 001: | Loss:0.20660 | Acc:22.113\n",
      "Epoch 001: | Loss:0.20728 | Acc:22.225\n",
      "Epoch 001: | Loss:0.20793 | Acc:22.325\n",
      "Epoch 001: | Loss:0.20867 | Acc:22.413\n",
      "Epoch 001: | Loss:0.20958 | Acc:22.475\n",
      "Epoch 001: | Loss:0.21020 | Acc:22.587\n",
      "Epoch 001: | Loss:0.21076 | Acc:22.700\n",
      "Epoch 001: | Loss:0.21145 | Acc:22.812\n",
      "Epoch 001: | Loss:0.21215 | Acc:22.913\n",
      "Epoch 001: | Loss:0.21286 | Acc:23.012\n",
      "Epoch 001: | Loss:0.21353 | Acc:23.113\n",
      "Epoch 001: | Loss:0.21432 | Acc:23.200\n",
      "Epoch 001: | Loss:0.21499 | Acc:23.300\n",
      "Epoch 001: | Loss:0.21562 | Acc:23.400\n",
      "Epoch 001: | Loss:0.21625 | Acc:23.500\n",
      "Epoch 001: | Loss:0.21716 | Acc:23.575\n",
      "Epoch 001: | Loss:0.21814 | Acc:23.650\n",
      "Epoch 001: | Loss:0.21885 | Acc:23.750\n",
      "Epoch 001: | Loss:0.21965 | Acc:23.825\n",
      "Epoch 001: | Loss:0.22049 | Acc:23.913\n",
      "Epoch 001: | Loss:0.22115 | Acc:24.012\n",
      "Epoch 001: | Loss:0.22170 | Acc:24.125\n",
      "Epoch 001: | Loss:0.22261 | Acc:24.188\n",
      "Epoch 001: | Loss:0.22327 | Acc:24.275\n",
      "Epoch 001: | Loss:0.22407 | Acc:24.363\n",
      "Epoch 001: | Loss:0.22452 | Acc:24.488\n",
      "Epoch 001: | Loss:0.22528 | Acc:24.562\n",
      "Epoch 001: | Loss:0.22584 | Acc:24.663\n",
      "Epoch 001: | Loss:0.22664 | Acc:24.738\n",
      "Epoch 001: | Loss:0.22735 | Acc:24.825\n",
      "Epoch 001: | Loss:0.22789 | Acc:24.950\n",
      "Epoch 001: | Loss:0.22864 | Acc:25.025\n",
      "Epoch 001: | Loss:0.22937 | Acc:25.113\n",
      "Epoch 001: | Loss:0.22992 | Acc:25.225\n",
      "Epoch 001: | Loss:0.23051 | Acc:25.325\n",
      "Epoch 001: | Loss:0.23144 | Acc:25.400\n",
      "Epoch 001: | Loss:0.23207 | Acc:25.500\n",
      "Epoch 001: | Loss:0.23271 | Acc:25.613\n",
      "Epoch 001: | Loss:0.23324 | Acc:25.725\n",
      "Epoch 001: | Loss:0.23384 | Acc:25.825\n",
      "Epoch 001: | Loss:0.23436 | Acc:25.938\n",
      "Epoch 001: | Loss:0.23497 | Acc:26.050\n",
      "Epoch 001: | Loss:0.23554 | Acc:26.163\n",
      "Epoch 001: | Loss:0.23611 | Acc:26.262\n",
      "Epoch 001: | Loss:0.23676 | Acc:26.350\n",
      "Epoch 001: | Loss:0.23749 | Acc:26.425\n",
      "Epoch 001: | Loss:0.23808 | Acc:26.525\n",
      "Epoch 001: | Loss:0.23892 | Acc:26.600\n",
      "Epoch 001: | Loss:0.23965 | Acc:26.688\n",
      "Epoch 001: | Loss:0.24042 | Acc:26.762\n",
      "Epoch 001: | Loss:0.24096 | Acc:26.875\n",
      "Epoch 001: | Loss:0.24156 | Acc:26.988\n",
      "Epoch 001: | Loss:0.24229 | Acc:27.075\n",
      "Epoch 001: | Loss:0.24306 | Acc:27.163\n",
      "Epoch 001: | Loss:0.24376 | Acc:27.250\n",
      "Epoch 001: | Loss:0.24430 | Acc:27.350\n",
      "Epoch 001: | Loss:0.24502 | Acc:27.438\n",
      "Epoch 001: | Loss:0.24558 | Acc:27.550\n",
      "Epoch 001: | Loss:0.24629 | Acc:27.637\n",
      "Epoch 001: | Loss:0.24693 | Acc:27.738\n",
      "Epoch 001: | Loss:0.24758 | Acc:27.825\n",
      "Epoch 001: | Loss:0.24819 | Acc:27.925\n",
      "Epoch 001: | Loss:0.24885 | Acc:28.025\n",
      "Epoch 001: | Loss:0.24953 | Acc:28.113\n",
      "Epoch 001: | Loss:0.25001 | Acc:28.238\n",
      "Epoch 001: | Loss:0.25060 | Acc:28.337\n",
      "Epoch 001: | Loss:0.25120 | Acc:28.438\n",
      "Epoch 001: | Loss:0.25205 | Acc:28.525\n",
      "Epoch 001: | Loss:0.25259 | Acc:28.637\n",
      "Epoch 001: | Loss:0.25299 | Acc:28.750\n",
      "Epoch 001: | Loss:0.25368 | Acc:28.850\n",
      "Epoch 001: | Loss:0.25415 | Acc:28.975\n",
      "Epoch 001: | Loss:0.25481 | Acc:29.075\n",
      "Epoch 001: | Loss:0.25549 | Acc:29.175\n",
      "Epoch 001: | Loss:0.25611 | Acc:29.262\n",
      "Epoch 001: | Loss:0.25688 | Acc:29.337\n",
      "Epoch 001: | Loss:0.25740 | Acc:29.450\n",
      "Epoch 001: | Loss:0.25798 | Acc:29.550\n",
      "Epoch 001: | Loss:0.25847 | Acc:29.663\n",
      "Epoch 001: | Loss:0.25915 | Acc:29.750\n",
      "Epoch 001: | Loss:0.25987 | Acc:29.837\n",
      "Epoch 001: | Loss:0.26081 | Acc:29.913\n",
      "Epoch 001: | Loss:0.26156 | Acc:30.000\n",
      "Epoch 001: | Loss:0.26223 | Acc:30.087\n",
      "Epoch 001: | Loss:0.26296 | Acc:30.163\n",
      "Epoch 001: | Loss:0.26356 | Acc:30.262\n",
      "Epoch 001: | Loss:0.26403 | Acc:30.375\n",
      "Epoch 001: | Loss:0.26462 | Acc:30.475\n",
      "Epoch 001: | Loss:0.26513 | Acc:30.587\n",
      "Epoch 001: | Loss:0.26587 | Acc:30.688\n",
      "Epoch 001: | Loss:0.26664 | Acc:30.775\n",
      "Epoch 001: | Loss:0.26722 | Acc:30.875\n",
      "Epoch 001: | Loss:0.26772 | Acc:30.988\n",
      "Epoch 001: | Loss:0.26835 | Acc:31.075\n",
      "Epoch 001: | Loss:0.26915 | Acc:31.137\n",
      "Epoch 001: | Loss:0.26976 | Acc:31.238\n",
      "Epoch 001: | Loss:0.27041 | Acc:31.337\n",
      "Epoch 001: | Loss:0.27113 | Acc:31.425\n",
      "Epoch 001: | Loss:0.27221 | Acc:31.475\n",
      "Epoch 001: | Loss:0.27288 | Acc:31.575\n",
      "Epoch 001: | Loss:0.27336 | Acc:31.688\n",
      "Epoch 001: | Loss:0.27404 | Acc:31.775\n",
      "Epoch 001: | Loss:0.27484 | Acc:31.875\n",
      "Epoch 001: | Loss:0.27535 | Acc:31.988\n",
      "Epoch 001: | Loss:0.27581 | Acc:32.100\n",
      "Epoch 001: | Loss:0.27638 | Acc:32.200\n",
      "Epoch 001: | Loss:0.27693 | Acc:32.300\n",
      "Epoch 001: | Loss:0.27749 | Acc:32.400\n",
      "Epoch 001: | Loss:0.27791 | Acc:32.525\n",
      "Epoch 001: | Loss:0.27842 | Acc:32.638\n",
      "Epoch 001: | Loss:0.27913 | Acc:32.712\n",
      "Epoch 001: | Loss:0.27986 | Acc:32.788\n",
      "Epoch 001: | Loss:0.28054 | Acc:32.875\n",
      "Epoch 001: | Loss:0.28108 | Acc:32.975\n",
      "Epoch 001: | Loss:0.28170 | Acc:33.075\n",
      "Epoch 001: | Loss:0.28223 | Acc:33.188\n",
      "Epoch 001: | Loss:0.28318 | Acc:33.225\n",
      "Epoch 001: | Loss:0.28371 | Acc:33.337\n",
      "Epoch 001: | Loss:0.28442 | Acc:33.413\n",
      "Epoch 001: | Loss:0.28515 | Acc:33.487\n",
      "Epoch 001: | Loss:0.28598 | Acc:33.562\n",
      "Epoch 001: | Loss:0.28661 | Acc:33.650\n",
      "Epoch 001: | Loss:0.28744 | Acc:33.737\n",
      "Epoch 001: | Loss:0.28822 | Acc:33.825\n",
      "Epoch 001: | Loss:0.28889 | Acc:33.925\n",
      "Epoch 001: | Loss:0.28960 | Acc:34.000\n",
      "Epoch 001: | Loss:0.29016 | Acc:34.112\n",
      "Epoch 001: | Loss:0.29069 | Acc:34.225\n",
      "Epoch 001: | Loss:0.29120 | Acc:34.325\n",
      "Epoch 001: | Loss:0.29174 | Acc:34.438\n",
      "Epoch 001: | Loss:0.29219 | Acc:34.550\n",
      "Epoch 001: | Loss:0.29273 | Acc:34.638\n",
      "Epoch 001: | Loss:0.29326 | Acc:34.750\n",
      "Epoch 001: | Loss:0.29415 | Acc:34.825\n",
      "Epoch 001: | Loss:0.29465 | Acc:34.950\n",
      "Epoch 001: | Loss:0.29532 | Acc:35.050\n",
      "Epoch 001: | Loss:0.29594 | Acc:35.150\n",
      "Epoch 001: | Loss:0.29666 | Acc:35.250\n",
      "Epoch 001: | Loss:0.29720 | Acc:35.362\n",
      "Epoch 001: | Loss:0.29773 | Acc:35.475\n",
      "Epoch 001: | Loss:0.29825 | Acc:35.575\n",
      "Epoch 001: | Loss:0.29879 | Acc:35.688\n",
      "Epoch 001: | Loss:0.29947 | Acc:35.775\n",
      "Epoch 001: | Loss:0.29993 | Acc:35.900\n",
      "Epoch 001: | Loss:0.30055 | Acc:35.987\n",
      "Epoch 001: | Loss:0.30124 | Acc:36.075\n",
      "Epoch 001: | Loss:0.30197 | Acc:36.163\n",
      "Epoch 001: | Loss:0.30262 | Acc:36.275\n",
      "Epoch 001: | Loss:0.30323 | Acc:36.362\n",
      "Epoch 001: | Loss:0.30362 | Acc:36.487\n",
      "Epoch 001: | Loss:0.30432 | Acc:36.575\n",
      "Epoch 001: | Loss:0.30503 | Acc:36.663\n",
      "Epoch 001: | Loss:0.30553 | Acc:36.775\n",
      "Epoch 001: | Loss:0.30599 | Acc:36.888\n",
      "Epoch 001: | Loss:0.30653 | Acc:37.000\n",
      "Epoch 001: | Loss:0.30746 | Acc:37.087\n",
      "Epoch 001: | Loss:0.30794 | Acc:37.200\n",
      "Epoch 001: | Loss:0.30878 | Acc:37.275\n",
      "Epoch 001: | Loss:0.30939 | Acc:37.375\n",
      "Epoch 001: | Loss:0.30979 | Acc:37.500\n",
      "Epoch 001: | Loss:0.31029 | Acc:37.612\n",
      "Epoch 001: | Loss:0.31078 | Acc:37.725\n",
      "Epoch 001: | Loss:0.31134 | Acc:37.837\n",
      "Epoch 001: | Loss:0.31195 | Acc:37.925\n",
      "Epoch 001: | Loss:0.31289 | Acc:38.013\n",
      "Epoch 001: | Loss:0.31349 | Acc:38.112\n",
      "Epoch 001: | Loss:0.31428 | Acc:38.188\n",
      "Epoch 001: | Loss:0.31500 | Acc:38.263\n",
      "Epoch 001: | Loss:0.31565 | Acc:38.362\n",
      "Epoch 001: | Loss:0.31624 | Acc:38.475\n",
      "Epoch 001: | Loss:0.31688 | Acc:38.562\n",
      "Epoch 001: | Loss:0.31738 | Acc:38.663\n",
      "Epoch 001: | Loss:0.31778 | Acc:38.788\n",
      "Epoch 001: | Loss:0.31825 | Acc:38.900\n",
      "Epoch 001: | Loss:0.31861 | Acc:39.013\n",
      "Epoch 001: | Loss:0.31910 | Acc:39.125\n",
      "Epoch 001: | Loss:0.31992 | Acc:39.212\n",
      "Epoch 001: | Loss:0.32064 | Acc:39.300\n",
      "Epoch 001: | Loss:0.32116 | Acc:39.413\n",
      "Epoch 001: | Loss:0.32160 | Acc:39.513\n",
      "Epoch 001: | Loss:0.32214 | Acc:39.612\n",
      "Epoch 001: | Loss:0.32286 | Acc:39.700\n",
      "Epoch 001: | Loss:0.32333 | Acc:39.812\n",
      "Epoch 001: | Loss:0.32405 | Acc:39.913\n",
      "Epoch 001: | Loss:0.32445 | Acc:40.038\n",
      "Epoch 001: | Loss:0.32490 | Acc:40.163\n",
      "Epoch 001: | Loss:0.32563 | Acc:40.250\n",
      "Epoch 001: | Loss:0.32617 | Acc:40.350\n",
      "Epoch 001: | Loss:0.32667 | Acc:40.462\n",
      "Epoch 001: | Loss:0.32760 | Acc:40.550\n",
      "Epoch 001: | Loss:0.32833 | Acc:40.638\n",
      "Epoch 001: | Loss:0.32893 | Acc:40.737\n",
      "Epoch 001: | Loss:0.32949 | Acc:40.837\n",
      "Epoch 001: | Loss:0.33000 | Acc:40.938\n",
      "Epoch 001: | Loss:0.33054 | Acc:41.050\n",
      "Epoch 001: | Loss:0.33121 | Acc:41.138\n",
      "Epoch 001: | Loss:0.33216 | Acc:41.200\n",
      "Epoch 001: | Loss:0.33274 | Acc:41.312\n",
      "Epoch 001: | Loss:0.33322 | Acc:41.425\n",
      "Epoch 001: | Loss:0.33390 | Acc:41.525\n",
      "Epoch 001: | Loss:0.33436 | Acc:41.638\n",
      "Epoch 001: | Loss:0.33508 | Acc:41.725\n",
      "Epoch 001: | Loss:0.33609 | Acc:41.788\n",
      "Epoch 001: | Loss:0.33686 | Acc:41.862\n",
      "Epoch 001: | Loss:0.33741 | Acc:41.962\n",
      "Epoch 001: | Loss:0.33792 | Acc:42.087\n",
      "Epoch 001: | Loss:0.33849 | Acc:42.200\n",
      "Epoch 001: | Loss:0.33931 | Acc:42.288\n",
      "Epoch 001: | Loss:0.34007 | Acc:42.375\n",
      "Epoch 001: | Loss:0.34083 | Acc:42.462\n",
      "Epoch 001: | Loss:0.34143 | Acc:42.562\n",
      "Epoch 001: | Loss:0.34193 | Acc:42.663\n",
      "Epoch 001: | Loss:0.34250 | Acc:42.750\n",
      "Epoch 001: | Loss:0.34290 | Acc:42.862\n",
      "Epoch 001: | Loss:0.34367 | Acc:42.950\n",
      "Epoch 001: | Loss:0.34442 | Acc:43.025\n",
      "Epoch 001: | Loss:0.34521 | Acc:43.125\n",
      "Epoch 001: | Loss:0.34579 | Acc:43.212\n",
      "Epoch 001: | Loss:0.34646 | Acc:43.300\n",
      "Epoch 001: | Loss:0.34689 | Acc:43.425\n",
      "Epoch 001: | Loss:0.34745 | Acc:43.513\n",
      "Epoch 001: | Loss:0.34809 | Acc:43.625\n",
      "Epoch 001: | Loss:0.34880 | Acc:43.712\n",
      "Epoch 001: | Loss:0.34965 | Acc:43.800\n",
      "Epoch 001: | Loss:0.35038 | Acc:43.888\n",
      "Epoch 001: | Loss:0.35092 | Acc:43.987\n",
      "Epoch 001: | Loss:0.35138 | Acc:44.100\n",
      "Epoch 001: | Loss:0.35213 | Acc:44.200\n",
      "Epoch 001: | Loss:0.35332 | Acc:44.250\n",
      "Epoch 001: | Loss:0.35393 | Acc:44.337\n",
      "Epoch 001: | Loss:0.35435 | Acc:44.462\n",
      "Epoch 001: | Loss:0.35508 | Acc:44.550\n",
      "Epoch 001: | Loss:0.35559 | Acc:44.663\n",
      "Epoch 001: | Loss:0.35636 | Acc:44.750\n",
      "Epoch 001: | Loss:0.35692 | Acc:44.850\n",
      "Epoch 001: | Loss:0.35756 | Acc:44.950\n",
      "Epoch 001: | Loss:0.35834 | Acc:45.025\n",
      "Epoch 001: | Loss:0.35888 | Acc:45.125\n",
      "Epoch 001: | Loss:0.35967 | Acc:45.200\n",
      "Epoch 001: | Loss:0.36033 | Acc:45.288\n",
      "Epoch 001: | Loss:0.36117 | Acc:45.375\n",
      "Epoch 001: | Loss:0.36167 | Acc:45.487\n",
      "Epoch 001: | Loss:0.36228 | Acc:45.587\n",
      "Epoch 001: | Loss:0.36282 | Acc:45.700\n",
      "Epoch 001: | Loss:0.36329 | Acc:45.812\n",
      "Epoch 001: | Loss:0.36388 | Acc:45.913\n",
      "Epoch 001: | Loss:0.36421 | Acc:46.038\n",
      "Epoch 001: | Loss:0.36471 | Acc:46.150\n",
      "Epoch 001: | Loss:0.36549 | Acc:46.237\n",
      "Epoch 001: | Loss:0.36595 | Acc:46.350\n",
      "Epoch 001: | Loss:0.36638 | Acc:46.475\n",
      "Epoch 001: | Loss:0.36709 | Acc:46.562\n",
      "Epoch 001: | Loss:0.36749 | Acc:46.688\n",
      "Epoch 001: | Loss:0.36816 | Acc:46.775\n",
      "Epoch 001: | Loss:0.36874 | Acc:46.888\n",
      "Epoch 001: | Loss:0.36936 | Acc:46.975\n",
      "Epoch 001: | Loss:0.37014 | Acc:47.050\n",
      "Epoch 001: | Loss:0.37069 | Acc:47.163\n",
      "Epoch 001: | Loss:0.37108 | Acc:47.288\n",
      "Epoch 001: | Loss:0.37162 | Acc:47.400\n",
      "Epoch 001: | Loss:0.37229 | Acc:47.487\n",
      "Epoch 001: | Loss:0.37291 | Acc:47.587\n",
      "Epoch 001: | Loss:0.37332 | Acc:47.700\n",
      "Epoch 001: | Loss:0.37382 | Acc:47.812\n",
      "Epoch 001: | Loss:0.37446 | Acc:47.900\n",
      "Epoch 001: | Loss:0.37502 | Acc:48.000\n",
      "Epoch 001: | Loss:0.37552 | Acc:48.100\n",
      "Epoch 001: | Loss:0.37602 | Acc:48.212\n",
      "Epoch 001: | Loss:0.37652 | Acc:48.312\n",
      "Epoch 001: | Loss:0.37701 | Acc:48.413\n",
      "Epoch 001: | Loss:0.37758 | Acc:48.513\n",
      "Epoch 001: | Loss:0.37814 | Acc:48.625\n",
      "Epoch 001: | Loss:0.37860 | Acc:48.725\n",
      "Epoch 001: | Loss:0.37899 | Acc:48.837\n",
      "Epoch 001: | Loss:0.37942 | Acc:48.950\n",
      "Epoch 001: | Loss:0.38012 | Acc:49.025\n",
      "Epoch 001: | Loss:0.38069 | Acc:49.125\n",
      "Epoch 001: | Loss:0.38146 | Acc:49.200\n",
      "Epoch 001: | Loss:0.38215 | Acc:49.288\n",
      "Epoch 001: | Loss:0.38257 | Acc:49.400\n",
      "Epoch 001: | Loss:0.38315 | Acc:49.500\n",
      "Epoch 001: | Loss:0.38388 | Acc:49.587\n",
      "Epoch 001: | Loss:0.38490 | Acc:49.675\n",
      "Epoch 001: | Loss:0.38550 | Acc:49.775\n",
      "Epoch 001: | Loss:0.38613 | Acc:49.875\n",
      "Epoch 001: | Loss:0.38691 | Acc:49.962\n",
      "Epoch 001: | Loss:0.38735 | Acc:50.062\n",
      "Epoch 001: | Loss:0.38807 | Acc:50.150\n",
      "Epoch 001: | Loss:0.38854 | Acc:50.263\n",
      "Epoch 001: | Loss:0.38893 | Acc:50.375\n",
      "Epoch 001: | Loss:0.38941 | Acc:50.487\n",
      "Epoch 001: | Loss:0.39003 | Acc:50.587\n",
      "Epoch 001: | Loss:0.39045 | Acc:50.700\n",
      "Epoch 001: | Loss:0.39085 | Acc:50.825\n",
      "Epoch 001: | Loss:0.39141 | Acc:50.913\n",
      "Epoch 001: | Loss:0.39186 | Acc:51.025\n",
      "Epoch 001: | Loss:0.39247 | Acc:51.125\n",
      "Epoch 001: | Loss:0.39284 | Acc:51.250\n",
      "Epoch 001: | Loss:0.39356 | Acc:51.350\n",
      "Epoch 001: | Loss:0.39397 | Acc:51.462\n",
      "Epoch 001: | Loss:0.39441 | Acc:51.575\n",
      "Epoch 001: | Loss:0.39508 | Acc:51.663\n",
      "Epoch 001: | Loss:0.39547 | Acc:51.775\n",
      "Epoch 001: | Loss:0.39622 | Acc:51.850\n",
      "Epoch 001: | Loss:0.39699 | Acc:51.950\n",
      "Epoch 001: | Loss:0.39746 | Acc:52.038\n",
      "Epoch 001: | Loss:0.39796 | Acc:52.150\n",
      "Epoch 001: | Loss:0.39836 | Acc:52.263\n",
      "Epoch 001: | Loss:0.39907 | Acc:52.350\n",
      "Epoch 001: | Loss:0.39943 | Acc:52.462\n",
      "Epoch 001: | Loss:0.40010 | Acc:52.562\n",
      "Epoch 001: | Loss:0.40066 | Acc:52.650\n",
      "Epoch 001: | Loss:0.40136 | Acc:52.725\n",
      "Epoch 001: | Loss:0.40191 | Acc:52.825\n",
      "Epoch 001: | Loss:0.40229 | Acc:52.938\n",
      "Epoch 001: | Loss:0.40258 | Acc:53.062\n",
      "Epoch 001: | Loss:0.40344 | Acc:53.150\n",
      "Epoch 001: | Loss:0.40385 | Acc:53.263\n",
      "Epoch 001: | Loss:0.40426 | Acc:53.375\n",
      "Epoch 001: | Loss:0.40473 | Acc:53.487\n",
      "Epoch 001: | Loss:0.40511 | Acc:53.612\n",
      "Epoch 001: | Loss:0.40594 | Acc:53.700\n",
      "Epoch 001: | Loss:0.40672 | Acc:53.800\n",
      "Epoch 001: | Loss:0.40712 | Acc:53.925\n",
      "Epoch 001: | Loss:0.40773 | Acc:54.038\n",
      "Epoch 001: | Loss:0.40845 | Acc:54.125\n",
      "Epoch 001: | Loss:0.40933 | Acc:54.188\n",
      "Epoch 001: | Loss:0.40964 | Acc:54.312\n",
      "Epoch 001: | Loss:0.41023 | Acc:54.413\n",
      "Epoch 001: | Loss:0.41082 | Acc:54.513\n",
      "Epoch 001: | Loss:0.41167 | Acc:54.575\n",
      "Epoch 001: | Loss:0.41238 | Acc:54.663\n",
      "Epoch 001: | Loss:0.41294 | Acc:54.763\n",
      "Epoch 001: | Loss:0.41335 | Acc:54.875\n",
      "Epoch 001: | Loss:0.41405 | Acc:54.962\n",
      "Epoch 001: | Loss:0.41495 | Acc:55.038\n",
      "Epoch 001: | Loss:0.41573 | Acc:55.125\n",
      "Epoch 001: | Loss:0.41652 | Acc:55.212\n",
      "Epoch 001: | Loss:0.41713 | Acc:55.288\n",
      "Epoch 001: | Loss:0.41778 | Acc:55.388\n",
      "Epoch 001: | Loss:0.41900 | Acc:55.450\n",
      "Epoch 001: | Loss:0.41938 | Acc:55.575\n",
      "Epoch 001: | Loss:0.41985 | Acc:55.688\n",
      "Epoch 001: | Loss:0.42028 | Acc:55.800\n",
      "Epoch 001: | Loss:0.42084 | Acc:55.900\n",
      "Epoch 001: | Loss:0.42126 | Acc:56.013\n",
      "Epoch 001: | Loss:0.42176 | Acc:56.112\n",
      "Epoch 001: | Loss:0.42218 | Acc:56.225\n",
      "Epoch 001: | Loss:0.42274 | Acc:56.325\n",
      "Epoch 001: | Loss:0.42371 | Acc:56.388\n",
      "Epoch 001: | Loss:0.42420 | Acc:56.487\n",
      "Epoch 001: | Loss:0.42491 | Acc:56.575\n",
      "Epoch 001: | Loss:0.42558 | Acc:56.663\n",
      "Epoch 001: | Loss:0.42617 | Acc:56.763\n",
      "Epoch 001: | Loss:0.42699 | Acc:56.850\n",
      "Epoch 001: | Loss:0.42760 | Acc:56.962\n",
      "Epoch 001: | Loss:0.42830 | Acc:57.062\n",
      "Epoch 001: | Loss:0.42874 | Acc:57.175\n",
      "Epoch 001: | Loss:0.42945 | Acc:57.275\n",
      "Epoch 001: | Loss:0.42988 | Acc:57.388\n",
      "Epoch 001: | Loss:0.43043 | Acc:57.475\n",
      "Epoch 001: | Loss:0.43083 | Acc:57.587\n",
      "Epoch 001: | Loss:0.43166 | Acc:57.650\n",
      "Epoch 001: | Loss:0.43206 | Acc:57.763\n",
      "Epoch 001: | Loss:0.43288 | Acc:57.837\n",
      "Epoch 001: | Loss:0.43335 | Acc:57.950\n",
      "Epoch 001: | Loss:0.43372 | Acc:58.075\n",
      "Epoch 001: | Loss:0.43413 | Acc:58.175\n",
      "Epoch 001: | Loss:0.43474 | Acc:58.288\n",
      "Epoch 001: | Loss:0.43522 | Acc:58.388\n",
      "Epoch 001: | Loss:0.43578 | Acc:58.475\n",
      "Epoch 001: | Loss:0.43627 | Acc:58.587\n",
      "Epoch 001: | Loss:0.43698 | Acc:58.675\n",
      "Epoch 001: | Loss:0.43792 | Acc:58.750\n",
      "Epoch 001: | Loss:0.43854 | Acc:58.837\n",
      "Epoch 001: | Loss:0.43918 | Acc:58.938\n",
      "Epoch 001: | Loss:0.43957 | Acc:59.050\n",
      "Epoch 001: | Loss:0.44023 | Acc:59.125\n",
      "Epoch 001: | Loss:0.44085 | Acc:59.237\n",
      "Epoch 001: | Loss:0.44178 | Acc:59.312\n",
      "Epoch 001: | Loss:0.44230 | Acc:59.413\n",
      "Epoch 001: | Loss:0.44283 | Acc:59.513\n",
      "Epoch 001: | Loss:0.44349 | Acc:59.600\n",
      "Epoch 001: | Loss:0.44398 | Acc:59.688\n",
      "Epoch 001: | Loss:0.44457 | Acc:59.775\n",
      "Epoch 001: | Loss:0.44519 | Acc:59.875\n",
      "Epoch 001: | Loss:0.44568 | Acc:59.987\n",
      "Epoch 001: | Loss:0.44633 | Acc:60.075\n",
      "Epoch 001: | Loss:0.44685 | Acc:60.175\n",
      "Epoch 001: | Loss:0.44720 | Acc:60.300\n",
      "Epoch 001: | Loss:0.44764 | Acc:60.388\n",
      "Epoch 001: | Loss:0.44815 | Acc:60.462\n",
      "Epoch 001: | Loss:0.44880 | Acc:60.562\n",
      "Epoch 001: | Loss:0.44949 | Acc:60.650\n",
      "Epoch 001: | Loss:0.44990 | Acc:60.763\n",
      "Epoch 001: | Loss:0.45035 | Acc:60.875\n",
      "Epoch 001: | Loss:0.45104 | Acc:60.950\n",
      "Epoch 001: | Loss:0.45156 | Acc:61.050\n",
      "Epoch 001: | Loss:0.45209 | Acc:61.150\n",
      "Epoch 001: | Loss:0.45255 | Acc:61.263\n",
      "Epoch 001: | Loss:0.45315 | Acc:61.362\n",
      "Epoch 001: | Loss:0.45384 | Acc:61.438\n",
      "Epoch 001: | Loss:0.45446 | Acc:61.538\n",
      "Epoch 001: | Loss:0.45498 | Acc:61.650\n",
      "Epoch 001: | Loss:0.45591 | Acc:61.725\n",
      "Epoch 001: | Loss:0.45635 | Acc:61.837\n",
      "Epoch 001: | Loss:0.45669 | Acc:61.962\n",
      "Epoch 001: | Loss:0.45722 | Acc:62.050\n",
      "Epoch 001: | Loss:0.45787 | Acc:62.138\n",
      "Epoch 001: | Loss:0.45841 | Acc:62.237\n",
      "Epoch 001: | Loss:0.45899 | Acc:62.350\n",
      "Epoch 001: | Loss:0.45937 | Acc:62.462\n",
      "Epoch 001: | Loss:0.45973 | Acc:62.587\n",
      "Epoch 001: | Loss:0.46013 | Acc:62.712\n",
      "Epoch 001: | Loss:0.46073 | Acc:62.812\n",
      "Epoch 001: | Loss:0.46110 | Acc:62.925\n",
      "Epoch 001: | Loss:0.46150 | Acc:63.025\n",
      "Epoch 001: | Loss:0.46205 | Acc:63.125\n",
      "Epoch 001: | Loss:0.46247 | Acc:63.237\n",
      "Epoch 001: | Loss:0.46273 | Acc:63.362\n",
      "Epoch 001: | Loss:0.46316 | Acc:63.475\n",
      "Epoch 001: | Loss:0.46368 | Acc:63.587\n",
      "Epoch 001: | Loss:0.46430 | Acc:63.675\n",
      "Epoch 001: | Loss:0.46506 | Acc:63.763\n",
      "Epoch 001: | Loss:0.46640 | Acc:63.825\n",
      "Epoch 001: | Loss:0.46684 | Acc:63.938\n",
      "Epoch 001: | Loss:0.46722 | Acc:64.050\n",
      "Epoch 001: | Loss:0.46771 | Acc:64.150\n",
      "Epoch 001: | Loss:0.46822 | Acc:64.250\n",
      "Epoch 001: | Loss:0.46869 | Acc:64.362\n",
      "Epoch 001: | Loss:0.46896 | Acc:64.487\n",
      "Epoch 001: | Loss:0.46932 | Acc:64.612\n",
      "Epoch 001: | Loss:0.47004 | Acc:64.700\n",
      "Epoch 001: | Loss:0.47049 | Acc:64.812\n",
      "Epoch 001: | Loss:0.47102 | Acc:64.912\n",
      "Epoch 001: | Loss:0.47154 | Acc:65.025\n",
      "Epoch 001: | Loss:0.47203 | Acc:65.125\n",
      "Epoch 001: | Loss:0.47236 | Acc:65.237\n",
      "Epoch 001: | Loss:0.47265 | Acc:65.362\n",
      "Epoch 001: | Loss:0.47315 | Acc:65.463\n",
      "Epoch 001: | Loss:0.47392 | Acc:65.562\n",
      "Epoch 001: | Loss:0.47451 | Acc:65.662\n",
      "Epoch 001: | Loss:0.47518 | Acc:65.775\n",
      "Epoch 001: | Loss:0.47567 | Acc:65.875\n",
      "Epoch 001: | Loss:0.47635 | Acc:65.963\n",
      "Epoch 001: | Loss:0.47680 | Acc:66.062\n",
      "Epoch 001: | Loss:0.47712 | Acc:66.188\n",
      "Epoch 001: | Loss:0.47776 | Acc:66.287\n",
      "Epoch 001: | Loss:0.47854 | Acc:66.375\n",
      "Epoch 001: | Loss:0.47890 | Acc:66.500\n",
      "Epoch 001: | Loss:0.47935 | Acc:66.612\n",
      "Epoch 001: | Loss:0.47981 | Acc:66.725\n",
      "Epoch 001: | Loss:0.48059 | Acc:66.838\n",
      "Epoch 001: | Loss:0.48107 | Acc:66.938\n",
      "Epoch 001: | Loss:0.48152 | Acc:67.050\n",
      "Epoch 001: | Loss:0.48202 | Acc:67.162\n",
      "Epoch 001: | Loss:0.48234 | Acc:67.287\n",
      "Epoch 001: | Loss:0.48314 | Acc:67.375\n",
      "Epoch 001: | Loss:0.48399 | Acc:67.450\n",
      "Epoch 001: | Loss:0.48448 | Acc:67.550\n",
      "Epoch 001: | Loss:0.48517 | Acc:67.638\n",
      "Epoch 001: | Loss:0.48560 | Acc:67.750\n",
      "Epoch 001: | Loss:0.48595 | Acc:67.875\n",
      "Epoch 001: | Loss:0.48672 | Acc:67.975\n",
      "Epoch 001: | Loss:0.48742 | Acc:68.050\n",
      "Epoch 001: | Loss:0.48786 | Acc:68.150\n",
      "Epoch 001: | Loss:0.48876 | Acc:68.225\n",
      "Epoch 001: | Loss:0.48923 | Acc:68.338\n",
      "Epoch 001: | Loss:0.48973 | Acc:68.450\n",
      "Epoch 001: | Loss:0.49005 | Acc:68.562\n",
      "Epoch 001: | Loss:0.49070 | Acc:68.662\n",
      "Epoch 001: | Loss:0.49124 | Acc:68.763\n",
      "Epoch 001: | Loss:0.49181 | Acc:68.850\n",
      "Epoch 001: | Loss:0.49260 | Acc:68.950\n",
      "Epoch 001: | Loss:0.49338 | Acc:69.037\n",
      "Epoch 001: | Loss:0.49413 | Acc:69.100\n",
      "Epoch 001: | Loss:0.49452 | Acc:69.213\n",
      "Epoch 001: | Loss:0.49508 | Acc:69.300\n",
      "Epoch 001: | Loss:0.49548 | Acc:69.412\n",
      "Epoch 001: | Loss:0.49633 | Acc:69.487\n",
      "Epoch 001: | Loss:0.49710 | Acc:69.562\n",
      "Epoch 001: | Loss:0.49757 | Acc:69.662\n",
      "Epoch 001: | Loss:0.49798 | Acc:69.775\n",
      "Epoch 001: | Loss:0.49874 | Acc:69.875\n",
      "Epoch 001: | Loss:0.49932 | Acc:69.975\n",
      "Epoch 001: | Loss:0.49970 | Acc:70.075\n",
      "Epoch 001: | Loss:0.50042 | Acc:70.162\n",
      "Epoch 001: | Loss:0.50104 | Acc:70.263\n",
      "Epoch 001: | Loss:0.50149 | Acc:70.362\n",
      "Epoch 001: | Loss:0.50176 | Acc:70.487\n",
      "Epoch 001: | Loss:0.50248 | Acc:70.588\n",
      "Epoch 001: | Loss:0.50337 | Acc:70.688\n",
      "Epoch 001: | Loss:0.50379 | Acc:70.800\n",
      "Epoch 001: | Loss:0.50411 | Acc:70.925\n",
      "Epoch 001: | Loss:0.50482 | Acc:71.025\n",
      "Epoch 001: | Loss:0.50550 | Acc:71.112\n",
      "Epoch 001: | Loss:0.50594 | Acc:71.213\n",
      "Epoch 001: | Loss:0.50657 | Acc:71.312\n",
      "Epoch 001: | Loss:0.50740 | Acc:71.400\n",
      "Epoch 001: | Loss:0.50792 | Acc:71.513\n",
      "Epoch 001: | Loss:0.50850 | Acc:71.612\n",
      "Epoch 001: | Loss:0.50903 | Acc:71.700\n",
      "Epoch 001: | Loss:0.50963 | Acc:71.800\n",
      "Epoch 001: | Loss:0.51075 | Acc:71.875\n",
      "Epoch 001: | Loss:0.51138 | Acc:71.975\n",
      "Epoch 001: | Loss:0.51236 | Acc:72.050\n",
      "Epoch 001: | Loss:0.51297 | Acc:72.150\n",
      "Epoch 001: | Loss:0.51377 | Acc:72.250\n",
      "Epoch 001: | Loss:0.51445 | Acc:72.338\n",
      "Epoch 001: | Loss:0.51500 | Acc:72.450\n",
      "Epoch 001: | Loss:0.51536 | Acc:72.562\n",
      "Epoch 001: | Loss:0.51578 | Acc:72.675\n",
      "Epoch 001: | Loss:0.51609 | Acc:72.800\n",
      "Epoch 001: | Loss:0.51674 | Acc:72.888\n",
      "Epoch 001: | Loss:0.51737 | Acc:72.987\n",
      "Epoch 001: | Loss:0.51803 | Acc:73.088\n",
      "Epoch 001: | Loss:0.51862 | Acc:73.200\n",
      "Epoch 001: | Loss:0.51913 | Acc:73.300\n",
      "Epoch 001: | Loss:0.51972 | Acc:73.388\n",
      "Epoch 001: | Loss:0.52067 | Acc:73.475\n",
      "Epoch 001: | Loss:0.52160 | Acc:73.562\n",
      "Epoch 001: | Loss:0.52247 | Acc:73.638\n",
      "Epoch 001: | Loss:0.52308 | Acc:73.737\n",
      "Epoch 001: | Loss:0.52351 | Acc:73.838\n",
      "Epoch 001: | Loss:0.52381 | Acc:73.963\n",
      "Epoch 001: | Loss:0.52421 | Acc:74.062\n",
      "Epoch 001: | Loss:0.52485 | Acc:74.150\n",
      "Epoch 001: | Loss:0.52549 | Acc:74.263\n",
      "Epoch 001: | Loss:0.52587 | Acc:74.375\n",
      "Epoch 001: | Loss:0.52645 | Acc:74.475\n",
      "Epoch 001: | Loss:0.52691 | Acc:74.575\n",
      "Epoch 001: | Loss:0.52745 | Acc:74.675\n",
      "Epoch 001: | Loss:0.52802 | Acc:74.775\n",
      "Epoch 001: | Loss:0.52878 | Acc:74.875\n",
      "Epoch 001: | Loss:0.52902 | Acc:75.000\n",
      "Epoch 001: | Loss:0.52933 | Acc:75.112\n",
      "Epoch 001: | Loss:0.52984 | Acc:75.213\n",
      "Epoch 001: | Loss:0.53039 | Acc:75.312\n"
     ]
    }
   ],
   "source": [
    "train(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'torch==1.7.1'])\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    #loading the X_test and y_test data\n",
    "    with open(f'{data_path}/test_data', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    #defining neural network architecture\n",
    "    class binaryClassification(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(binaryClassification, self).__init__()\n",
    "            #number of input features is 12\n",
    "            self.layer_1 = nn.Linear(12, 16)\n",
    "            self.layer_2 = nn.Linear(16, 8)\n",
    "            self.layer_out = nn.Linear(8, 1) \n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "            self.batchnorm2 = nn.BatchNorm1d(8)       \n",
    "        #feed forward network\n",
    "        def forward(self, inputs):\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.relu(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            return x\n",
    "    \n",
    "    #loading model\n",
    "    classifier = binaryClassification()\n",
    "    classifier.load_state_dict(torch.load(f'{data_path}/pyclassifier.pt'))\n",
    "    \n",
    "     #test data\n",
    "    class testData(Dataset):\n",
    "        def __init__(self, X_data):\n",
    "            self.X_data = X_data\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "            return self.X_data[index]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X_data)\n",
    "\n",
    "    test_data = testData(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1, num_workers=0)\n",
    "    \n",
    "    #test model\n",
    "    y_pred_list = []\n",
    "    classifier.eval()\n",
    "    count = 0\n",
    "    #ensures no back propagation during testing and reduces memeory usage\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            y_test_pred = classifier(X_batch)\n",
    "            y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_pred_tag = torch.round(y_test_pred)\n",
    "\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_pred_list = [i.squeeze().tolist() for i in y_pred_list] \n",
    "        y_pred_list = [bool(i) for i in y_pred_list]\n",
    "    \n",
    "    with open(f'{data_path}/result.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {}, Actual: {} \".format(y_pred_list,y_test.astype(np.bool)))\n",
    "    \n",
    "    print('Prediction has be saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction has be saved successfully!\n"
     ]
    }
   ],
   "source": [
    "predict(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_op = kfp.components.create_component_from_func(preprocessing,base_image=\"python:3.7.1\")\n",
    "train_op = kfp.components.create_component_from_func(train, base_image=\"pytorch/pytorch:latest\")\n",
    "predict_op = kfp.components.create_component_from_func(predict, base_image=\"pytorch/pytorch:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"Churn Pipeline\", description=\"Performs Preprocessing, training and prediction of churn rate\")\n",
    "\n",
    "def churn_prediction(data_path:str):\n",
    "    volume_op = dsl.VolumeOp(\n",
    "    name=\"data_volume\",\n",
    "    resource_name=\"data-volume\",\n",
    "    size=\"1Gi\",\n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "\n",
    "    # Create preprocess components.\n",
    "    preprocess_container = preprocess_op(data_path).add_pvolumes({data_path: volume_op.volume})\n",
    "    # Create train component.\n",
    "    train_container = train_op(data_path).add_pvolumes({data_path: preprocess_container.pvolume})\n",
    "    # Create prediction component.\n",
    "    predict_container = predict_op(data_path).add_pvolumes({data_path: train_container.pvolume})\n",
    "\n",
    "    # Print the result of the prediction\n",
    "    result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: predict_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/result.txt']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/dsl/_container_op.py:1039: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/c0683194-036d-4dd5-98b2-e2dad91a03f6\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/ad7a976c-0c8d-4500-ade3-3d83fbfef6bb\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = '/home/jovyan/test/'\n",
    "\n",
    "\n",
    "pipeline_func = churn_prediction\n",
    "\n",
    "experiment_name = 'churn_prediction_pytorch'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,'{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
