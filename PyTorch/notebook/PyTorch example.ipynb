{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer retention Analytics\n",
    "\n",
    "##### Business Problem:\n",
    "An International bank collected a sample data of 10,000 customers. They observed that some of their custimers are leaving or churning in an unusually high rate and they want to find understand and access why their customers keep leaving. They have hiered you as a data scientist to look into the data to give then some insight.\n",
    "\n",
    "##### Data:\n",
    "The data was collected within the last 5 months, the feautures include the customer's name, creditscore, geography, gendey,age,tenure,balnce, number of products(accounts),credit card status(whether they have one or not), estimated salary, activity status(active member or not) and if the customer remained with them.\n",
    "\n",
    "#### Goal:\n",
    "Your goal is to create a geodemographic segmentation  to identify which of the customers have the highest risk of leaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on a banking dataset to detect churn activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/AdeloreSimiloluwa/Artificial-Neural-Network/master/data/Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of the target classes\n",
    "sns.countplot(x='Exited', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data we have more information on the customers that stayed at the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for datatype of each column\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some columns that are not needed\n",
    "data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the unique values in Geography column\n",
    "data['Geography'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data features\n",
    "X = data.iloc[:,:-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target data\n",
    "y = data.iloc[:,-1:]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the categorical columns\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder()\n",
    "X['Gender'] = le.fit_transform(X['Gender'])\n",
    "geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "#getting feature name after onehotencoding\n",
    "geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "#merging geo_df with the main data\n",
    "X = X.join(geo_df)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the old columns after encoding\n",
    "X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"X_train\":X_train, \"X_test\":X_test, \"y_train\":y_train, \"y_test\":y_test}\n",
    "for i in data_dict:\n",
    "    print(\"The shape of {} is {}\".format(i,data_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting model hyper-parameters\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE =10\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train data\n",
    "class trainData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = trainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "class testData(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.X_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining dataloader to read dataset class in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        #number of input features is 12\n",
    "        self.layer_1 = nn.Linear(12, 16)\n",
    "        self.layer_2 = nn.Linear(16, 8)\n",
    "        self.layer_out = nn.Linear(8, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(8)\n",
    "        \n",
    "    #feed forward network\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer_1): Linear(in_features=12, out_features=16, bias=True)\n",
      "  (layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (layer_out): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#initializing optimizer and loss\n",
    "classifier = binaryClassification()\n",
    "classifier.to(device)\n",
    "print(classifier)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate accuracy\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    \n",
    "    results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = results_sum/y_test.shape[0]\n",
    "    acc =torch.round(acc*100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss:0.00092 | Acc:0.062\n",
      "Epoch 001: | Loss:0.00177 | Acc:0.138\n",
      "Epoch 001: | Loss:0.00266 | Acc:0.212\n",
      "Epoch 001: | Loss:0.00358 | Acc:0.263\n",
      "Epoch 001: | Loss:0.00459 | Acc:0.300\n",
      "Epoch 001: | Loss:0.00545 | Acc:0.375\n",
      "Epoch 001: | Loss:0.00633 | Acc:0.425\n",
      "Epoch 001: | Loss:0.00720 | Acc:0.500\n",
      "Epoch 001: | Loss:0.00815 | Acc:0.537\n",
      "Epoch 001: | Loss:0.00907 | Acc:0.588\n",
      "Epoch 001: | Loss:0.00992 | Acc:0.675\n",
      "Epoch 001: | Loss:0.01081 | Acc:0.750\n",
      "Epoch 001: | Loss:0.01171 | Acc:0.812\n",
      "Epoch 001: | Loss:0.01257 | Acc:0.887\n",
      "Epoch 001: | Loss:0.01339 | Acc:0.975\n",
      "Epoch 001: | Loss:0.01442 | Acc:1.012\n",
      "Epoch 001: | Loss:0.01533 | Acc:1.050\n",
      "Epoch 001: | Loss:0.01626 | Acc:1.087\n",
      "Epoch 001: | Loss:0.01720 | Acc:1.137\n",
      "Epoch 001: | Loss:0.01805 | Acc:1.212\n",
      "Epoch 001: | Loss:0.01898 | Acc:1.262\n",
      "Epoch 001: | Loss:0.01984 | Acc:1.350\n",
      "Epoch 001: | Loss:0.02083 | Acc:1.387\n",
      "Epoch 001: | Loss:0.02161 | Acc:1.462\n",
      "Epoch 001: | Loss:0.02261 | Acc:1.500\n",
      "Epoch 001: | Loss:0.02346 | Acc:1.587\n",
      "Epoch 001: | Loss:0.02442 | Acc:1.637\n",
      "Epoch 001: | Loss:0.02527 | Acc:1.700\n",
      "Epoch 001: | Loss:0.02616 | Acc:1.750\n",
      "Epoch 001: | Loss:0.02699 | Acc:1.812\n",
      "Epoch 001: | Loss:0.02782 | Acc:1.900\n",
      "Epoch 001: | Loss:0.02857 | Acc:2.000\n",
      "Epoch 001: | Loss:0.02945 | Acc:2.062\n",
      "Epoch 001: | Loss:0.03037 | Acc:2.125\n",
      "Epoch 001: | Loss:0.03127 | Acc:2.188\n",
      "Epoch 001: | Loss:0.03212 | Acc:2.263\n",
      "Epoch 001: | Loss:0.03302 | Acc:2.312\n",
      "Epoch 001: | Loss:0.03386 | Acc:2.375\n",
      "Epoch 001: | Loss:0.03460 | Acc:2.462\n",
      "Epoch 001: | Loss:0.03539 | Acc:2.550\n",
      "Epoch 001: | Loss:0.03631 | Acc:2.600\n",
      "Epoch 001: | Loss:0.03710 | Acc:2.688\n",
      "Epoch 001: | Loss:0.03797 | Acc:2.737\n",
      "Epoch 001: | Loss:0.03900 | Acc:2.775\n",
      "Epoch 001: | Loss:0.03983 | Acc:2.850\n",
      "Epoch 001: | Loss:0.04076 | Acc:2.913\n",
      "Epoch 001: | Loss:0.04167 | Acc:2.975\n",
      "Epoch 001: | Loss:0.04247 | Acc:3.062\n",
      "Epoch 001: | Loss:0.04335 | Acc:3.100\n",
      "Epoch 001: | Loss:0.04422 | Acc:3.163\n",
      "Epoch 001: | Loss:0.04502 | Acc:3.250\n",
      "Epoch 001: | Loss:0.04591 | Acc:3.312\n",
      "Epoch 001: | Loss:0.04675 | Acc:3.388\n",
      "Epoch 001: | Loss:0.04755 | Acc:3.475\n",
      "Epoch 001: | Loss:0.04840 | Acc:3.538\n",
      "Epoch 001: | Loss:0.04940 | Acc:3.600\n",
      "Epoch 001: | Loss:0.05030 | Acc:3.650\n",
      "Epoch 001: | Loss:0.05112 | Acc:3.737\n",
      "Epoch 001: | Loss:0.05190 | Acc:3.837\n",
      "Epoch 001: | Loss:0.05268 | Acc:3.938\n",
      "Epoch 001: | Loss:0.05347 | Acc:4.013\n",
      "Epoch 001: | Loss:0.05419 | Acc:4.100\n",
      "Epoch 001: | Loss:0.05492 | Acc:4.188\n",
      "Epoch 001: | Loss:0.05585 | Acc:4.250\n",
      "Epoch 001: | Loss:0.05672 | Acc:4.312\n",
      "Epoch 001: | Loss:0.05770 | Acc:4.350\n",
      "Epoch 001: | Loss:0.05845 | Acc:4.438\n",
      "Epoch 001: | Loss:0.05917 | Acc:4.537\n",
      "Epoch 001: | Loss:0.05994 | Acc:4.625\n",
      "Epoch 001: | Loss:0.06070 | Acc:4.725\n",
      "Epoch 001: | Loss:0.06146 | Acc:4.812\n",
      "Epoch 001: | Loss:0.06238 | Acc:4.850\n",
      "Epoch 001: | Loss:0.06313 | Acc:4.938\n",
      "Epoch 001: | Loss:0.06396 | Acc:5.025\n",
      "Epoch 001: | Loss:0.06474 | Acc:5.100\n",
      "Epoch 001: | Loss:0.06580 | Acc:5.138\n",
      "Epoch 001: | Loss:0.06663 | Acc:5.213\n",
      "Epoch 001: | Loss:0.06731 | Acc:5.325\n",
      "Epoch 001: | Loss:0.06812 | Acc:5.412\n",
      "Epoch 001: | Loss:0.06887 | Acc:5.500\n",
      "Epoch 001: | Loss:0.06964 | Acc:5.575\n",
      "Epoch 001: | Loss:0.07043 | Acc:5.662\n",
      "Epoch 001: | Loss:0.07118 | Acc:5.750\n",
      "Epoch 001: | Loss:0.07208 | Acc:5.812\n",
      "Epoch 001: | Loss:0.07279 | Acc:5.900\n",
      "Epoch 001: | Loss:0.07352 | Acc:5.987\n",
      "Epoch 001: | Loss:0.07443 | Acc:6.062\n",
      "Epoch 001: | Loss:0.07525 | Acc:6.150\n",
      "Epoch 001: | Loss:0.07614 | Acc:6.213\n",
      "Epoch 001: | Loss:0.07694 | Acc:6.287\n",
      "Epoch 001: | Loss:0.07777 | Acc:6.338\n",
      "Epoch 001: | Loss:0.07858 | Acc:6.412\n",
      "Epoch 001: | Loss:0.07927 | Acc:6.500\n",
      "Epoch 001: | Loss:0.08018 | Acc:6.588\n",
      "Epoch 001: | Loss:0.08095 | Acc:6.662\n",
      "Epoch 001: | Loss:0.08175 | Acc:6.737\n",
      "Epoch 001: | Loss:0.08255 | Acc:6.800\n",
      "Epoch 001: | Loss:0.08334 | Acc:6.875\n",
      "Epoch 001: | Loss:0.08416 | Acc:6.963\n",
      "Epoch 001: | Loss:0.08481 | Acc:7.075\n",
      "Epoch 001: | Loss:0.08562 | Acc:7.162\n",
      "Epoch 001: | Loss:0.08648 | Acc:7.213\n",
      "Epoch 001: | Loss:0.08728 | Acc:7.263\n",
      "Epoch 001: | Loss:0.08816 | Acc:7.338\n",
      "Epoch 001: | Loss:0.08895 | Acc:7.425\n",
      "Epoch 001: | Loss:0.08971 | Acc:7.513\n",
      "Epoch 001: | Loss:0.09045 | Acc:7.600\n",
      "Epoch 001: | Loss:0.09136 | Acc:7.662\n",
      "Epoch 001: | Loss:0.09210 | Acc:7.750\n",
      "Epoch 001: | Loss:0.09282 | Acc:7.850\n",
      "Epoch 001: | Loss:0.09363 | Acc:7.925\n",
      "Epoch 001: | Loss:0.09446 | Acc:7.975\n",
      "Epoch 001: | Loss:0.09534 | Acc:8.025\n",
      "Epoch 001: | Loss:0.09609 | Acc:8.125\n",
      "Epoch 001: | Loss:0.09682 | Acc:8.213\n",
      "Epoch 001: | Loss:0.09759 | Acc:8.287\n",
      "Epoch 001: | Loss:0.09823 | Acc:8.387\n",
      "Epoch 001: | Loss:0.09897 | Acc:8.463\n",
      "Epoch 001: | Loss:0.09974 | Acc:8.550\n",
      "Epoch 001: | Loss:0.10051 | Acc:8.625\n",
      "Epoch 001: | Loss:0.10126 | Acc:8.725\n",
      "Epoch 001: | Loss:0.10209 | Acc:8.812\n",
      "Epoch 001: | Loss:0.10297 | Acc:8.863\n",
      "Epoch 001: | Loss:0.10370 | Acc:8.963\n",
      "Epoch 001: | Loss:0.10458 | Acc:9.037\n",
      "Epoch 001: | Loss:0.10532 | Acc:9.125\n",
      "Epoch 001: | Loss:0.10617 | Acc:9.225\n",
      "Epoch 001: | Loss:0.10685 | Acc:9.338\n",
      "Epoch 001: | Loss:0.10761 | Acc:9.412\n",
      "Epoch 001: | Loss:0.10838 | Acc:9.500\n",
      "Epoch 001: | Loss:0.10913 | Acc:9.588\n",
      "Epoch 001: | Loss:0.10991 | Acc:9.688\n",
      "Epoch 001: | Loss:0.11070 | Acc:9.775\n",
      "Epoch 001: | Loss:0.11140 | Acc:9.875\n",
      "Epoch 001: | Loss:0.11210 | Acc:9.988\n",
      "Epoch 001: | Loss:0.11297 | Acc:10.075\n",
      "Epoch 001: | Loss:0.11370 | Acc:10.162\n",
      "Epoch 001: | Loss:0.11448 | Acc:10.225\n",
      "Epoch 001: | Loss:0.11517 | Acc:10.338\n",
      "Epoch 001: | Loss:0.11597 | Acc:10.438\n",
      "Epoch 001: | Loss:0.11668 | Acc:10.537\n",
      "Epoch 001: | Loss:0.11738 | Acc:10.637\n",
      "Epoch 001: | Loss:0.11821 | Acc:10.725\n",
      "Epoch 001: | Loss:0.11890 | Acc:10.825\n",
      "Epoch 001: | Loss:0.11976 | Acc:10.900\n",
      "Epoch 001: | Loss:0.12057 | Acc:10.988\n",
      "Epoch 001: | Loss:0.12114 | Acc:11.100\n",
      "Epoch 001: | Loss:0.12215 | Acc:11.150\n",
      "Epoch 001: | Loss:0.12289 | Acc:11.250\n",
      "Epoch 001: | Loss:0.12375 | Acc:11.312\n",
      "Epoch 001: | Loss:0.12450 | Acc:11.387\n",
      "Epoch 001: | Loss:0.12525 | Acc:11.463\n",
      "Epoch 001: | Loss:0.12600 | Acc:11.550\n",
      "Epoch 001: | Loss:0.12669 | Acc:11.637\n",
      "Epoch 001: | Loss:0.12762 | Acc:11.725\n",
      "Epoch 001: | Loss:0.12836 | Acc:11.800\n",
      "Epoch 001: | Loss:0.12921 | Acc:11.875\n",
      "Epoch 001: | Loss:0.13006 | Acc:11.975\n",
      "Epoch 001: | Loss:0.13085 | Acc:12.062\n",
      "Epoch 001: | Loss:0.13159 | Acc:12.150\n",
      "Epoch 001: | Loss:0.13231 | Acc:12.225\n",
      "Epoch 001: | Loss:0.13304 | Acc:12.312\n",
      "Epoch 001: | Loss:0.13370 | Acc:12.425\n",
      "Epoch 001: | Loss:0.13441 | Acc:12.525\n",
      "Epoch 001: | Loss:0.13518 | Acc:12.600\n",
      "Epoch 001: | Loss:0.13588 | Acc:12.688\n",
      "Epoch 001: | Loss:0.13657 | Acc:12.787\n",
      "Epoch 001: | Loss:0.13736 | Acc:12.863\n",
      "Epoch 001: | Loss:0.13809 | Acc:12.963\n",
      "Epoch 001: | Loss:0.13879 | Acc:13.075\n",
      "Epoch 001: | Loss:0.13949 | Acc:13.175\n",
      "Epoch 001: | Loss:0.14020 | Acc:13.275\n",
      "Epoch 001: | Loss:0.14107 | Acc:13.325\n",
      "Epoch 001: | Loss:0.14208 | Acc:13.387\n",
      "Epoch 001: | Loss:0.14281 | Acc:13.488\n",
      "Epoch 001: | Loss:0.14348 | Acc:13.588\n",
      "Epoch 001: | Loss:0.14440 | Acc:13.662\n",
      "Epoch 001: | Loss:0.14510 | Acc:13.775\n",
      "Epoch 001: | Loss:0.14602 | Acc:13.850\n",
      "Epoch 001: | Loss:0.14667 | Acc:13.963\n",
      "Epoch 001: | Loss:0.14738 | Acc:14.037\n",
      "Epoch 001: | Loss:0.14827 | Acc:14.125\n",
      "Epoch 001: | Loss:0.14894 | Acc:14.225\n",
      "Epoch 001: | Loss:0.14960 | Acc:14.325\n",
      "Epoch 001: | Loss:0.15036 | Acc:14.412\n",
      "Epoch 001: | Loss:0.15102 | Acc:14.525\n",
      "Epoch 001: | Loss:0.15173 | Acc:14.625\n",
      "Epoch 001: | Loss:0.15242 | Acc:14.725\n",
      "Epoch 001: | Loss:0.15308 | Acc:14.825\n",
      "Epoch 001: | Loss:0.15379 | Acc:14.912\n",
      "Epoch 001: | Loss:0.15439 | Acc:15.037\n",
      "Epoch 001: | Loss:0.15508 | Acc:15.137\n",
      "Epoch 001: | Loss:0.15599 | Acc:15.225\n",
      "Epoch 001: | Loss:0.15687 | Acc:15.312\n",
      "Epoch 001: | Loss:0.15750 | Acc:15.412\n",
      "Epoch 001: | Loss:0.15821 | Acc:15.512\n",
      "Epoch 001: | Loss:0.15889 | Acc:15.600\n",
      "Epoch 001: | Loss:0.15953 | Acc:15.700\n",
      "Epoch 001: | Loss:0.16024 | Acc:15.800\n",
      "Epoch 001: | Loss:0.16093 | Acc:15.912\n",
      "Epoch 001: | Loss:0.16175 | Acc:15.988\n",
      "Epoch 001: | Loss:0.16254 | Acc:16.075\n",
      "Epoch 001: | Loss:0.16325 | Acc:16.188\n",
      "Epoch 001: | Loss:0.16420 | Acc:16.238\n",
      "Epoch 001: | Loss:0.16478 | Acc:16.337\n",
      "Epoch 001: | Loss:0.16545 | Acc:16.438\n",
      "Epoch 001: | Loss:0.16613 | Acc:16.525\n",
      "Epoch 001: | Loss:0.16676 | Acc:16.637\n",
      "Epoch 001: | Loss:0.16744 | Acc:16.738\n",
      "Epoch 001: | Loss:0.16825 | Acc:16.825\n",
      "Epoch 001: | Loss:0.16884 | Acc:16.925\n",
      "Epoch 001: | Loss:0.16952 | Acc:17.038\n",
      "Epoch 001: | Loss:0.17017 | Acc:17.137\n",
      "Epoch 001: | Loss:0.17097 | Acc:17.200\n",
      "Epoch 001: | Loss:0.17182 | Acc:17.288\n",
      "Epoch 001: | Loss:0.17251 | Acc:17.375\n",
      "Epoch 001: | Loss:0.17328 | Acc:17.475\n",
      "Epoch 001: | Loss:0.17400 | Acc:17.575\n",
      "Epoch 001: | Loss:0.17466 | Acc:17.675\n",
      "Epoch 001: | Loss:0.17550 | Acc:17.750\n",
      "Epoch 001: | Loss:0.17621 | Acc:17.837\n",
      "Epoch 001: | Loss:0.17689 | Acc:17.925\n",
      "Epoch 001: | Loss:0.17760 | Acc:18.025\n",
      "Epoch 001: | Loss:0.17853 | Acc:18.087\n",
      "Epoch 001: | Loss:0.17911 | Acc:18.188\n",
      "Epoch 001: | Loss:0.18009 | Acc:18.250\n",
      "Epoch 001: | Loss:0.18073 | Acc:18.363\n",
      "Epoch 001: | Loss:0.18129 | Acc:18.475\n",
      "Epoch 001: | Loss:0.18187 | Acc:18.575\n",
      "Epoch 001: | Loss:0.18267 | Acc:18.625\n",
      "Epoch 001: | Loss:0.18340 | Acc:18.712\n",
      "Epoch 001: | Loss:0.18417 | Acc:18.788\n",
      "Epoch 001: | Loss:0.18492 | Acc:18.863\n",
      "Epoch 001: | Loss:0.18551 | Acc:18.962\n",
      "Epoch 001: | Loss:0.18633 | Acc:19.062\n",
      "Epoch 001: | Loss:0.18703 | Acc:19.163\n",
      "Epoch 001: | Loss:0.18753 | Acc:19.288\n",
      "Epoch 001: | Loss:0.18829 | Acc:19.387\n",
      "Epoch 001: | Loss:0.18889 | Acc:19.488\n",
      "Epoch 001: | Loss:0.18964 | Acc:19.575\n",
      "Epoch 001: | Loss:0.19045 | Acc:19.663\n",
      "Epoch 001: | Loss:0.19109 | Acc:19.775\n",
      "Epoch 001: | Loss:0.19170 | Acc:19.887\n",
      "Epoch 001: | Loss:0.19230 | Acc:20.000\n",
      "Epoch 001: | Loss:0.19298 | Acc:20.100\n",
      "Epoch 001: | Loss:0.19369 | Acc:20.188\n",
      "Epoch 001: | Loss:0.19422 | Acc:20.312\n",
      "Epoch 001: | Loss:0.19490 | Acc:20.413\n",
      "Epoch 001: | Loss:0.19574 | Acc:20.500\n",
      "Epoch 001: | Loss:0.19638 | Acc:20.587\n",
      "Epoch 001: | Loss:0.19697 | Acc:20.712\n",
      "Epoch 001: | Loss:0.19764 | Acc:20.812\n",
      "Epoch 001: | Loss:0.19838 | Acc:20.900\n",
      "Epoch 001: | Loss:0.19907 | Acc:20.988\n",
      "Epoch 001: | Loss:0.19962 | Acc:21.100\n",
      "Epoch 001: | Loss:0.20034 | Acc:21.175\n",
      "Epoch 001: | Loss:0.20114 | Acc:21.250\n",
      "Epoch 001: | Loss:0.20168 | Acc:21.363\n",
      "Epoch 001: | Loss:0.20256 | Acc:21.438\n",
      "Epoch 001: | Loss:0.20315 | Acc:21.525\n",
      "Epoch 001: | Loss:0.20373 | Acc:21.637\n",
      "Epoch 001: | Loss:0.20464 | Acc:21.725\n",
      "Epoch 001: | Loss:0.20526 | Acc:21.825\n",
      "Epoch 001: | Loss:0.20588 | Acc:21.925\n",
      "Epoch 001: | Loss:0.20654 | Acc:22.012\n",
      "Epoch 001: | Loss:0.20713 | Acc:22.100\n",
      "Epoch 001: | Loss:0.20787 | Acc:22.188\n",
      "Epoch 001: | Loss:0.20846 | Acc:22.300\n",
      "Epoch 001: | Loss:0.20911 | Acc:22.400\n",
      "Epoch 001: | Loss:0.21011 | Acc:22.462\n",
      "Epoch 001: | Loss:0.21073 | Acc:22.562\n",
      "Epoch 001: | Loss:0.21149 | Acc:22.625\n",
      "Epoch 001: | Loss:0.21223 | Acc:22.700\n",
      "Epoch 001: | Loss:0.21287 | Acc:22.800\n",
      "Epoch 001: | Loss:0.21345 | Acc:22.913\n",
      "Epoch 001: | Loss:0.21401 | Acc:23.025\n",
      "Epoch 001: | Loss:0.21461 | Acc:23.125\n",
      "Epoch 001: | Loss:0.21524 | Acc:23.225\n",
      "Epoch 001: | Loss:0.21593 | Acc:23.312\n",
      "Epoch 001: | Loss:0.21661 | Acc:23.413\n",
      "Epoch 001: | Loss:0.21721 | Acc:23.512\n",
      "Epoch 001: | Loss:0.21791 | Acc:23.600\n",
      "Epoch 001: | Loss:0.21847 | Acc:23.712\n",
      "Epoch 001: | Loss:0.21914 | Acc:23.812\n",
      "Epoch 001: | Loss:0.21979 | Acc:23.900\n",
      "Epoch 001: | Loss:0.22065 | Acc:23.962\n",
      "Epoch 001: | Loss:0.22132 | Acc:24.050\n",
      "Epoch 001: | Loss:0.22195 | Acc:24.125\n",
      "Epoch 001: | Loss:0.22254 | Acc:24.225\n",
      "Epoch 001: | Loss:0.22335 | Acc:24.300\n",
      "Epoch 001: | Loss:0.22391 | Acc:24.400\n",
      "Epoch 001: | Loss:0.22459 | Acc:24.512\n",
      "Epoch 001: | Loss:0.22507 | Acc:24.637\n",
      "Epoch 001: | Loss:0.22582 | Acc:24.725\n",
      "Epoch 001: | Loss:0.22648 | Acc:24.800\n",
      "Epoch 001: | Loss:0.22717 | Acc:24.887\n",
      "Epoch 001: | Loss:0.22773 | Acc:25.000\n",
      "Epoch 001: | Loss:0.22826 | Acc:25.125\n",
      "Epoch 001: | Loss:0.22884 | Acc:25.225\n",
      "Epoch 001: | Loss:0.22949 | Acc:25.312\n",
      "Epoch 001: | Loss:0.23006 | Acc:25.425\n",
      "Epoch 001: | Loss:0.23071 | Acc:25.525\n",
      "Epoch 001: | Loss:0.23134 | Acc:25.613\n",
      "Epoch 001: | Loss:0.23196 | Acc:25.725\n",
      "Epoch 001: | Loss:0.23247 | Acc:25.850\n",
      "Epoch 001: | Loss:0.23319 | Acc:25.938\n",
      "Epoch 001: | Loss:0.23410 | Acc:26.000\n",
      "Epoch 001: | Loss:0.23472 | Acc:26.113\n",
      "Epoch 001: | Loss:0.23521 | Acc:26.225\n",
      "Epoch 001: | Loss:0.23586 | Acc:26.312\n",
      "Epoch 001: | Loss:0.23645 | Acc:26.400\n",
      "Epoch 001: | Loss:0.23722 | Acc:26.488\n",
      "Epoch 001: | Loss:0.23793 | Acc:26.562\n",
      "Epoch 001: | Loss:0.23863 | Acc:26.650\n",
      "Epoch 001: | Loss:0.23929 | Acc:26.750\n",
      "Epoch 001: | Loss:0.23994 | Acc:26.850\n",
      "Epoch 001: | Loss:0.24080 | Acc:26.938\n",
      "Epoch 001: | Loss:0.24155 | Acc:27.012\n",
      "Epoch 001: | Loss:0.24207 | Acc:27.113\n",
      "Epoch 001: | Loss:0.24258 | Acc:27.238\n",
      "Epoch 001: | Loss:0.24326 | Acc:27.312\n",
      "Epoch 001: | Loss:0.24387 | Acc:27.400\n",
      "Epoch 001: | Loss:0.24464 | Acc:27.500\n",
      "Epoch 001: | Loss:0.24550 | Acc:27.587\n",
      "Epoch 001: | Loss:0.24610 | Acc:27.675\n",
      "Epoch 001: | Loss:0.24676 | Acc:27.775\n",
      "Epoch 001: | Loss:0.24737 | Acc:27.875\n",
      "Epoch 001: | Loss:0.24792 | Acc:28.000\n",
      "Epoch 001: | Loss:0.24838 | Acc:28.125\n",
      "Epoch 001: | Loss:0.24882 | Acc:28.238\n",
      "Epoch 001: | Loss:0.24943 | Acc:28.312\n",
      "Epoch 001: | Loss:0.25005 | Acc:28.413\n",
      "Epoch 001: | Loss:0.25060 | Acc:28.538\n",
      "Epoch 001: | Loss:0.25120 | Acc:28.637\n",
      "Epoch 001: | Loss:0.25197 | Acc:28.725\n",
      "Epoch 001: | Loss:0.25248 | Acc:28.837\n",
      "Epoch 001: | Loss:0.25318 | Acc:28.938\n",
      "Epoch 001: | Loss:0.25372 | Acc:29.050\n",
      "Epoch 001: | Loss:0.25438 | Acc:29.150\n",
      "Epoch 001: | Loss:0.25497 | Acc:29.238\n",
      "Epoch 001: | Loss:0.25556 | Acc:29.350\n",
      "Epoch 001: | Loss:0.25630 | Acc:29.438\n",
      "Epoch 001: | Loss:0.25696 | Acc:29.538\n",
      "Epoch 001: | Loss:0.25788 | Acc:29.600\n",
      "Epoch 001: | Loss:0.25845 | Acc:29.700\n",
      "Epoch 001: | Loss:0.25919 | Acc:29.775\n",
      "Epoch 001: | Loss:0.25972 | Acc:29.875\n",
      "Epoch 001: | Loss:0.26045 | Acc:29.950\n",
      "Epoch 001: | Loss:0.26101 | Acc:30.062\n",
      "Epoch 001: | Loss:0.26170 | Acc:30.163\n",
      "Epoch 001: | Loss:0.26259 | Acc:30.225\n",
      "Epoch 001: | Loss:0.26322 | Acc:30.312\n",
      "Epoch 001: | Loss:0.26394 | Acc:30.400\n",
      "Epoch 001: | Loss:0.26484 | Acc:30.462\n",
      "Epoch 001: | Loss:0.26531 | Acc:30.575\n",
      "Epoch 001: | Loss:0.26618 | Acc:30.650\n",
      "Epoch 001: | Loss:0.26668 | Acc:30.750\n",
      "Epoch 001: | Loss:0.26726 | Acc:30.850\n",
      "Epoch 001: | Loss:0.26808 | Acc:30.938\n",
      "Epoch 001: | Loss:0.26900 | Acc:31.025\n",
      "Epoch 001: | Loss:0.26956 | Acc:31.150\n",
      "Epoch 001: | Loss:0.27010 | Acc:31.262\n",
      "Epoch 001: | Loss:0.27058 | Acc:31.375\n",
      "Epoch 001: | Loss:0.27116 | Acc:31.475\n",
      "Epoch 001: | Loss:0.27169 | Acc:31.587\n",
      "Epoch 001: | Loss:0.27246 | Acc:31.675\n",
      "Epoch 001: | Loss:0.27303 | Acc:31.775\n",
      "Epoch 001: | Loss:0.27377 | Acc:31.863\n",
      "Epoch 001: | Loss:0.27462 | Acc:31.950\n",
      "Epoch 001: | Loss:0.27532 | Acc:32.038\n",
      "Epoch 001: | Loss:0.27571 | Acc:32.163\n",
      "Epoch 001: | Loss:0.27633 | Acc:32.275\n",
      "Epoch 001: | Loss:0.27692 | Acc:32.375\n",
      "Epoch 001: | Loss:0.27733 | Acc:32.487\n",
      "Epoch 001: | Loss:0.27803 | Acc:32.587\n",
      "Epoch 001: | Loss:0.27856 | Acc:32.700\n",
      "Epoch 001: | Loss:0.27895 | Acc:32.812\n",
      "Epoch 001: | Loss:0.27947 | Acc:32.913\n",
      "Epoch 001: | Loss:0.28003 | Acc:33.000\n",
      "Epoch 001: | Loss:0.28057 | Acc:33.112\n",
      "Epoch 001: | Loss:0.28126 | Acc:33.188\n",
      "Epoch 001: | Loss:0.28200 | Acc:33.275\n",
      "Epoch 001: | Loss:0.28250 | Acc:33.388\n",
      "Epoch 001: | Loss:0.28308 | Acc:33.475\n",
      "Epoch 001: | Loss:0.28365 | Acc:33.575\n",
      "Epoch 001: | Loss:0.28442 | Acc:33.663\n",
      "Epoch 001: | Loss:0.28514 | Acc:33.737\n",
      "Epoch 001: | Loss:0.28583 | Acc:33.837\n",
      "Epoch 001: | Loss:0.28664 | Acc:33.925\n",
      "Epoch 001: | Loss:0.28722 | Acc:34.025\n",
      "Epoch 001: | Loss:0.28772 | Acc:34.150\n",
      "Epoch 001: | Loss:0.28834 | Acc:34.263\n",
      "Epoch 001: | Loss:0.28891 | Acc:34.375\n",
      "Epoch 001: | Loss:0.28957 | Acc:34.462\n",
      "Epoch 001: | Loss:0.29016 | Acc:34.550\n",
      "Epoch 001: | Loss:0.29101 | Acc:34.638\n",
      "Epoch 001: | Loss:0.29161 | Acc:34.737\n",
      "Epoch 001: | Loss:0.29260 | Acc:34.800\n",
      "Epoch 001: | Loss:0.29316 | Acc:34.900\n",
      "Epoch 001: | Loss:0.29387 | Acc:34.975\n",
      "Epoch 001: | Loss:0.29450 | Acc:35.062\n",
      "Epoch 001: | Loss:0.29530 | Acc:35.138\n",
      "Epoch 001: | Loss:0.29575 | Acc:35.237\n",
      "Epoch 001: | Loss:0.29641 | Acc:35.325\n",
      "Epoch 001: | Loss:0.29685 | Acc:35.438\n",
      "Epoch 001: | Loss:0.29740 | Acc:35.550\n",
      "Epoch 001: | Loss:0.29818 | Acc:35.638\n",
      "Epoch 001: | Loss:0.29879 | Acc:35.737\n",
      "Epoch 001: | Loss:0.29936 | Acc:35.837\n",
      "Epoch 001: | Loss:0.30003 | Acc:35.950\n",
      "Epoch 001: | Loss:0.30095 | Acc:36.013\n",
      "Epoch 001: | Loss:0.30139 | Acc:36.125\n",
      "Epoch 001: | Loss:0.30208 | Acc:36.212\n",
      "Epoch 001: | Loss:0.30291 | Acc:36.288\n",
      "Epoch 001: | Loss:0.30384 | Acc:36.362\n",
      "Epoch 001: | Loss:0.30452 | Acc:36.462\n",
      "Epoch 001: | Loss:0.30501 | Acc:36.575\n",
      "Epoch 001: | Loss:0.30541 | Acc:36.688\n",
      "Epoch 001: | Loss:0.30587 | Acc:36.800\n",
      "Epoch 001: | Loss:0.30649 | Acc:36.900\n",
      "Epoch 001: | Loss:0.30701 | Acc:37.013\n",
      "Epoch 001: | Loss:0.30787 | Acc:37.087\n",
      "Epoch 001: | Loss:0.30845 | Acc:37.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss:0.30894 | Acc:37.275\n",
      "Epoch 001: | Loss:0.30959 | Acc:37.375\n",
      "Epoch 001: | Loss:0.31043 | Acc:37.450\n",
      "Epoch 001: | Loss:0.31113 | Acc:37.550\n",
      "Epoch 001: | Loss:0.31163 | Acc:37.650\n",
      "Epoch 001: | Loss:0.31257 | Acc:37.725\n",
      "Epoch 001: | Loss:0.31321 | Acc:37.837\n",
      "Epoch 001: | Loss:0.31387 | Acc:37.925\n",
      "Epoch 001: | Loss:0.31436 | Acc:38.025\n",
      "Epoch 001: | Loss:0.31504 | Acc:38.100\n",
      "Epoch 001: | Loss:0.31568 | Acc:38.200\n",
      "Epoch 001: | Loss:0.31641 | Acc:38.300\n",
      "Epoch 001: | Loss:0.31726 | Acc:38.388\n",
      "Epoch 001: | Loss:0.31825 | Acc:38.462\n",
      "Epoch 001: | Loss:0.31868 | Acc:38.575\n",
      "Epoch 001: | Loss:0.31933 | Acc:38.675\n",
      "Epoch 001: | Loss:0.31982 | Acc:38.788\n",
      "Epoch 001: | Loss:0.32026 | Acc:38.900\n",
      "Epoch 001: | Loss:0.32105 | Acc:38.987\n",
      "Epoch 001: | Loss:0.32154 | Acc:39.100\n",
      "Epoch 001: | Loss:0.32222 | Acc:39.200\n",
      "Epoch 001: | Loss:0.32306 | Acc:39.288\n",
      "Epoch 001: | Loss:0.32364 | Acc:39.400\n",
      "Epoch 001: | Loss:0.32413 | Acc:39.513\n",
      "Epoch 001: | Loss:0.32497 | Acc:39.587\n",
      "Epoch 001: | Loss:0.32555 | Acc:39.675\n",
      "Epoch 001: | Loss:0.32615 | Acc:39.788\n",
      "Epoch 001: | Loss:0.32699 | Acc:39.862\n",
      "Epoch 001: | Loss:0.32743 | Acc:39.975\n",
      "Epoch 001: | Loss:0.32823 | Acc:40.062\n",
      "Epoch 001: | Loss:0.32877 | Acc:40.163\n",
      "Epoch 001: | Loss:0.32933 | Acc:40.263\n",
      "Epoch 001: | Loss:0.32998 | Acc:40.350\n",
      "Epoch 001: | Loss:0.33043 | Acc:40.462\n",
      "Epoch 001: | Loss:0.33105 | Acc:40.562\n",
      "Epoch 001: | Loss:0.33189 | Acc:40.638\n",
      "Epoch 001: | Loss:0.33228 | Acc:40.763\n",
      "Epoch 001: | Loss:0.33298 | Acc:40.850\n",
      "Epoch 001: | Loss:0.33362 | Acc:40.938\n",
      "Epoch 001: | Loss:0.33413 | Acc:41.038\n",
      "Epoch 001: | Loss:0.33478 | Acc:41.138\n",
      "Epoch 001: | Loss:0.33538 | Acc:41.237\n",
      "Epoch 001: | Loss:0.33612 | Acc:41.325\n",
      "Epoch 001: | Loss:0.33651 | Acc:41.438\n",
      "Epoch 001: | Loss:0.33703 | Acc:41.550\n",
      "Epoch 001: | Loss:0.33755 | Acc:41.650\n",
      "Epoch 001: | Loss:0.33797 | Acc:41.775\n",
      "Epoch 001: | Loss:0.33840 | Acc:41.888\n",
      "Epoch 001: | Loss:0.33891 | Acc:42.000\n",
      "Epoch 001: | Loss:0.33937 | Acc:42.100\n",
      "Epoch 001: | Loss:0.33989 | Acc:42.212\n",
      "Epoch 001: | Loss:0.34044 | Acc:42.312\n",
      "Epoch 001: | Loss:0.34091 | Acc:42.425\n",
      "Epoch 001: | Loss:0.34137 | Acc:42.538\n",
      "Epoch 001: | Loss:0.34193 | Acc:42.650\n",
      "Epoch 001: | Loss:0.34242 | Acc:42.763\n",
      "Epoch 001: | Loss:0.34285 | Acc:42.875\n",
      "Epoch 001: | Loss:0.34390 | Acc:42.925\n",
      "Epoch 001: | Loss:0.34452 | Acc:43.025\n",
      "Epoch 001: | Loss:0.34508 | Acc:43.125\n",
      "Epoch 001: | Loss:0.34556 | Acc:43.225\n",
      "Epoch 001: | Loss:0.34593 | Acc:43.337\n",
      "Epoch 001: | Loss:0.34641 | Acc:43.450\n",
      "Epoch 001: | Loss:0.34687 | Acc:43.562\n",
      "Epoch 001: | Loss:0.34731 | Acc:43.688\n",
      "Epoch 001: | Loss:0.34826 | Acc:43.750\n",
      "Epoch 001: | Loss:0.34871 | Acc:43.862\n",
      "Epoch 001: | Loss:0.34923 | Acc:43.975\n",
      "Epoch 001: | Loss:0.34979 | Acc:44.075\n",
      "Epoch 001: | Loss:0.35040 | Acc:44.188\n",
      "Epoch 001: | Loss:0.35084 | Acc:44.300\n",
      "Epoch 001: | Loss:0.35170 | Acc:44.375\n",
      "Epoch 001: | Loss:0.35239 | Acc:44.462\n",
      "Epoch 001: | Loss:0.35297 | Acc:44.550\n",
      "Epoch 001: | Loss:0.35382 | Acc:44.638\n",
      "Epoch 001: | Loss:0.35438 | Acc:44.737\n",
      "Epoch 001: | Loss:0.35489 | Acc:44.850\n",
      "Epoch 001: | Loss:0.35548 | Acc:44.950\n",
      "Epoch 001: | Loss:0.35613 | Acc:45.050\n",
      "Epoch 001: | Loss:0.35702 | Acc:45.125\n",
      "Epoch 001: | Loss:0.35794 | Acc:45.188\n",
      "Epoch 001: | Loss:0.35837 | Acc:45.300\n",
      "Epoch 001: | Loss:0.35899 | Acc:45.413\n",
      "Epoch 001: | Loss:0.35961 | Acc:45.500\n",
      "Epoch 001: | Loss:0.36000 | Acc:45.625\n",
      "Epoch 001: | Loss:0.36069 | Acc:45.712\n",
      "Epoch 001: | Loss:0.36119 | Acc:45.812\n",
      "Epoch 001: | Loss:0.36198 | Acc:45.900\n",
      "Epoch 001: | Loss:0.36237 | Acc:46.025\n",
      "Epoch 001: | Loss:0.36285 | Acc:46.125\n",
      "Epoch 001: | Loss:0.36364 | Acc:46.225\n",
      "Epoch 001: | Loss:0.36425 | Acc:46.337\n",
      "Epoch 001: | Loss:0.36493 | Acc:46.438\n",
      "Epoch 001: | Loss:0.36546 | Acc:46.538\n",
      "Epoch 001: | Loss:0.36601 | Acc:46.638\n",
      "Epoch 001: | Loss:0.36665 | Acc:46.725\n",
      "Epoch 001: | Loss:0.36715 | Acc:46.837\n",
      "Epoch 001: | Loss:0.36776 | Acc:46.938\n",
      "Epoch 001: | Loss:0.36842 | Acc:47.025\n",
      "Epoch 001: | Loss:0.36887 | Acc:47.150\n",
      "Epoch 001: | Loss:0.36930 | Acc:47.263\n",
      "Epoch 001: | Loss:0.37008 | Acc:47.350\n",
      "Epoch 001: | Loss:0.37050 | Acc:47.475\n",
      "Epoch 001: | Loss:0.37096 | Acc:47.587\n",
      "Epoch 001: | Loss:0.37182 | Acc:47.675\n",
      "Epoch 001: | Loss:0.37233 | Acc:47.775\n",
      "Epoch 001: | Loss:0.37284 | Acc:47.875\n",
      "Epoch 001: | Loss:0.37334 | Acc:47.987\n",
      "Epoch 001: | Loss:0.37398 | Acc:48.100\n",
      "Epoch 001: | Loss:0.37485 | Acc:48.163\n",
      "Epoch 001: | Loss:0.37563 | Acc:48.263\n",
      "Epoch 001: | Loss:0.37605 | Acc:48.375\n",
      "Epoch 001: | Loss:0.37667 | Acc:48.475\n",
      "Epoch 001: | Loss:0.37710 | Acc:48.600\n",
      "Epoch 001: | Loss:0.37788 | Acc:48.700\n",
      "Epoch 001: | Loss:0.37841 | Acc:48.800\n",
      "Epoch 001: | Loss:0.37903 | Acc:48.900\n",
      "Epoch 001: | Loss:0.37977 | Acc:48.987\n",
      "Epoch 001: | Loss:0.38085 | Acc:49.062\n",
      "Epoch 001: | Loss:0.38131 | Acc:49.163\n",
      "Epoch 001: | Loss:0.38180 | Acc:49.263\n",
      "Epoch 001: | Loss:0.38245 | Acc:49.350\n",
      "Epoch 001: | Loss:0.38302 | Acc:49.462\n",
      "Epoch 001: | Loss:0.38342 | Acc:49.575\n",
      "Epoch 001: | Loss:0.38393 | Acc:49.663\n",
      "Epoch 001: | Loss:0.38495 | Acc:49.737\n",
      "Epoch 001: | Loss:0.38582 | Acc:49.812\n",
      "Epoch 001: | Loss:0.38679 | Acc:49.888\n",
      "Epoch 001: | Loss:0.38735 | Acc:49.975\n",
      "Epoch 001: | Loss:0.38818 | Acc:50.062\n",
      "Epoch 001: | Loss:0.38863 | Acc:50.175\n",
      "Epoch 001: | Loss:0.38906 | Acc:50.288\n",
      "Epoch 001: | Loss:0.38951 | Acc:50.388\n",
      "Epoch 001: | Loss:0.39023 | Acc:50.462\n",
      "Epoch 001: | Loss:0.39081 | Acc:50.562\n",
      "Epoch 001: | Loss:0.39118 | Acc:50.675\n",
      "Epoch 001: | Loss:0.39182 | Acc:50.763\n",
      "Epoch 001: | Loss:0.39252 | Acc:50.837\n",
      "Epoch 001: | Loss:0.39293 | Acc:50.962\n",
      "Epoch 001: | Loss:0.39347 | Acc:51.075\n",
      "Epoch 001: | Loss:0.39407 | Acc:51.175\n",
      "Epoch 001: | Loss:0.39450 | Acc:51.300\n",
      "Epoch 001: | Loss:0.39513 | Acc:51.400\n",
      "Epoch 001: | Loss:0.39559 | Acc:51.500\n",
      "Epoch 001: | Loss:0.39593 | Acc:51.625\n",
      "Epoch 001: | Loss:0.39645 | Acc:51.737\n",
      "Epoch 001: | Loss:0.39691 | Acc:51.837\n",
      "Epoch 001: | Loss:0.39744 | Acc:51.950\n",
      "Epoch 001: | Loss:0.39785 | Acc:52.050\n",
      "Epoch 001: | Loss:0.39864 | Acc:52.150\n",
      "Epoch 001: | Loss:0.39911 | Acc:52.250\n",
      "Epoch 001: | Loss:0.39983 | Acc:52.337\n",
      "Epoch 001: | Loss:0.40059 | Acc:52.413\n",
      "Epoch 001: | Loss:0.40116 | Acc:52.500\n",
      "Epoch 001: | Loss:0.40152 | Acc:52.625\n",
      "Epoch 001: | Loss:0.40237 | Acc:52.712\n",
      "Epoch 001: | Loss:0.40313 | Acc:52.800\n",
      "Epoch 001: | Loss:0.40382 | Acc:52.888\n",
      "Epoch 001: | Loss:0.40446 | Acc:52.975\n",
      "Epoch 001: | Loss:0.40493 | Acc:53.075\n",
      "Epoch 001: | Loss:0.40531 | Acc:53.188\n",
      "Epoch 001: | Loss:0.40571 | Acc:53.300\n",
      "Epoch 001: | Loss:0.40634 | Acc:53.388\n",
      "Epoch 001: | Loss:0.40674 | Acc:53.500\n",
      "Epoch 001: | Loss:0.40737 | Acc:53.600\n",
      "Epoch 001: | Loss:0.40854 | Acc:53.663\n",
      "Epoch 001: | Loss:0.40897 | Acc:53.788\n",
      "Epoch 001: | Loss:0.40960 | Acc:53.888\n",
      "Epoch 001: | Loss:0.41028 | Acc:53.987\n",
      "Epoch 001: | Loss:0.41075 | Acc:54.100\n",
      "Epoch 001: | Loss:0.41128 | Acc:54.212\n",
      "Epoch 001: | Loss:0.41173 | Acc:54.312\n",
      "Epoch 001: | Loss:0.41238 | Acc:54.400\n",
      "Epoch 001: | Loss:0.41291 | Acc:54.513\n",
      "Epoch 001: | Loss:0.41344 | Acc:54.625\n",
      "Epoch 001: | Loss:0.41404 | Acc:54.725\n",
      "Epoch 001: | Loss:0.41449 | Acc:54.825\n",
      "Epoch 001: | Loss:0.41503 | Acc:54.925\n",
      "Epoch 001: | Loss:0.41568 | Acc:55.013\n",
      "Epoch 001: | Loss:0.41654 | Acc:55.087\n",
      "Epoch 001: | Loss:0.41699 | Acc:55.200\n",
      "Epoch 001: | Loss:0.41747 | Acc:55.312\n",
      "Epoch 001: | Loss:0.41804 | Acc:55.413\n",
      "Epoch 001: | Loss:0.41857 | Acc:55.513\n",
      "Epoch 001: | Loss:0.41921 | Acc:55.600\n",
      "Epoch 001: | Loss:0.42002 | Acc:55.675\n",
      "Epoch 001: | Loss:0.42065 | Acc:55.763\n",
      "Epoch 001: | Loss:0.42105 | Acc:55.875\n",
      "Epoch 001: | Loss:0.42139 | Acc:56.000\n",
      "Epoch 001: | Loss:0.42184 | Acc:56.112\n",
      "Epoch 001: | Loss:0.42232 | Acc:56.212\n",
      "Epoch 001: | Loss:0.42294 | Acc:56.312\n",
      "Epoch 001: | Loss:0.42330 | Acc:56.438\n",
      "Epoch 001: | Loss:0.42375 | Acc:56.550\n",
      "Epoch 001: | Loss:0.42413 | Acc:56.675\n",
      "Epoch 001: | Loss:0.42478 | Acc:56.763\n",
      "Epoch 001: | Loss:0.42549 | Acc:56.862\n",
      "Epoch 001: | Loss:0.42605 | Acc:56.962\n",
      "Epoch 001: | Loss:0.42642 | Acc:57.075\n",
      "Epoch 001: | Loss:0.42714 | Acc:57.138\n",
      "Epoch 001: | Loss:0.42777 | Acc:57.225\n",
      "Epoch 001: | Loss:0.42830 | Acc:57.312\n",
      "Epoch 001: | Loss:0.42865 | Acc:57.438\n",
      "Epoch 001: | Loss:0.42924 | Acc:57.525\n",
      "Epoch 001: | Loss:0.42970 | Acc:57.638\n",
      "Epoch 001: | Loss:0.43035 | Acc:57.712\n",
      "Epoch 001: | Loss:0.43079 | Acc:57.812\n",
      "Epoch 001: | Loss:0.43126 | Acc:57.900\n",
      "Epoch 001: | Loss:0.43178 | Acc:58.013\n",
      "Epoch 001: | Loss:0.43246 | Acc:58.100\n",
      "Epoch 001: | Loss:0.43301 | Acc:58.212\n",
      "Epoch 001: | Loss:0.43360 | Acc:58.325\n",
      "Epoch 001: | Loss:0.43399 | Acc:58.438\n",
      "Epoch 001: | Loss:0.43456 | Acc:58.538\n",
      "Epoch 001: | Loss:0.43509 | Acc:58.650\n",
      "Epoch 001: | Loss:0.43576 | Acc:58.750\n",
      "Epoch 001: | Loss:0.43615 | Acc:58.862\n",
      "Epoch 001: | Loss:0.43654 | Acc:58.975\n",
      "Epoch 001: | Loss:0.43716 | Acc:59.075\n",
      "Epoch 001: | Loss:0.43820 | Acc:59.150\n",
      "Epoch 001: | Loss:0.43871 | Acc:59.263\n",
      "Epoch 001: | Loss:0.43928 | Acc:59.375\n",
      "Epoch 001: | Loss:0.44041 | Acc:59.450\n",
      "Epoch 001: | Loss:0.44072 | Acc:59.575\n",
      "Epoch 001: | Loss:0.44110 | Acc:59.688\n",
      "Epoch 001: | Loss:0.44196 | Acc:59.775\n",
      "Epoch 001: | Loss:0.44238 | Acc:59.888\n",
      "Epoch 001: | Loss:0.44292 | Acc:59.975\n",
      "Epoch 001: | Loss:0.44332 | Acc:60.087\n",
      "Epoch 001: | Loss:0.44405 | Acc:60.188\n",
      "Epoch 001: | Loss:0.44451 | Acc:60.300\n",
      "Epoch 001: | Loss:0.44514 | Acc:60.400\n",
      "Epoch 001: | Loss:0.44556 | Acc:60.513\n",
      "Epoch 001: | Loss:0.44605 | Acc:60.625\n",
      "Epoch 001: | Loss:0.44638 | Acc:60.750\n",
      "Epoch 001: | Loss:0.44700 | Acc:60.837\n",
      "Epoch 001: | Loss:0.44742 | Acc:60.950\n",
      "Epoch 001: | Loss:0.44824 | Acc:61.038\n",
      "Epoch 001: | Loss:0.44871 | Acc:61.150\n",
      "Epoch 001: | Loss:0.44902 | Acc:61.275\n",
      "Epoch 001: | Loss:0.44967 | Acc:61.375\n",
      "Epoch 001: | Loss:0.45045 | Acc:61.450\n",
      "Epoch 001: | Loss:0.45093 | Acc:61.550\n",
      "Epoch 001: | Loss:0.45127 | Acc:61.675\n",
      "Epoch 001: | Loss:0.45187 | Acc:61.763\n",
      "Epoch 001: | Loss:0.45292 | Acc:61.825\n",
      "Epoch 001: | Loss:0.45325 | Acc:61.950\n",
      "Epoch 001: | Loss:0.45367 | Acc:62.050\n",
      "Epoch 001: | Loss:0.45423 | Acc:62.150\n",
      "Epoch 001: | Loss:0.45469 | Acc:62.263\n",
      "Epoch 001: | Loss:0.45497 | Acc:62.388\n",
      "Epoch 001: | Loss:0.45529 | Acc:62.513\n",
      "Epoch 001: | Loss:0.45607 | Acc:62.587\n",
      "Epoch 001: | Loss:0.45690 | Acc:62.650\n",
      "Epoch 001: | Loss:0.45764 | Acc:62.750\n",
      "Epoch 001: | Loss:0.45830 | Acc:62.850\n",
      "Epoch 001: | Loss:0.45887 | Acc:62.938\n",
      "Epoch 001: | Loss:0.45921 | Acc:63.062\n",
      "Epoch 001: | Loss:0.45963 | Acc:63.175\n",
      "Epoch 001: | Loss:0.46046 | Acc:63.250\n",
      "Epoch 001: | Loss:0.46089 | Acc:63.362\n",
      "Epoch 001: | Loss:0.46162 | Acc:63.450\n",
      "Epoch 001: | Loss:0.46216 | Acc:63.550\n",
      "Epoch 001: | Loss:0.46256 | Acc:63.675\n",
      "Epoch 001: | Loss:0.46301 | Acc:63.775\n",
      "Epoch 001: | Loss:0.46372 | Acc:63.862\n",
      "Epoch 001: | Loss:0.46413 | Acc:63.975\n",
      "Epoch 001: | Loss:0.46491 | Acc:64.062\n",
      "Epoch 001: | Loss:0.46534 | Acc:64.162\n",
      "Epoch 001: | Loss:0.46584 | Acc:64.263\n",
      "Epoch 001: | Loss:0.46615 | Acc:64.388\n",
      "Epoch 001: | Loss:0.46650 | Acc:64.500\n",
      "Epoch 001: | Loss:0.46696 | Acc:64.588\n",
      "Epoch 001: | Loss:0.46767 | Acc:64.675\n",
      "Epoch 001: | Loss:0.46839 | Acc:64.750\n",
      "Epoch 001: | Loss:0.46883 | Acc:64.862\n",
      "Epoch 001: | Loss:0.46938 | Acc:64.963\n",
      "Epoch 001: | Loss:0.47031 | Acc:65.037\n",
      "Epoch 001: | Loss:0.47069 | Acc:65.150\n",
      "Epoch 001: | Loss:0.47177 | Acc:65.225\n",
      "Epoch 001: | Loss:0.47251 | Acc:65.312\n",
      "Epoch 001: | Loss:0.47308 | Acc:65.412\n",
      "Epoch 001: | Loss:0.47366 | Acc:65.513\n",
      "Epoch 001: | Loss:0.47410 | Acc:65.625\n",
      "Epoch 001: | Loss:0.47479 | Acc:65.700\n",
      "Epoch 001: | Loss:0.47515 | Acc:65.825\n",
      "Epoch 001: | Loss:0.47553 | Acc:65.938\n",
      "Epoch 001: | Loss:0.47601 | Acc:66.050\n",
      "Epoch 001: | Loss:0.47657 | Acc:66.138\n",
      "Epoch 001: | Loss:0.47706 | Acc:66.237\n",
      "Epoch 001: | Loss:0.47790 | Acc:66.338\n",
      "Epoch 001: | Loss:0.47872 | Acc:66.425\n",
      "Epoch 001: | Loss:0.47912 | Acc:66.525\n",
      "Epoch 001: | Loss:0.48037 | Acc:66.588\n",
      "Epoch 001: | Loss:0.48103 | Acc:66.675\n",
      "Epoch 001: | Loss:0.48197 | Acc:66.763\n",
      "Epoch 001: | Loss:0.48247 | Acc:66.850\n",
      "Epoch 001: | Loss:0.48316 | Acc:66.950\n",
      "Epoch 001: | Loss:0.48363 | Acc:67.062\n",
      "Epoch 001: | Loss:0.48409 | Acc:67.162\n",
      "Epoch 001: | Loss:0.48468 | Acc:67.263\n",
      "Epoch 001: | Loss:0.48514 | Acc:67.362\n",
      "Epoch 001: | Loss:0.48563 | Acc:67.463\n",
      "Epoch 001: | Loss:0.48587 | Acc:67.588\n",
      "Epoch 001: | Loss:0.48704 | Acc:67.638\n",
      "Epoch 001: | Loss:0.48756 | Acc:67.750\n",
      "Epoch 001: | Loss:0.48809 | Acc:67.862\n",
      "Epoch 001: | Loss:0.48836 | Acc:67.987\n",
      "Epoch 001: | Loss:0.48943 | Acc:68.062\n",
      "Epoch 001: | Loss:0.48984 | Acc:68.188\n",
      "Epoch 001: | Loss:0.49052 | Acc:68.287\n",
      "Epoch 001: | Loss:0.49096 | Acc:68.388\n",
      "Epoch 001: | Loss:0.49126 | Acc:68.513\n",
      "Epoch 001: | Loss:0.49174 | Acc:68.600\n",
      "Epoch 001: | Loss:0.49228 | Acc:68.700\n",
      "Epoch 001: | Loss:0.49295 | Acc:68.787\n",
      "Epoch 001: | Loss:0.49342 | Acc:68.888\n",
      "Epoch 001: | Loss:0.49383 | Acc:69.000\n",
      "Epoch 001: | Loss:0.49481 | Acc:69.075\n",
      "Epoch 001: | Loss:0.49523 | Acc:69.188\n",
      "Epoch 001: | Loss:0.49579 | Acc:69.263\n",
      "Epoch 001: | Loss:0.49647 | Acc:69.350\n",
      "Epoch 001: | Loss:0.49732 | Acc:69.450\n",
      "Epoch 001: | Loss:0.49765 | Acc:69.562\n",
      "Epoch 001: | Loss:0.49833 | Acc:69.650\n",
      "Epoch 001: | Loss:0.49875 | Acc:69.763\n",
      "Epoch 001: | Loss:0.49913 | Acc:69.875\n",
      "Epoch 001: | Loss:0.49934 | Acc:70.000\n",
      "Epoch 001: | Loss:0.50027 | Acc:70.075\n",
      "Epoch 001: | Loss:0.50103 | Acc:70.162\n",
      "Epoch 001: | Loss:0.50136 | Acc:70.287\n",
      "Epoch 001: | Loss:0.50222 | Acc:70.350\n",
      "Epoch 001: | Loss:0.50321 | Acc:70.425\n",
      "Epoch 001: | Loss:0.50368 | Acc:70.537\n",
      "Epoch 001: | Loss:0.50444 | Acc:70.638\n",
      "Epoch 001: | Loss:0.50487 | Acc:70.750\n",
      "Epoch 001: | Loss:0.50530 | Acc:70.850\n",
      "Epoch 001: | Loss:0.50565 | Acc:70.975\n",
      "Epoch 001: | Loss:0.50608 | Acc:71.088\n",
      "Epoch 001: | Loss:0.50647 | Acc:71.200\n",
      "Epoch 001: | Loss:0.50709 | Acc:71.300\n",
      "Epoch 001: | Loss:0.50757 | Acc:71.412\n",
      "Epoch 001: | Loss:0.50810 | Acc:71.525\n",
      "Epoch 001: | Loss:0.50852 | Acc:71.625\n",
      "Epoch 001: | Loss:0.50884 | Acc:71.750\n",
      "Epoch 001: | Loss:0.50960 | Acc:71.850\n",
      "Epoch 001: | Loss:0.51007 | Acc:71.950\n",
      "Epoch 001: | Loss:0.51074 | Acc:72.037\n",
      "Epoch 001: | Loss:0.51110 | Acc:72.162\n",
      "Epoch 001: | Loss:0.51153 | Acc:72.275\n",
      "Epoch 001: | Loss:0.51183 | Acc:72.400\n",
      "Epoch 001: | Loss:0.51224 | Acc:72.513\n",
      "Epoch 001: | Loss:0.51270 | Acc:72.612\n",
      "Epoch 001: | Loss:0.51348 | Acc:72.700\n",
      "Epoch 001: | Loss:0.51400 | Acc:72.800\n",
      "Epoch 001: | Loss:0.51438 | Acc:72.912\n",
      "Epoch 001: | Loss:0.51471 | Acc:73.025\n",
      "Epoch 001: | Loss:0.51536 | Acc:73.125\n",
      "Epoch 001: | Loss:0.51576 | Acc:73.225\n",
      "Epoch 001: | Loss:0.51610 | Acc:73.338\n",
      "Epoch 001: | Loss:0.51662 | Acc:73.450\n",
      "Epoch 001: | Loss:0.51742 | Acc:73.550\n",
      "Epoch 001: | Loss:0.51774 | Acc:73.675\n",
      "Epoch 001: | Loss:0.51865 | Acc:73.763\n",
      "Epoch 001: | Loss:0.51916 | Acc:73.862\n",
      "Epoch 001: | Loss:0.51956 | Acc:73.963\n",
      "Epoch 001: | Loss:0.52027 | Acc:74.062\n",
      "Epoch 001: | Loss:0.52062 | Acc:74.175\n",
      "Epoch 001: | Loss:0.52105 | Acc:74.275\n",
      "Epoch 001: | Loss:0.52135 | Acc:74.400\n",
      "Epoch 001: | Loss:0.52233 | Acc:74.475\n",
      "Epoch 001: | Loss:0.52274 | Acc:74.588\n",
      "Epoch 001: | Loss:0.52316 | Acc:74.700\n",
      "Epoch 001: | Loss:0.52369 | Acc:74.812\n",
      "Epoch 001: | Loss:0.52433 | Acc:74.900\n",
      "Epoch 001: | Loss:0.52470 | Acc:75.013\n",
      "Epoch 001: | Loss:0.52522 | Acc:75.112\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "classifier.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        #setting gradient to 0 per mini-batch\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = classifier(X_batch)\n",
    "        loss =criterion(y_pred, y_batch)\n",
    "        acc = binary_acc(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        print(f'Epoch {e+0:03}: | Loss:{epoch_loss/len(train_loader):.5f} | Acc:{epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model\n",
    "y_pred_list = []\n",
    "classifier.eval()\n",
    "count = 0\n",
    "#ensures no back propagation during testing and reduces memeory usage\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = classifier(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        \n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "    y_pred_list = [i.squeeze().tolist() for i in y_pred_list] \n",
    "    y_pred_list = [bool(i) for i in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  predicted\n",
       "0       0      False\n",
       "1       0      False\n",
       "2       0      False\n",
       "3       0      False\n",
       "4       0      False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(y_test)\n",
    "df1.reset_index(inplace=True)\n",
    "df1.drop(columns=['index'], axis=1, inplace=True)\n",
    "df2 = pd.DataFrame(y_pred_list)\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "df.columns=['target','predicted']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1550   57]\n",
      " [ 289  104]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "cm = confusion_matrix(df['target'],df['predicted'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our confusion matrix we conclude that:\n",
    "1. **True positive:** 91(We predicted a positive result and it was positive)- the model rightly predicted the ones who left the bank \n",
    "2. **True negative:** 1570(We predicted a negative result and it was negative)-the model rightly predicted the ones who stayed at the bank \n",
    "3. **False positive:** 37(We predicted a positive result and it was negative)-the model predicted that these ones left when they actually stayed\n",
    "4. **False negative:** 302(We predicted a negative result and it was positive)- the model predicted that these ones stayed when they actually left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90      1607\n",
      "           1       0.71      0.23      0.35       393\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      2000\n",
      "   macro avg       0.77      0.60      0.63      2000\n",
      "weighted avg       0.81      0.83      0.79      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
