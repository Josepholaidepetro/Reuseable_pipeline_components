{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer retention Analytics\n",
    "\n",
    "##### Business Problem:\n",
    "An International bank collected a sample data of 10,000 customers. They observed that some of their custimers are leaving or churning in an unusually high rate and they want to find understand and access why their customers keep leaving. They have hiered you as a data scientist to look into the data to give then some insight.\n",
    "\n",
    "##### Data:\n",
    "The data was collected within the last 5 months, the feautures include the customer's name, creditscore, geography, gendey,age,tenure,balnce, number of products(accounts),credit card status(whether they have one or not), estimated salary, activity status(active member or not) and if the customer remained with them.\n",
    "\n",
    "#### Goal:\n",
    "Your goal is to create a geodemographic segmentation  to identify which of the customers have the highest risk of leaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on a banking dataset to detect churn activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/AdeloreSimiloluwa/Artificial-Neural-Network/master/data/Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22ea2d002e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#frequency of the target classes\n",
    "sns.countplot(x='Exited', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data we have more information on the customers that stayed at the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber            int64\n",
       "CustomerId           int64\n",
       "Surname             object\n",
       "CreditScore          int64\n",
       "Geography           object\n",
       "Gender              object\n",
       "Age                  int64\n",
       "Tenure               int64\n",
       "Balance            float64\n",
       "NumOfProducts        int64\n",
       "HasCrCard            int64\n",
       "IsActiveMember       int64\n",
       "EstimatedSalary    float64\n",
       "Exited               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for datatype of each column\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber          0\n",
       "CustomerId         0\n",
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0          1               1        101348.88       1  \n",
       "1          0               1        112542.58       0  \n",
       "2          1               0        113931.57       1  \n",
       "3          0               0         93826.63       0  \n",
       "4          1               1         79084.10       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping some columns that are not needed\n",
    "data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['France', 'Spain', 'Germany'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing the unique values in Geography column\n",
    "data['Geography'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0          1               1        101348.88  \n",
       "1          0               1        112542.58  \n",
       "2          1               0        113931.57  \n",
       "3          0               0         93826.63  \n",
       "4          1               1         79084.10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data features\n",
    "X = data.iloc[:,:-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Exited\n",
       "0       1\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#target data\n",
    "y = data.iloc[:,-1:]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France       0   42       2       0.00              1   \n",
       "1          608     Spain       0   41       1   83807.86              1   \n",
       "2          502    France       0   42       8  159660.80              3   \n",
       "3          699    France       0   39       1       0.00              2   \n",
       "4          850     Spain       0   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Geography_France  \\\n",
       "0          1               1        101348.88               1.0   \n",
       "1          0               1        112542.58               0.0   \n",
       "2          1               0        113931.57               1.0   \n",
       "3          0               0         93826.63               1.0   \n",
       "4          1               1         79084.10               0.0   \n",
       "\n",
       "   Geography_Germany  Geography_Spain  \n",
       "0                0.0              0.0  \n",
       "1                0.0              1.0  \n",
       "2                0.0              0.0  \n",
       "3                0.0              0.0  \n",
       "4                0.0              1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoding the categorical columns\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder()\n",
    "X['Gender'] = le.fit_transform(X['Gender'])\n",
    "geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "#getting feature name after onehotencoding\n",
    "geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "#merging geo_df with the main data\n",
    "X = X.join(geo_df)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Gender  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0          619       0   42       2       0.00              1          1   \n",
       "1          608       0   41       1   83807.86              1          0   \n",
       "2          502       0   42       8  159660.80              3          1   \n",
       "3          699       0   39       1       0.00              2          0   \n",
       "4          850       0   43       2  125510.82              1          1   \n",
       "\n",
       "   IsActiveMember  EstimatedSalary  Geography_France  Geography_Germany  \\\n",
       "0               1        101348.88               1.0                0.0   \n",
       "1               1        112542.58               0.0                0.0   \n",
       "2               0        113931.57               1.0                0.0   \n",
       "3               0         93826.63               1.0                0.0   \n",
       "4               1         79084.10               0.0                0.0   \n",
       "\n",
       "   Geography_Spain  \n",
       "0              0.0  \n",
       "1              1.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping the old columns after encoding\n",
    "X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (8000, 12)\n",
      "The shape of X_test is (2000, 12)\n",
      "The shape of y_train is (8000, 1)\n",
      "The shape of y_test is (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\"X_train\":X_train, \"X_test\":X_test, \"y_train\":y_train, \"y_test\":y_test}\n",
    "for i in data_dict:\n",
    "    print(\"The shape of {} is {}\".format(i,data_dict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JADESOLA\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\JADESOLA\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\JADESOLA\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "sc =StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting model hyper-parameters\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE =10\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train data\n",
    "class trainData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = trainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "class testData(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.X_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "test_data = testData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining dataloader to read dataset class in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binaryClassification, self).__init__()\n",
    "        #number of input features is 12\n",
    "        self.layer_1 = nn.Linear(12, 16)\n",
    "        self.layer_2 = nn.Linear(16, 8)\n",
    "        self.layer_out = nn.Linear(8, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(8)\n",
    "        \n",
    "    #feed forward network\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binaryClassification(\n",
      "  (layer_1): Linear(in_features=12, out_features=16, bias=True)\n",
      "  (layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (layer_out): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#initializing optimizer and loss\n",
    "classifier = binaryClassification()\n",
    "classifier.to(device)\n",
    "print(classifier)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate accuracy\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    \n",
    "    results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = results_sum/y_test.shape[0]\n",
    "    acc =torch.round(acc*100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss:0.00079 | Acc:0.075\n",
      "Epoch 001: | Loss:0.00175 | Acc:0.125\n",
      "Epoch 001: | Loss:0.00256 | Acc:0.188\n",
      "Epoch 001: | Loss:0.00331 | Acc:0.275\n",
      "Epoch 001: | Loss:0.00426 | Acc:0.338\n",
      "Epoch 001: | Loss:0.00510 | Acc:0.425\n",
      "Epoch 001: | Loss:0.00591 | Acc:0.487\n",
      "Epoch 001: | Loss:0.00671 | Acc:0.562\n",
      "Epoch 001: | Loss:0.00753 | Acc:0.650\n",
      "Epoch 001: | Loss:0.00852 | Acc:0.688\n",
      "Epoch 001: | Loss:0.00933 | Acc:0.750\n",
      "Epoch 001: | Loss:0.01021 | Acc:0.812\n",
      "Epoch 001: | Loss:0.01104 | Acc:0.863\n",
      "Epoch 001: | Loss:0.01188 | Acc:0.950\n",
      "Epoch 001: | Loss:0.01265 | Acc:1.050\n",
      "Epoch 001: | Loss:0.01342 | Acc:1.137\n",
      "Epoch 001: | Loss:0.01420 | Acc:1.238\n",
      "Epoch 001: | Loss:0.01512 | Acc:1.312\n",
      "Epoch 001: | Loss:0.01599 | Acc:1.400\n",
      "Epoch 001: | Loss:0.01697 | Acc:1.450\n",
      "Epoch 001: | Loss:0.01776 | Acc:1.538\n",
      "Epoch 001: | Loss:0.01845 | Acc:1.650\n",
      "Epoch 001: | Loss:0.01928 | Acc:1.725\n",
      "Epoch 001: | Loss:0.02006 | Acc:1.812\n",
      "Epoch 001: | Loss:0.02088 | Acc:1.863\n",
      "Epoch 001: | Loss:0.02176 | Acc:1.950\n",
      "Epoch 001: | Loss:0.02269 | Acc:2.025\n",
      "Epoch 001: | Loss:0.02349 | Acc:2.112\n",
      "Epoch 001: | Loss:0.02438 | Acc:2.188\n",
      "Epoch 001: | Loss:0.02512 | Acc:2.300\n",
      "Epoch 001: | Loss:0.02586 | Acc:2.400\n",
      "Epoch 001: | Loss:0.02660 | Acc:2.487\n",
      "Epoch 001: | Loss:0.02738 | Acc:2.562\n",
      "Epoch 001: | Loss:0.02802 | Acc:2.675\n",
      "Epoch 001: | Loss:0.02881 | Acc:2.763\n",
      "Epoch 001: | Loss:0.02970 | Acc:2.837\n",
      "Epoch 001: | Loss:0.03041 | Acc:2.950\n",
      "Epoch 001: | Loss:0.03117 | Acc:3.050\n",
      "Epoch 001: | Loss:0.03197 | Acc:3.138\n",
      "Epoch 001: | Loss:0.03282 | Acc:3.200\n",
      "Epoch 001: | Loss:0.03354 | Acc:3.275\n",
      "Epoch 001: | Loss:0.03439 | Acc:3.362\n",
      "Epoch 001: | Loss:0.03522 | Acc:3.450\n",
      "Epoch 001: | Loss:0.03593 | Acc:3.550\n",
      "Epoch 001: | Loss:0.03688 | Acc:3.612\n",
      "Epoch 001: | Loss:0.03761 | Acc:3.725\n",
      "Epoch 001: | Loss:0.03830 | Acc:3.825\n",
      "Epoch 001: | Loss:0.03909 | Acc:3.900\n",
      "Epoch 001: | Loss:0.03998 | Acc:3.962\n",
      "Epoch 001: | Loss:0.04072 | Acc:4.075\n",
      "Epoch 001: | Loss:0.04163 | Acc:4.138\n",
      "Epoch 001: | Loss:0.04255 | Acc:4.175\n",
      "Epoch 001: | Loss:0.04328 | Acc:4.287\n",
      "Epoch 001: | Loss:0.04407 | Acc:4.388\n",
      "Epoch 001: | Loss:0.04491 | Acc:4.475\n",
      "Epoch 001: | Loss:0.04569 | Acc:4.575\n",
      "Epoch 001: | Loss:0.04635 | Acc:4.688\n",
      "Epoch 001: | Loss:0.04706 | Acc:4.800\n",
      "Epoch 001: | Loss:0.04782 | Acc:4.888\n",
      "Epoch 001: | Loss:0.04859 | Acc:4.975\n",
      "Epoch 001: | Loss:0.04944 | Acc:5.037\n",
      "Epoch 001: | Loss:0.05018 | Acc:5.138\n",
      "Epoch 001: | Loss:0.05100 | Acc:5.213\n",
      "Epoch 001: | Loss:0.05187 | Acc:5.275\n",
      "Epoch 001: | Loss:0.05272 | Acc:5.338\n",
      "Epoch 001: | Loss:0.05366 | Acc:5.388\n",
      "Epoch 001: | Loss:0.05451 | Acc:5.450\n",
      "Epoch 001: | Loss:0.05526 | Acc:5.525\n",
      "Epoch 001: | Loss:0.05610 | Acc:5.600\n",
      "Epoch 001: | Loss:0.05691 | Acc:5.675\n",
      "Epoch 001: | Loss:0.05780 | Acc:5.763\n",
      "Epoch 001: | Loss:0.05849 | Acc:5.862\n",
      "Epoch 001: | Loss:0.05921 | Acc:5.963\n",
      "Epoch 001: | Loss:0.05994 | Acc:6.062\n",
      "Epoch 001: | Loss:0.06067 | Acc:6.138\n",
      "Epoch 001: | Loss:0.06163 | Acc:6.213\n",
      "Epoch 001: | Loss:0.06245 | Acc:6.275\n",
      "Epoch 001: | Loss:0.06316 | Acc:6.375\n",
      "Epoch 001: | Loss:0.06385 | Acc:6.487\n",
      "Epoch 001: | Loss:0.06454 | Acc:6.600\n",
      "Epoch 001: | Loss:0.06544 | Acc:6.662\n",
      "Epoch 001: | Loss:0.06628 | Acc:6.725\n",
      "Epoch 001: | Loss:0.06722 | Acc:6.775\n",
      "Epoch 001: | Loss:0.06790 | Acc:6.875\n",
      "Epoch 001: | Loss:0.06867 | Acc:6.963\n",
      "Epoch 001: | Loss:0.06940 | Acc:7.062\n",
      "Epoch 001: | Loss:0.07010 | Acc:7.162\n",
      "Epoch 001: | Loss:0.07086 | Acc:7.250\n",
      "Epoch 001: | Loss:0.07165 | Acc:7.325\n",
      "Epoch 001: | Loss:0.07238 | Acc:7.425\n",
      "Epoch 001: | Loss:0.07310 | Acc:7.525\n",
      "Epoch 001: | Loss:0.07382 | Acc:7.625\n",
      "Epoch 001: | Loss:0.07453 | Acc:7.737\n",
      "Epoch 001: | Loss:0.07529 | Acc:7.825\n",
      "Epoch 001: | Loss:0.07608 | Acc:7.900\n",
      "Epoch 001: | Loss:0.07683 | Acc:8.000\n",
      "Epoch 001: | Loss:0.07760 | Acc:8.100\n",
      "Epoch 001: | Loss:0.07841 | Acc:8.175\n",
      "Epoch 001: | Loss:0.07925 | Acc:8.262\n",
      "Epoch 001: | Loss:0.08010 | Acc:8.338\n",
      "Epoch 001: | Loss:0.08089 | Acc:8.400\n",
      "Epoch 001: | Loss:0.08161 | Acc:8.500\n",
      "Epoch 001: | Loss:0.08245 | Acc:8.562\n",
      "Epoch 001: | Loss:0.08323 | Acc:8.662\n",
      "Epoch 001: | Loss:0.08394 | Acc:8.762\n",
      "Epoch 001: | Loss:0.08471 | Acc:8.850\n",
      "Epoch 001: | Loss:0.08547 | Acc:8.938\n",
      "Epoch 001: | Loss:0.08626 | Acc:9.012\n",
      "Epoch 001: | Loss:0.08712 | Acc:9.075\n",
      "Epoch 001: | Loss:0.08784 | Acc:9.188\n",
      "Epoch 001: | Loss:0.08858 | Acc:9.275\n",
      "Epoch 001: | Loss:0.08947 | Acc:9.363\n",
      "Epoch 001: | Loss:0.09014 | Acc:9.463\n",
      "Epoch 001: | Loss:0.09108 | Acc:9.500\n",
      "Epoch 001: | Loss:0.09189 | Acc:9.575\n",
      "Epoch 001: | Loss:0.09260 | Acc:9.688\n",
      "Epoch 001: | Loss:0.09345 | Acc:9.762\n",
      "Epoch 001: | Loss:0.09415 | Acc:9.863\n",
      "Epoch 001: | Loss:0.09482 | Acc:9.963\n",
      "Epoch 001: | Loss:0.09563 | Acc:10.050\n",
      "Epoch 001: | Loss:0.09630 | Acc:10.162\n",
      "Epoch 001: | Loss:0.09705 | Acc:10.250\n",
      "Epoch 001: | Loss:0.09773 | Acc:10.375\n",
      "Epoch 001: | Loss:0.09852 | Acc:10.463\n",
      "Epoch 001: | Loss:0.09927 | Acc:10.562\n",
      "Epoch 001: | Loss:0.10000 | Acc:10.662\n",
      "Epoch 001: | Loss:0.10074 | Acc:10.738\n",
      "Epoch 001: | Loss:0.10152 | Acc:10.838\n",
      "Epoch 001: | Loss:0.10217 | Acc:10.938\n",
      "Epoch 001: | Loss:0.10291 | Acc:11.012\n",
      "Epoch 001: | Loss:0.10357 | Acc:11.125\n",
      "Epoch 001: | Loss:0.10422 | Acc:11.238\n",
      "Epoch 001: | Loss:0.10487 | Acc:11.350\n",
      "Epoch 001: | Loss:0.10547 | Acc:11.463\n",
      "Epoch 001: | Loss:0.10617 | Acc:11.562\n",
      "Epoch 001: | Loss:0.10700 | Acc:11.625\n",
      "Epoch 001: | Loss:0.10775 | Acc:11.700\n",
      "Epoch 001: | Loss:0.10840 | Acc:11.825\n",
      "Epoch 001: | Loss:0.10918 | Acc:11.912\n",
      "Epoch 001: | Loss:0.10997 | Acc:12.000\n",
      "Epoch 001: | Loss:0.11070 | Acc:12.100\n",
      "Epoch 001: | Loss:0.11146 | Acc:12.175\n",
      "Epoch 001: | Loss:0.11214 | Acc:12.275\n",
      "Epoch 001: | Loss:0.11280 | Acc:12.387\n",
      "Epoch 001: | Loss:0.11356 | Acc:12.475\n",
      "Epoch 001: | Loss:0.11420 | Acc:12.575\n",
      "Epoch 001: | Loss:0.11497 | Acc:12.662\n",
      "Epoch 001: | Loss:0.11578 | Acc:12.725\n",
      "Epoch 001: | Loss:0.11641 | Acc:12.838\n",
      "Epoch 001: | Loss:0.11712 | Acc:12.938\n",
      "Epoch 001: | Loss:0.11779 | Acc:13.037\n",
      "Epoch 001: | Loss:0.11861 | Acc:13.113\n",
      "Epoch 001: | Loss:0.11929 | Acc:13.200\n",
      "Epoch 001: | Loss:0.12006 | Acc:13.275\n",
      "Epoch 001: | Loss:0.12068 | Acc:13.400\n",
      "Epoch 001: | Loss:0.12145 | Acc:13.488\n",
      "Epoch 001: | Loss:0.12208 | Acc:13.613\n",
      "Epoch 001: | Loss:0.12283 | Acc:13.700\n",
      "Epoch 001: | Loss:0.12346 | Acc:13.812\n",
      "Epoch 001: | Loss:0.12444 | Acc:13.863\n",
      "Epoch 001: | Loss:0.12508 | Acc:13.963\n",
      "Epoch 001: | Loss:0.12573 | Acc:14.062\n",
      "Epoch 001: | Loss:0.12647 | Acc:14.162\n",
      "Epoch 001: | Loss:0.12717 | Acc:14.262\n",
      "Epoch 001: | Loss:0.12789 | Acc:14.350\n",
      "Epoch 001: | Loss:0.12859 | Acc:14.450\n",
      "Epoch 001: | Loss:0.12920 | Acc:14.550\n",
      "Epoch 001: | Loss:0.12996 | Acc:14.637\n",
      "Epoch 001: | Loss:0.13057 | Acc:14.762\n",
      "Epoch 001: | Loss:0.13119 | Acc:14.887\n",
      "Epoch 001: | Loss:0.13213 | Acc:14.938\n",
      "Epoch 001: | Loss:0.13294 | Acc:15.025\n",
      "Epoch 001: | Loss:0.13365 | Acc:15.113\n",
      "Epoch 001: | Loss:0.13434 | Acc:15.213\n",
      "Epoch 001: | Loss:0.13503 | Acc:15.325\n",
      "Epoch 001: | Loss:0.13569 | Acc:15.438\n",
      "Epoch 001: | Loss:0.13641 | Acc:15.525\n",
      "Epoch 001: | Loss:0.13732 | Acc:15.588\n",
      "Epoch 001: | Loss:0.13792 | Acc:15.713\n",
      "Epoch 001: | Loss:0.13868 | Acc:15.812\n",
      "Epoch 001: | Loss:0.13934 | Acc:15.912\n",
      "Epoch 001: | Loss:0.13996 | Acc:16.012\n",
      "Epoch 001: | Loss:0.14066 | Acc:16.113\n",
      "Epoch 001: | Loss:0.14133 | Acc:16.212\n",
      "Epoch 001: | Loss:0.14200 | Acc:16.312\n",
      "Epoch 001: | Loss:0.14264 | Acc:16.425\n",
      "Epoch 001: | Loss:0.14326 | Acc:16.525\n",
      "Epoch 001: | Loss:0.14386 | Acc:16.625\n",
      "Epoch 001: | Loss:0.14456 | Acc:16.712\n",
      "Epoch 001: | Loss:0.14529 | Acc:16.812\n",
      "Epoch 001: | Loss:0.14599 | Acc:16.913\n",
      "Epoch 001: | Loss:0.14665 | Acc:17.012\n",
      "Epoch 001: | Loss:0.14732 | Acc:17.113\n",
      "Epoch 001: | Loss:0.14791 | Acc:17.225\n",
      "Epoch 001: | Loss:0.14855 | Acc:17.337\n",
      "Epoch 001: | Loss:0.14916 | Acc:17.450\n",
      "Epoch 001: | Loss:0.14981 | Acc:17.550\n",
      "Epoch 001: | Loss:0.15051 | Acc:17.625\n",
      "Epoch 001: | Loss:0.15113 | Acc:17.738\n",
      "Epoch 001: | Loss:0.15176 | Acc:17.850\n",
      "Epoch 001: | Loss:0.15258 | Acc:17.938\n",
      "Epoch 001: | Loss:0.15334 | Acc:18.038\n",
      "Epoch 001: | Loss:0.15396 | Acc:18.137\n",
      "Epoch 001: | Loss:0.15456 | Acc:18.250\n",
      "Epoch 001: | Loss:0.15539 | Acc:18.325\n",
      "Epoch 001: | Loss:0.15607 | Acc:18.413\n",
      "Epoch 001: | Loss:0.15684 | Acc:18.512\n",
      "Epoch 001: | Loss:0.15764 | Acc:18.587\n",
      "Epoch 001: | Loss:0.15831 | Acc:18.700\n",
      "Epoch 001: | Loss:0.15893 | Acc:18.788\n",
      "Epoch 001: | Loss:0.15959 | Acc:18.875\n",
      "Epoch 001: | Loss:0.16032 | Acc:18.962\n",
      "Epoch 001: | Loss:0.16102 | Acc:19.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss:0.16171 | Acc:19.137\n",
      "Epoch 001: | Loss:0.16250 | Acc:19.200\n",
      "Epoch 001: | Loss:0.16329 | Acc:19.275\n",
      "Epoch 001: | Loss:0.16393 | Acc:19.363\n",
      "Epoch 001: | Loss:0.16473 | Acc:19.438\n",
      "Epoch 001: | Loss:0.16539 | Acc:19.538\n",
      "Epoch 001: | Loss:0.16612 | Acc:19.625\n",
      "Epoch 001: | Loss:0.16690 | Acc:19.725\n",
      "Epoch 001: | Loss:0.16754 | Acc:19.825\n",
      "Epoch 001: | Loss:0.16833 | Acc:19.913\n",
      "Epoch 001: | Loss:0.16899 | Acc:20.012\n",
      "Epoch 001: | Loss:0.16965 | Acc:20.100\n",
      "Epoch 001: | Loss:0.17018 | Acc:20.225\n",
      "Epoch 001: | Loss:0.17078 | Acc:20.325\n",
      "Epoch 001: | Loss:0.17138 | Acc:20.425\n",
      "Epoch 001: | Loss:0.17205 | Acc:20.512\n",
      "Epoch 001: | Loss:0.17273 | Acc:20.600\n",
      "Epoch 001: | Loss:0.17352 | Acc:20.688\n",
      "Epoch 001: | Loss:0.17425 | Acc:20.775\n",
      "Epoch 001: | Loss:0.17498 | Acc:20.863\n",
      "Epoch 001: | Loss:0.17555 | Acc:20.975\n",
      "Epoch 001: | Loss:0.17612 | Acc:21.087\n",
      "Epoch 001: | Loss:0.17686 | Acc:21.188\n",
      "Epoch 001: | Loss:0.17757 | Acc:21.275\n",
      "Epoch 001: | Loss:0.17813 | Acc:21.387\n",
      "Epoch 001: | Loss:0.17871 | Acc:21.488\n",
      "Epoch 001: | Loss:0.17941 | Acc:21.575\n",
      "Epoch 001: | Loss:0.18016 | Acc:21.675\n",
      "Epoch 001: | Loss:0.18099 | Acc:21.750\n",
      "Epoch 001: | Loss:0.18182 | Acc:21.837\n",
      "Epoch 001: | Loss:0.18243 | Acc:21.938\n",
      "Epoch 001: | Loss:0.18312 | Acc:22.025\n",
      "Epoch 001: | Loss:0.18375 | Acc:22.137\n",
      "Epoch 001: | Loss:0.18427 | Acc:22.262\n",
      "Epoch 001: | Loss:0.18486 | Acc:22.363\n",
      "Epoch 001: | Loss:0.18551 | Acc:22.462\n",
      "Epoch 001: | Loss:0.18606 | Acc:22.587\n",
      "Epoch 001: | Loss:0.18675 | Acc:22.688\n",
      "Epoch 001: | Loss:0.18762 | Acc:22.775\n",
      "Epoch 001: | Loss:0.18827 | Acc:22.875\n",
      "Epoch 001: | Loss:0.18900 | Acc:22.975\n",
      "Epoch 001: | Loss:0.18950 | Acc:23.087\n",
      "Epoch 001: | Loss:0.19013 | Acc:23.188\n",
      "Epoch 001: | Loss:0.19069 | Acc:23.312\n",
      "Epoch 001: | Loss:0.19131 | Acc:23.400\n",
      "Epoch 001: | Loss:0.19188 | Acc:23.512\n",
      "Epoch 001: | Loss:0.19262 | Acc:23.575\n",
      "Epoch 001: | Loss:0.19361 | Acc:23.637\n",
      "Epoch 001: | Loss:0.19421 | Acc:23.750\n",
      "Epoch 001: | Loss:0.19472 | Acc:23.875\n",
      "Epoch 001: | Loss:0.19562 | Acc:23.962\n",
      "Epoch 001: | Loss:0.19628 | Acc:24.062\n",
      "Epoch 001: | Loss:0.19684 | Acc:24.175\n",
      "Epoch 001: | Loss:0.19750 | Acc:24.275\n",
      "Epoch 001: | Loss:0.19832 | Acc:24.363\n",
      "Epoch 001: | Loss:0.19891 | Acc:24.462\n",
      "Epoch 001: | Loss:0.19969 | Acc:24.550\n",
      "Epoch 001: | Loss:0.20029 | Acc:24.650\n",
      "Epoch 001: | Loss:0.20099 | Acc:24.725\n",
      "Epoch 001: | Loss:0.20148 | Acc:24.850\n",
      "Epoch 001: | Loss:0.20198 | Acc:24.962\n",
      "Epoch 001: | Loss:0.20264 | Acc:25.050\n",
      "Epoch 001: | Loss:0.20314 | Acc:25.175\n",
      "Epoch 001: | Loss:0.20368 | Acc:25.288\n",
      "Epoch 001: | Loss:0.20422 | Acc:25.413\n",
      "Epoch 001: | Loss:0.20509 | Acc:25.488\n",
      "Epoch 001: | Loss:0.20600 | Acc:25.550\n",
      "Epoch 001: | Loss:0.20662 | Acc:25.650\n",
      "Epoch 001: | Loss:0.20705 | Acc:25.775\n",
      "Epoch 001: | Loss:0.20816 | Acc:25.825\n",
      "Epoch 001: | Loss:0.20867 | Acc:25.950\n",
      "Epoch 001: | Loss:0.20941 | Acc:26.038\n",
      "Epoch 001: | Loss:0.20994 | Acc:26.150\n",
      "Epoch 001: | Loss:0.21048 | Acc:26.250\n",
      "Epoch 001: | Loss:0.21111 | Acc:26.350\n",
      "Epoch 001: | Loss:0.21178 | Acc:26.450\n",
      "Epoch 001: | Loss:0.21229 | Acc:26.575\n",
      "Epoch 001: | Loss:0.21282 | Acc:26.700\n",
      "Epoch 001: | Loss:0.21354 | Acc:26.788\n",
      "Epoch 001: | Loss:0.21413 | Acc:26.887\n",
      "Epoch 001: | Loss:0.21485 | Acc:26.975\n",
      "Epoch 001: | Loss:0.21538 | Acc:27.087\n",
      "Epoch 001: | Loss:0.21615 | Acc:27.175\n",
      "Epoch 001: | Loss:0.21690 | Acc:27.275\n",
      "Epoch 001: | Loss:0.21762 | Acc:27.363\n",
      "Epoch 001: | Loss:0.21820 | Acc:27.462\n",
      "Epoch 001: | Loss:0.21880 | Acc:27.562\n",
      "Epoch 001: | Loss:0.21973 | Acc:27.637\n",
      "Epoch 001: | Loss:0.22029 | Acc:27.750\n",
      "Epoch 001: | Loss:0.22084 | Acc:27.850\n",
      "Epoch 001: | Loss:0.22160 | Acc:27.938\n",
      "Epoch 001: | Loss:0.22227 | Acc:28.038\n",
      "Epoch 001: | Loss:0.22284 | Acc:28.150\n",
      "Epoch 001: | Loss:0.22344 | Acc:28.225\n",
      "Epoch 001: | Loss:0.22425 | Acc:28.288\n",
      "Epoch 001: | Loss:0.22519 | Acc:28.363\n",
      "Epoch 001: | Loss:0.22578 | Acc:28.475\n",
      "Epoch 001: | Loss:0.22643 | Acc:28.575\n",
      "Epoch 001: | Loss:0.22703 | Acc:28.663\n",
      "Epoch 001: | Loss:0.22758 | Acc:28.750\n",
      "Epoch 001: | Loss:0.22820 | Acc:28.850\n",
      "Epoch 001: | Loss:0.22886 | Acc:28.950\n",
      "Epoch 001: | Loss:0.22937 | Acc:29.062\n",
      "Epoch 001: | Loss:0.22986 | Acc:29.163\n",
      "Epoch 001: | Loss:0.23032 | Acc:29.288\n",
      "Epoch 001: | Loss:0.23098 | Acc:29.375\n",
      "Epoch 001: | Loss:0.23163 | Acc:29.462\n",
      "Epoch 001: | Loss:0.23208 | Acc:29.587\n",
      "Epoch 001: | Loss:0.23274 | Acc:29.675\n",
      "Epoch 001: | Loss:0.23326 | Acc:29.775\n",
      "Epoch 001: | Loss:0.23383 | Acc:29.875\n",
      "Epoch 001: | Loss:0.23445 | Acc:29.975\n",
      "Epoch 001: | Loss:0.23494 | Acc:30.087\n",
      "Epoch 001: | Loss:0.23545 | Acc:30.200\n",
      "Epoch 001: | Loss:0.23620 | Acc:30.275\n",
      "Epoch 001: | Loss:0.23679 | Acc:30.363\n",
      "Epoch 001: | Loss:0.23730 | Acc:30.475\n",
      "Epoch 001: | Loss:0.23787 | Acc:30.575\n",
      "Epoch 001: | Loss:0.23839 | Acc:30.688\n",
      "Epoch 001: | Loss:0.23884 | Acc:30.800\n",
      "Epoch 001: | Loss:0.23939 | Acc:30.913\n",
      "Epoch 001: | Loss:0.24005 | Acc:31.012\n",
      "Epoch 001: | Loss:0.24087 | Acc:31.087\n",
      "Epoch 001: | Loss:0.24154 | Acc:31.188\n",
      "Epoch 001: | Loss:0.24235 | Acc:31.275\n",
      "Epoch 001: | Loss:0.24284 | Acc:31.387\n",
      "Epoch 001: | Loss:0.24340 | Acc:31.488\n",
      "Epoch 001: | Loss:0.24397 | Acc:31.587\n",
      "Epoch 001: | Loss:0.24443 | Acc:31.712\n",
      "Epoch 001: | Loss:0.24502 | Acc:31.812\n",
      "Epoch 001: | Loss:0.24548 | Acc:31.925\n",
      "Epoch 001: | Loss:0.24600 | Acc:32.025\n",
      "Epoch 001: | Loss:0.24660 | Acc:32.125\n",
      "Epoch 001: | Loss:0.24719 | Acc:32.225\n",
      "Epoch 001: | Loss:0.24766 | Acc:32.337\n",
      "Epoch 001: | Loss:0.24820 | Acc:32.425\n",
      "Epoch 001: | Loss:0.24876 | Acc:32.538\n",
      "Epoch 001: | Loss:0.24933 | Acc:32.650\n",
      "Epoch 001: | Loss:0.25025 | Acc:32.712\n",
      "Epoch 001: | Loss:0.25098 | Acc:32.788\n",
      "Epoch 001: | Loss:0.25153 | Acc:32.900\n",
      "Epoch 001: | Loss:0.25227 | Acc:32.987\n",
      "Epoch 001: | Loss:0.25290 | Acc:33.087\n",
      "Epoch 001: | Loss:0.25381 | Acc:33.163\n",
      "Epoch 001: | Loss:0.25447 | Acc:33.250\n",
      "Epoch 001: | Loss:0.25497 | Acc:33.362\n",
      "Epoch 001: | Loss:0.25544 | Acc:33.475\n",
      "Epoch 001: | Loss:0.25584 | Acc:33.587\n",
      "Epoch 001: | Loss:0.25629 | Acc:33.700\n",
      "Epoch 001: | Loss:0.25692 | Acc:33.775\n",
      "Epoch 001: | Loss:0.25738 | Acc:33.900\n",
      "Epoch 001: | Loss:0.25818 | Acc:33.975\n",
      "Epoch 001: | Loss:0.25893 | Acc:34.062\n",
      "Epoch 001: | Loss:0.25942 | Acc:34.175\n",
      "Epoch 001: | Loss:0.25996 | Acc:34.275\n",
      "Epoch 001: | Loss:0.26063 | Acc:34.362\n",
      "Epoch 001: | Loss:0.26126 | Acc:34.450\n",
      "Epoch 001: | Loss:0.26179 | Acc:34.550\n",
      "Epoch 001: | Loss:0.26256 | Acc:34.638\n",
      "Epoch 001: | Loss:0.26319 | Acc:34.725\n",
      "Epoch 001: | Loss:0.26381 | Acc:34.825\n",
      "Epoch 001: | Loss:0.26455 | Acc:34.913\n",
      "Epoch 001: | Loss:0.26520 | Acc:35.013\n",
      "Epoch 001: | Loss:0.26582 | Acc:35.125\n",
      "Epoch 001: | Loss:0.26648 | Acc:35.225\n",
      "Epoch 001: | Loss:0.26702 | Acc:35.337\n",
      "Epoch 001: | Loss:0.26770 | Acc:35.413\n",
      "Epoch 001: | Loss:0.26816 | Acc:35.525\n",
      "Epoch 001: | Loss:0.26899 | Acc:35.600\n",
      "Epoch 001: | Loss:0.26966 | Acc:35.688\n",
      "Epoch 001: | Loss:0.27017 | Acc:35.800\n",
      "Epoch 001: | Loss:0.27077 | Acc:35.900\n",
      "Epoch 001: | Loss:0.27136 | Acc:35.987\n",
      "Epoch 001: | Loss:0.27189 | Acc:36.100\n",
      "Epoch 001: | Loss:0.27267 | Acc:36.188\n",
      "Epoch 001: | Loss:0.27332 | Acc:36.288\n",
      "Epoch 001: | Loss:0.27385 | Acc:36.388\n",
      "Epoch 001: | Loss:0.27427 | Acc:36.513\n",
      "Epoch 001: | Loss:0.27480 | Acc:36.612\n",
      "Epoch 001: | Loss:0.27545 | Acc:36.712\n",
      "Epoch 001: | Loss:0.27610 | Acc:36.812\n",
      "Epoch 001: | Loss:0.27658 | Acc:36.925\n",
      "Epoch 001: | Loss:0.27726 | Acc:37.025\n",
      "Epoch 001: | Loss:0.27811 | Acc:37.112\n",
      "Epoch 001: | Loss:0.27863 | Acc:37.200\n",
      "Epoch 001: | Loss:0.27908 | Acc:37.312\n",
      "Epoch 001: | Loss:0.27954 | Acc:37.413\n",
      "Epoch 001: | Loss:0.28032 | Acc:37.487\n",
      "Epoch 001: | Loss:0.28078 | Acc:37.600\n",
      "Epoch 001: | Loss:0.28129 | Acc:37.712\n",
      "Epoch 001: | Loss:0.28186 | Acc:37.812\n",
      "Epoch 001: | Loss:0.28276 | Acc:37.875\n",
      "Epoch 001: | Loss:0.28346 | Acc:37.962\n",
      "Epoch 001: | Loss:0.28402 | Acc:38.062\n",
      "Epoch 001: | Loss:0.28449 | Acc:38.188\n",
      "Epoch 001: | Loss:0.28510 | Acc:38.275\n",
      "Epoch 001: | Loss:0.28577 | Acc:38.362\n",
      "Epoch 001: | Loss:0.28644 | Acc:38.450\n",
      "Epoch 001: | Loss:0.28695 | Acc:38.562\n",
      "Epoch 001: | Loss:0.28741 | Acc:38.675\n",
      "Epoch 001: | Loss:0.28781 | Acc:38.788\n",
      "Epoch 001: | Loss:0.28820 | Acc:38.913\n",
      "Epoch 001: | Loss:0.28871 | Acc:39.000\n",
      "Epoch 001: | Loss:0.28917 | Acc:39.112\n",
      "Epoch 001: | Loss:0.28985 | Acc:39.188\n",
      "Epoch 001: | Loss:0.29057 | Acc:39.275\n",
      "Epoch 001: | Loss:0.29101 | Acc:39.388\n",
      "Epoch 001: | Loss:0.29147 | Acc:39.487\n",
      "Epoch 001: | Loss:0.29194 | Acc:39.612\n",
      "Epoch 001: | Loss:0.29255 | Acc:39.725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss:0.29309 | Acc:39.825\n",
      "Epoch 001: | Loss:0.29383 | Acc:39.900\n",
      "Epoch 001: | Loss:0.29473 | Acc:39.975\n",
      "Epoch 001: | Loss:0.29518 | Acc:40.087\n",
      "Epoch 001: | Loss:0.29567 | Acc:40.200\n",
      "Epoch 001: | Loss:0.29645 | Acc:40.263\n",
      "Epoch 001: | Loss:0.29704 | Acc:40.362\n",
      "Epoch 001: | Loss:0.29771 | Acc:40.450\n",
      "Epoch 001: | Loss:0.29823 | Acc:40.562\n",
      "Epoch 001: | Loss:0.29872 | Acc:40.663\n",
      "Epoch 001: | Loss:0.29934 | Acc:40.750\n",
      "Epoch 001: | Loss:0.30016 | Acc:40.825\n",
      "Epoch 001: | Loss:0.30055 | Acc:40.950\n",
      "Epoch 001: | Loss:0.30104 | Acc:41.062\n",
      "Epoch 001: | Loss:0.30154 | Acc:41.175\n",
      "Epoch 001: | Loss:0.30198 | Acc:41.288\n",
      "Epoch 001: | Loss:0.30233 | Acc:41.413\n",
      "Epoch 001: | Loss:0.30294 | Acc:41.500\n",
      "Epoch 001: | Loss:0.30338 | Acc:41.612\n",
      "Epoch 001: | Loss:0.30374 | Acc:41.737\n",
      "Epoch 001: | Loss:0.30427 | Acc:41.837\n",
      "Epoch 001: | Loss:0.30486 | Acc:41.938\n",
      "Epoch 001: | Loss:0.30538 | Acc:42.025\n",
      "Epoch 001: | Loss:0.30580 | Acc:42.138\n",
      "Epoch 001: | Loss:0.30662 | Acc:42.225\n",
      "Epoch 001: | Loss:0.30723 | Acc:42.300\n",
      "Epoch 001: | Loss:0.30771 | Acc:42.400\n",
      "Epoch 001: | Loss:0.30826 | Acc:42.513\n",
      "Epoch 001: | Loss:0.30888 | Acc:42.625\n",
      "Epoch 001: | Loss:0.30943 | Acc:42.725\n",
      "Epoch 001: | Loss:0.30987 | Acc:42.837\n",
      "Epoch 001: | Loss:0.31048 | Acc:42.938\n",
      "Epoch 001: | Loss:0.31098 | Acc:43.050\n",
      "Epoch 001: | Loss:0.31128 | Acc:43.175\n",
      "Epoch 001: | Loss:0.31174 | Acc:43.288\n",
      "Epoch 001: | Loss:0.31214 | Acc:43.400\n",
      "Epoch 001: | Loss:0.31314 | Acc:43.475\n",
      "Epoch 001: | Loss:0.31366 | Acc:43.587\n",
      "Epoch 001: | Loss:0.31406 | Acc:43.712\n",
      "Epoch 001: | Loss:0.31482 | Acc:43.788\n",
      "Epoch 001: | Loss:0.31527 | Acc:43.913\n",
      "Epoch 001: | Loss:0.31567 | Acc:44.038\n",
      "Epoch 001: | Loss:0.31607 | Acc:44.150\n",
      "Epoch 001: | Loss:0.31660 | Acc:44.250\n",
      "Epoch 001: | Loss:0.31704 | Acc:44.350\n",
      "Epoch 001: | Loss:0.31748 | Acc:44.462\n",
      "Epoch 001: | Loss:0.31791 | Acc:44.575\n",
      "Epoch 001: | Loss:0.31848 | Acc:44.675\n",
      "Epoch 001: | Loss:0.31894 | Acc:44.788\n",
      "Epoch 001: | Loss:0.31930 | Acc:44.913\n",
      "Epoch 001: | Loss:0.31983 | Acc:45.025\n",
      "Epoch 001: | Loss:0.32027 | Acc:45.138\n",
      "Epoch 001: | Loss:0.32082 | Acc:45.250\n",
      "Epoch 001: | Loss:0.32119 | Acc:45.375\n",
      "Epoch 001: | Loss:0.32174 | Acc:45.462\n",
      "Epoch 001: | Loss:0.32231 | Acc:45.575\n",
      "Epoch 001: | Loss:0.32285 | Acc:45.688\n",
      "Epoch 001: | Loss:0.32354 | Acc:45.775\n",
      "Epoch 001: | Loss:0.32435 | Acc:45.850\n",
      "Epoch 001: | Loss:0.32490 | Acc:45.962\n",
      "Epoch 001: | Loss:0.32554 | Acc:46.050\n",
      "Epoch 001: | Loss:0.32600 | Acc:46.163\n",
      "Epoch 001: | Loss:0.32637 | Acc:46.275\n",
      "Epoch 001: | Loss:0.32672 | Acc:46.400\n",
      "Epoch 001: | Loss:0.32749 | Acc:46.475\n",
      "Epoch 001: | Loss:0.32798 | Acc:46.587\n",
      "Epoch 001: | Loss:0.32843 | Acc:46.700\n",
      "Epoch 001: | Loss:0.32892 | Acc:46.800\n",
      "Epoch 001: | Loss:0.32984 | Acc:46.888\n",
      "Epoch 001: | Loss:0.33053 | Acc:46.950\n",
      "Epoch 001: | Loss:0.33089 | Acc:47.075\n",
      "Epoch 001: | Loss:0.33140 | Acc:47.188\n",
      "Epoch 001: | Loss:0.33175 | Acc:47.312\n",
      "Epoch 001: | Loss:0.33237 | Acc:47.413\n",
      "Epoch 001: | Loss:0.33299 | Acc:47.500\n",
      "Epoch 001: | Loss:0.33339 | Acc:47.625\n",
      "Epoch 001: | Loss:0.33386 | Acc:47.725\n",
      "Epoch 001: | Loss:0.33450 | Acc:47.800\n",
      "Epoch 001: | Loss:0.33508 | Acc:47.888\n",
      "Epoch 001: | Loss:0.33562 | Acc:47.987\n",
      "Epoch 001: | Loss:0.33652 | Acc:48.062\n",
      "Epoch 001: | Loss:0.33711 | Acc:48.175\n",
      "Epoch 001: | Loss:0.33758 | Acc:48.288\n",
      "Epoch 001: | Loss:0.33801 | Acc:48.400\n",
      "Epoch 001: | Loss:0.33849 | Acc:48.500\n",
      "Epoch 001: | Loss:0.33907 | Acc:48.612\n",
      "Epoch 001: | Loss:0.33971 | Acc:48.688\n",
      "Epoch 001: | Loss:0.34024 | Acc:48.800\n",
      "Epoch 001: | Loss:0.34079 | Acc:48.900\n",
      "Epoch 001: | Loss:0.34121 | Acc:49.013\n",
      "Epoch 001: | Loss:0.34176 | Acc:49.125\n",
      "Epoch 001: | Loss:0.34233 | Acc:49.225\n",
      "Epoch 001: | Loss:0.34275 | Acc:49.337\n",
      "Epoch 001: | Loss:0.34318 | Acc:49.438\n",
      "Epoch 001: | Loss:0.34360 | Acc:49.538\n",
      "Epoch 001: | Loss:0.34416 | Acc:49.625\n",
      "Epoch 001: | Loss:0.34534 | Acc:49.688\n",
      "Epoch 001: | Loss:0.34571 | Acc:49.812\n",
      "Epoch 001: | Loss:0.34629 | Acc:49.913\n",
      "Epoch 001: | Loss:0.34686 | Acc:50.000\n",
      "Epoch 001: | Loss:0.34721 | Acc:50.125\n",
      "Epoch 001: | Loss:0.34773 | Acc:50.225\n",
      "Epoch 001: | Loss:0.34841 | Acc:50.300\n",
      "Epoch 001: | Loss:0.34935 | Acc:50.375\n",
      "Epoch 001: | Loss:0.34991 | Acc:50.475\n",
      "Epoch 001: | Loss:0.35033 | Acc:50.575\n",
      "Epoch 001: | Loss:0.35094 | Acc:50.675\n",
      "Epoch 001: | Loss:0.35182 | Acc:50.750\n",
      "Epoch 001: | Loss:0.35237 | Acc:50.850\n",
      "Epoch 001: | Loss:0.35277 | Acc:50.962\n",
      "Epoch 001: | Loss:0.35315 | Acc:51.075\n",
      "Epoch 001: | Loss:0.35374 | Acc:51.150\n",
      "Epoch 001: | Loss:0.35430 | Acc:51.250\n",
      "Epoch 001: | Loss:0.35521 | Acc:51.325\n",
      "Epoch 001: | Loss:0.35587 | Acc:51.425\n",
      "Epoch 001: | Loss:0.35629 | Acc:51.538\n",
      "Epoch 001: | Loss:0.35674 | Acc:51.638\n",
      "Epoch 001: | Loss:0.35734 | Acc:51.737\n",
      "Epoch 001: | Loss:0.35789 | Acc:51.837\n",
      "Epoch 001: | Loss:0.35838 | Acc:51.938\n",
      "Epoch 001: | Loss:0.35877 | Acc:52.062\n",
      "Epoch 001: | Loss:0.35940 | Acc:52.163\n",
      "Epoch 001: | Loss:0.36004 | Acc:52.250\n",
      "Epoch 001: | Loss:0.36056 | Acc:52.350\n",
      "Epoch 001: | Loss:0.36108 | Acc:52.438\n",
      "Epoch 001: | Loss:0.36151 | Acc:52.550\n",
      "Epoch 001: | Loss:0.36205 | Acc:52.650\n",
      "Epoch 001: | Loss:0.36277 | Acc:52.750\n",
      "Epoch 001: | Loss:0.36343 | Acc:52.850\n",
      "Epoch 001: | Loss:0.36425 | Acc:52.938\n",
      "Epoch 001: | Loss:0.36477 | Acc:53.050\n",
      "Epoch 001: | Loss:0.36544 | Acc:53.138\n",
      "Epoch 001: | Loss:0.36598 | Acc:53.237\n",
      "Epoch 001: | Loss:0.36626 | Acc:53.362\n",
      "Epoch 001: | Loss:0.36676 | Acc:53.462\n",
      "Epoch 001: | Loss:0.36720 | Acc:53.562\n",
      "Epoch 001: | Loss:0.36826 | Acc:53.638\n",
      "Epoch 001: | Loss:0.36946 | Acc:53.688\n",
      "Epoch 001: | Loss:0.37031 | Acc:53.763\n",
      "Epoch 001: | Loss:0.37095 | Acc:53.850\n",
      "Epoch 001: | Loss:0.37130 | Acc:53.962\n",
      "Epoch 001: | Loss:0.37185 | Acc:54.038\n",
      "Epoch 001: | Loss:0.37240 | Acc:54.138\n",
      "Epoch 001: | Loss:0.37332 | Acc:54.212\n",
      "Epoch 001: | Loss:0.37386 | Acc:54.312\n",
      "Epoch 001: | Loss:0.37448 | Acc:54.400\n",
      "Epoch 001: | Loss:0.37498 | Acc:54.500\n",
      "Epoch 001: | Loss:0.37561 | Acc:54.600\n",
      "Epoch 001: | Loss:0.37626 | Acc:54.700\n",
      "Epoch 001: | Loss:0.37672 | Acc:54.800\n",
      "Epoch 001: | Loss:0.37713 | Acc:54.900\n",
      "Epoch 001: | Loss:0.37780 | Acc:55.000\n",
      "Epoch 001: | Loss:0.37817 | Acc:55.125\n",
      "Epoch 001: | Loss:0.37858 | Acc:55.225\n",
      "Epoch 001: | Loss:0.37888 | Acc:55.350\n",
      "Epoch 001: | Loss:0.37962 | Acc:55.425\n",
      "Epoch 001: | Loss:0.38034 | Acc:55.513\n",
      "Epoch 001: | Loss:0.38062 | Acc:55.638\n",
      "Epoch 001: | Loss:0.38092 | Acc:55.763\n",
      "Epoch 001: | Loss:0.38159 | Acc:55.862\n",
      "Epoch 001: | Loss:0.38238 | Acc:55.938\n",
      "Epoch 001: | Loss:0.38303 | Acc:56.038\n",
      "Epoch 001: | Loss:0.38339 | Acc:56.150\n",
      "Epoch 001: | Loss:0.38378 | Acc:56.263\n",
      "Epoch 001: | Loss:0.38449 | Acc:56.350\n",
      "Epoch 001: | Loss:0.38507 | Acc:56.425\n",
      "Epoch 001: | Loss:0.38542 | Acc:56.538\n",
      "Epoch 001: | Loss:0.38614 | Acc:56.625\n",
      "Epoch 001: | Loss:0.38677 | Acc:56.725\n",
      "Epoch 001: | Loss:0.38764 | Acc:56.812\n",
      "Epoch 001: | Loss:0.38821 | Acc:56.913\n",
      "Epoch 001: | Loss:0.38868 | Acc:57.013\n",
      "Epoch 001: | Loss:0.38944 | Acc:57.112\n",
      "Epoch 001: | Loss:0.38987 | Acc:57.212\n",
      "Epoch 001: | Loss:0.39032 | Acc:57.325\n",
      "Epoch 001: | Loss:0.39101 | Acc:57.425\n",
      "Epoch 001: | Loss:0.39160 | Acc:57.538\n",
      "Epoch 001: | Loss:0.39224 | Acc:57.625\n",
      "Epoch 001: | Loss:0.39292 | Acc:57.712\n",
      "Epoch 001: | Loss:0.39341 | Acc:57.825\n",
      "Epoch 001: | Loss:0.39372 | Acc:57.950\n",
      "Epoch 001: | Loss:0.39414 | Acc:58.050\n",
      "Epoch 001: | Loss:0.39481 | Acc:58.138\n",
      "Epoch 001: | Loss:0.39540 | Acc:58.237\n",
      "Epoch 001: | Loss:0.39586 | Acc:58.350\n",
      "Epoch 001: | Loss:0.39652 | Acc:58.438\n",
      "Epoch 001: | Loss:0.39711 | Acc:58.538\n",
      "Epoch 001: | Loss:0.39767 | Acc:58.650\n",
      "Epoch 001: | Loss:0.39805 | Acc:58.763\n",
      "Epoch 001: | Loss:0.39849 | Acc:58.875\n",
      "Epoch 001: | Loss:0.39881 | Acc:59.000\n",
      "Epoch 001: | Loss:0.39980 | Acc:59.062\n",
      "Epoch 001: | Loss:0.40038 | Acc:59.150\n",
      "Epoch 001: | Loss:0.40107 | Acc:59.237\n",
      "Epoch 001: | Loss:0.40169 | Acc:59.337\n",
      "Epoch 001: | Loss:0.40238 | Acc:59.425\n",
      "Epoch 001: | Loss:0.40281 | Acc:59.525\n",
      "Epoch 001: | Loss:0.40330 | Acc:59.625\n",
      "Epoch 001: | Loss:0.40374 | Acc:59.725\n",
      "Epoch 001: | Loss:0.40478 | Acc:59.800\n",
      "Epoch 001: | Loss:0.40520 | Acc:59.925\n",
      "Epoch 001: | Loss:0.40569 | Acc:60.025\n",
      "Epoch 001: | Loss:0.40617 | Acc:60.125\n",
      "Epoch 001: | Loss:0.40649 | Acc:60.250\n",
      "Epoch 001: | Loss:0.40736 | Acc:60.325\n",
      "Epoch 001: | Loss:0.40798 | Acc:60.413\n",
      "Epoch 001: | Loss:0.40844 | Acc:60.525\n",
      "Epoch 001: | Loss:0.40886 | Acc:60.625\n",
      "Epoch 001: | Loss:0.40932 | Acc:60.725\n",
      "Epoch 001: | Loss:0.40961 | Acc:60.850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss:0.41026 | Acc:60.950\n",
      "Epoch 001: | Loss:0.41099 | Acc:61.025\n",
      "Epoch 001: | Loss:0.41142 | Acc:61.138\n",
      "Epoch 001: | Loss:0.41189 | Acc:61.237\n",
      "Epoch 001: | Loss:0.41231 | Acc:61.350\n",
      "Epoch 001: | Loss:0.41282 | Acc:61.425\n",
      "Epoch 001: | Loss:0.41360 | Acc:61.500\n",
      "Epoch 001: | Loss:0.41410 | Acc:61.600\n",
      "Epoch 001: | Loss:0.41458 | Acc:61.700\n",
      "Epoch 001: | Loss:0.41529 | Acc:61.788\n",
      "Epoch 001: | Loss:0.41578 | Acc:61.888\n",
      "Epoch 001: | Loss:0.41619 | Acc:62.000\n",
      "Epoch 001: | Loss:0.41670 | Acc:62.100\n",
      "Epoch 001: | Loss:0.41719 | Acc:62.200\n",
      "Epoch 001: | Loss:0.41758 | Acc:62.312\n",
      "Epoch 001: | Loss:0.41836 | Acc:62.400\n",
      "Epoch 001: | Loss:0.41939 | Acc:62.450\n",
      "Epoch 001: | Loss:0.41992 | Acc:62.550\n",
      "Epoch 001: | Loss:0.42047 | Acc:62.638\n",
      "Epoch 001: | Loss:0.42129 | Acc:62.712\n",
      "Epoch 001: | Loss:0.42185 | Acc:62.800\n",
      "Epoch 001: | Loss:0.42238 | Acc:62.900\n",
      "Epoch 001: | Loss:0.42279 | Acc:63.013\n",
      "Epoch 001: | Loss:0.42322 | Acc:63.112\n",
      "Epoch 001: | Loss:0.42393 | Acc:63.200\n",
      "Epoch 001: | Loss:0.42434 | Acc:63.312\n",
      "Epoch 001: | Loss:0.42492 | Acc:63.413\n",
      "Epoch 001: | Loss:0.42524 | Acc:63.525\n",
      "Epoch 001: | Loss:0.42564 | Acc:63.638\n",
      "Epoch 001: | Loss:0.42616 | Acc:63.750\n",
      "Epoch 001: | Loss:0.42669 | Acc:63.850\n",
      "Epoch 001: | Loss:0.42725 | Acc:63.962\n",
      "Epoch 001: | Loss:0.42794 | Acc:64.062\n",
      "Epoch 001: | Loss:0.42835 | Acc:64.175\n",
      "Epoch 001: | Loss:0.42879 | Acc:64.287\n",
      "Epoch 001: | Loss:0.42928 | Acc:64.388\n",
      "Epoch 001: | Loss:0.42992 | Acc:64.500\n",
      "Epoch 001: | Loss:0.43063 | Acc:64.588\n",
      "Epoch 001: | Loss:0.43135 | Acc:64.675\n",
      "Epoch 001: | Loss:0.43169 | Acc:64.800\n",
      "Epoch 001: | Loss:0.43210 | Acc:64.912\n",
      "Epoch 001: | Loss:0.43288 | Acc:65.000\n",
      "Epoch 001: | Loss:0.43341 | Acc:65.088\n",
      "Epoch 001: | Loss:0.43372 | Acc:65.200\n",
      "Epoch 001: | Loss:0.43424 | Acc:65.312\n",
      "Epoch 001: | Loss:0.43470 | Acc:65.412\n",
      "Epoch 001: | Loss:0.43540 | Acc:65.487\n",
      "Epoch 001: | Loss:0.43655 | Acc:65.550\n",
      "Epoch 001: | Loss:0.43726 | Acc:65.650\n",
      "Epoch 001: | Loss:0.43777 | Acc:65.750\n",
      "Epoch 001: | Loss:0.43865 | Acc:65.838\n",
      "Epoch 001: | Loss:0.43920 | Acc:65.938\n",
      "Epoch 001: | Loss:0.43989 | Acc:66.037\n",
      "Epoch 001: | Loss:0.44047 | Acc:66.138\n",
      "Epoch 001: | Loss:0.44093 | Acc:66.250\n",
      "Epoch 001: | Loss:0.44154 | Acc:66.350\n",
      "Epoch 001: | Loss:0.44229 | Acc:66.438\n",
      "Epoch 001: | Loss:0.44269 | Acc:66.537\n",
      "Epoch 001: | Loss:0.44337 | Acc:66.625\n",
      "Epoch 001: | Loss:0.44380 | Acc:66.725\n",
      "Epoch 001: | Loss:0.44426 | Acc:66.825\n",
      "Epoch 001: | Loss:0.44500 | Acc:66.912\n",
      "Epoch 001: | Loss:0.44607 | Acc:66.975\n",
      "Epoch 001: | Loss:0.44685 | Acc:67.062\n",
      "Epoch 001: | Loss:0.44730 | Acc:67.162\n",
      "Epoch 001: | Loss:0.44766 | Acc:67.275\n",
      "Epoch 001: | Loss:0.44817 | Acc:67.388\n",
      "Epoch 001: | Loss:0.44899 | Acc:67.463\n",
      "Epoch 001: | Loss:0.44943 | Acc:67.562\n",
      "Epoch 001: | Loss:0.44975 | Acc:67.688\n",
      "Epoch 001: | Loss:0.45014 | Acc:67.787\n",
      "Epoch 001: | Loss:0.45095 | Acc:67.862\n",
      "Epoch 001: | Loss:0.45166 | Acc:67.963\n",
      "Epoch 001: | Loss:0.45235 | Acc:68.050\n",
      "Epoch 001: | Loss:0.45321 | Acc:68.138\n",
      "Epoch 001: | Loss:0.45385 | Acc:68.250\n",
      "Epoch 001: | Loss:0.45483 | Acc:68.350\n",
      "Epoch 001: | Loss:0.45552 | Acc:68.438\n",
      "Epoch 001: | Loss:0.45608 | Acc:68.537\n",
      "Epoch 001: | Loss:0.45643 | Acc:68.650\n",
      "Epoch 001: | Loss:0.45745 | Acc:68.713\n",
      "Epoch 001: | Loss:0.45793 | Acc:68.812\n",
      "Epoch 001: | Loss:0.45860 | Acc:68.900\n",
      "Epoch 001: | Loss:0.45886 | Acc:69.025\n",
      "Epoch 001: | Loss:0.45934 | Acc:69.125\n",
      "Epoch 001: | Loss:0.45978 | Acc:69.237\n",
      "Epoch 001: | Loss:0.46035 | Acc:69.338\n",
      "Epoch 001: | Loss:0.46095 | Acc:69.438\n",
      "Epoch 001: | Loss:0.46177 | Acc:69.513\n",
      "Epoch 001: | Loss:0.46220 | Acc:69.612\n",
      "Epoch 001: | Loss:0.46253 | Acc:69.737\n",
      "Epoch 001: | Loss:0.46318 | Acc:69.825\n",
      "Epoch 001: | Loss:0.46375 | Acc:69.925\n",
      "Epoch 001: | Loss:0.46417 | Acc:70.037\n",
      "Epoch 001: | Loss:0.46468 | Acc:70.138\n",
      "Epoch 001: | Loss:0.46504 | Acc:70.250\n",
      "Epoch 001: | Loss:0.46542 | Acc:70.362\n",
      "Epoch 001: | Loss:0.46593 | Acc:70.450\n",
      "Epoch 001: | Loss:0.46630 | Acc:70.562\n",
      "Epoch 001: | Loss:0.46691 | Acc:70.650\n",
      "Epoch 001: | Loss:0.46737 | Acc:70.763\n",
      "Epoch 001: | Loss:0.46794 | Acc:70.862\n",
      "Epoch 001: | Loss:0.46835 | Acc:70.975\n",
      "Epoch 001: | Loss:0.46952 | Acc:71.050\n",
      "Epoch 001: | Loss:0.46991 | Acc:71.150\n",
      "Epoch 001: | Loss:0.47065 | Acc:71.237\n",
      "Epoch 001: | Loss:0.47108 | Acc:71.338\n",
      "Epoch 001: | Loss:0.47164 | Acc:71.425\n",
      "Epoch 001: | Loss:0.47203 | Acc:71.537\n",
      "Epoch 001: | Loss:0.47241 | Acc:71.638\n",
      "Epoch 001: | Loss:0.47269 | Acc:71.763\n",
      "Epoch 001: | Loss:0.47310 | Acc:71.875\n",
      "Epoch 001: | Loss:0.47404 | Acc:71.963\n",
      "Epoch 001: | Loss:0.47438 | Acc:72.088\n",
      "Epoch 001: | Loss:0.47490 | Acc:72.188\n",
      "Epoch 001: | Loss:0.47576 | Acc:72.275\n",
      "Epoch 001: | Loss:0.47636 | Acc:72.362\n",
      "Epoch 001: | Loss:0.47690 | Acc:72.463\n",
      "Epoch 001: | Loss:0.47789 | Acc:72.525\n",
      "Epoch 001: | Loss:0.47823 | Acc:72.638\n",
      "Epoch 001: | Loss:0.47896 | Acc:72.713\n",
      "Epoch 001: | Loss:0.47952 | Acc:72.800\n",
      "Epoch 001: | Loss:0.47988 | Acc:72.912\n",
      "Epoch 001: | Loss:0.48032 | Acc:73.025\n",
      "Epoch 001: | Loss:0.48103 | Acc:73.100\n",
      "Epoch 001: | Loss:0.48139 | Acc:73.213\n",
      "Epoch 001: | Loss:0.48172 | Acc:73.325\n",
      "Epoch 001: | Loss:0.48207 | Acc:73.450\n",
      "Epoch 001: | Loss:0.48241 | Acc:73.562\n",
      "Epoch 001: | Loss:0.48282 | Acc:73.675\n",
      "Epoch 001: | Loss:0.48334 | Acc:73.775\n",
      "Epoch 001: | Loss:0.48366 | Acc:73.900\n",
      "Epoch 001: | Loss:0.48390 | Acc:74.025\n",
      "Epoch 001: | Loss:0.48440 | Acc:74.112\n",
      "Epoch 001: | Loss:0.48499 | Acc:74.188\n",
      "Epoch 001: | Loss:0.48564 | Acc:74.263\n",
      "Epoch 001: | Loss:0.48617 | Acc:74.350\n",
      "Epoch 001: | Loss:0.48660 | Acc:74.463\n",
      "Epoch 001: | Loss:0.48692 | Acc:74.588\n",
      "Epoch 001: | Loss:0.48727 | Acc:74.713\n",
      "Epoch 001: | Loss:0.48787 | Acc:74.812\n",
      "Epoch 001: | Loss:0.48860 | Acc:74.900\n",
      "Epoch 001: | Loss:0.48945 | Acc:74.963\n",
      "Epoch 001: | Loss:0.49030 | Acc:75.062\n",
      "Epoch 001: | Loss:0.49096 | Acc:75.150\n",
      "Epoch 001: | Loss:0.49187 | Acc:75.237\n",
      "Epoch 001: | Loss:0.49228 | Acc:75.350\n",
      "Epoch 001: | Loss:0.49270 | Acc:75.463\n",
      "Epoch 001: | Loss:0.49314 | Acc:75.562\n",
      "Epoch 001: | Loss:0.49344 | Acc:75.688\n",
      "Epoch 001: | Loss:0.49402 | Acc:75.787\n",
      "Epoch 001: | Loss:0.49462 | Acc:75.875\n",
      "Epoch 001: | Loss:0.49501 | Acc:75.987\n",
      "Epoch 001: | Loss:0.49551 | Acc:76.088\n",
      "Epoch 001: | Loss:0.49613 | Acc:76.175\n",
      "Epoch 001: | Loss:0.49645 | Acc:76.300\n",
      "Epoch 001: | Loss:0.49675 | Acc:76.425\n",
      "Epoch 001: | Loss:0.49731 | Acc:76.525\n",
      "Epoch 001: | Loss:0.49817 | Acc:76.600\n",
      "Epoch 001: | Loss:0.49853 | Acc:76.713\n",
      "Epoch 001: | Loss:0.49930 | Acc:76.787\n",
      "Epoch 001: | Loss:0.49961 | Acc:76.912\n",
      "Epoch 001: | Loss:0.50002 | Acc:77.013\n",
      "Epoch 001: | Loss:0.50027 | Acc:77.138\n",
      "Epoch 001: | Loss:0.50116 | Acc:77.200\n",
      "Epoch 001: | Loss:0.50162 | Acc:77.312\n",
      "Epoch 001: | Loss:0.50254 | Acc:77.388\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "classifier.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        #setting gradient to 0 per mini-batch\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = classifier(X_batch)\n",
    "        loss =criterion(y_pred, y_batch)\n",
    "        acc = binary_acc(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        print(f'Epoch {e+0:03}: | Loss:{epoch_loss/len(train_loader):.5f} | Acc:{epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model\n",
    "y_pred_list = []\n",
    "classifier.eval()\n",
    "count = 0\n",
    "#ensures no back propagation during testing and reduces memeory usage\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = classifier(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        \n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "    y_pred_list = [i.squeeze().tolist() for i in y_pred_list] \n",
    "    #y_pred_list = [bool(i) for i in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.101\n",
      "152202.0\n"
     ]
    }
   ],
   "source": [
    "acc = binary_acc(torch.FloatTensor(y_pred_list), torch.FloatTensor(y_test.values))\n",
    "accu = acc.item()\n",
    "print(accu/len(y_test))\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  predicted\n",
       "0       0      False\n",
       "1       0      False\n",
       "2       0      False\n",
       "3       0      False\n",
       "4       0      False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(y_test)\n",
    "df1.reset_index(inplace=True)\n",
    "df1.drop(columns=['index'], axis=1, inplace=True)\n",
    "df2 = pd.DataFrame(y_pred_list)\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "df.columns=['target','predicted']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1550   57]\n",
      " [ 289  104]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "cm = confusion_matrix(df['target'],df['predicted'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our confusion matrix we conclude that:\n",
    "1. **True positive:** 91(We predicted a positive result and it was positive)- the model rightly predicted the ones who left the bank \n",
    "2. **True negative:** 1570(We predicted a negative result and it was negative)-the model rightly predicted the ones who stayed at the bank \n",
    "3. **False positive:** 37(We predicted a positive result and it was negative)-the model predicted that these ones left when they actually stayed\n",
    "4. **False negative:** 302(We predicted a negative result and it was positive)- the model predicted that these ones stayed when they actually left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90      1607\n",
      "           1       0.71      0.23      0.35       393\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      2000\n",
      "   macro avg       0.77      0.60      0.63      2000\n",
      "weighted avg       0.81      0.83      0.79      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
